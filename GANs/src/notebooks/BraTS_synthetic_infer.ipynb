{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"BraTS_W_PWA100__W_PWT100__Unet_FC_new\"\n",
    "UNET = \"Unet_FC\" # Unet or Unet_FC\n",
    "RESUME = 999\n",
    "DEVICE = \"cuda:0\"\n",
    "HOME_DIR = \"../../..\"\n",
    "WORK_DIR = \"../../..\"\n",
    "\n",
    "from os.path import join\n",
    "# Fix variablles #\n",
    "DATA_LIST_KEY = \"training\"\n",
    "DATA_LIST_FILE_PATH = join(WORK_DIR, \"GANs/data/BraTS2023_GLI_data_split.json\") # Path where to save the json file \n",
    "DATA_DIR = join(WORK_DIR, \"brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\") # WORK_DIR: Working dir\n",
    "CHECKPOINT_PATH = f\"../../checkpoint/style_256/{EXPERIMENT_NAME}\"\n",
    "IMAGE_SIZE = (256, 256, 256)\n",
    "DIM = 1024\n",
    "NOISE_DIM = 512\n",
    "IN_CHANNEL_G = 3\n",
    "OUT_CHANNEL_G = 1\n",
    "SKIP_LATENT = False\n",
    "TAHN_ACT = False\n",
    "SAVE_DIR = \"../../nnUNet/nnUNet_raw/Dataset996_BraTS_GAN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists, dirname, basename\n",
    "from os import listdir, makedirs, environ\n",
    "def maybe_make_dir(directory):\n",
    "    if not exists(directory):\n",
    "        # If it doesn't exist, create the directory\n",
    "        makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING THE UNET Fully Connected LIKE GENERATOR\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from tqdm import tqdm\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset, Dataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    CropForegroundd,\n",
    "    Invertd,\n",
    "    RandZoomd,\n",
    "    \n",
    ")\n",
    "\n",
    "sys.path.insert(1, join(HOME_DIR, \"GANs/src\"))\n",
    "sys.path.insert(1, \"..\")\n",
    "sys.path.insert(1, \".\")\n",
    "from utils.data_loader_utils import ConvertToMultiChannelBasedOnBratsClasses2023d\n",
    "if UNET==\"Unet_FC\":\n",
    "    print(\"USING THE UNET Fully Connected LIKE GENERATOR\")\n",
    "    from networks.cWGAN_Style_Unet_256_FC import Generator, Critic\n",
    "elif UNET==\"Unet\":\n",
    "    print(\"USING THE UNET LIKE GENERATOR\")\n",
    "    from networks.cWGAN_Style_Unet_256 import Generator, Critic\n",
    "else:\n",
    "    print(\"WELCOME TO THE ERROR ZONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen(checkpoint_path, RESUME):\n",
    "    print(f\"Loading from: {checkpoint_path}, {RESUME}\")\n",
    "    gen = Generator(in_channels=DIM, latent_dim=NOISE_DIM, IN_CHANNEL_G=IN_CHANNEL_G, OUT_CHANNEL_G=OUT_CHANNEL_G, z_dim=NOISE_DIM, w_dim=NOISE_DIM, skip_latent=SKIP_LATENT, tahn_act=TAHN_ACT)\n",
    "    gen.to(DEVICE)\n",
    "    gen_weight_path = join(checkpoint_path, \"weights\", f\"{RESUME}_gen.pth\")\n",
    "    checkpoint = torch.load(gen_weight_path, map_location=torch.device(DEVICE))\n",
    "    # Load the model's state dictionary\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "def generate_detection_train_transform(\n",
    "    image_key,\n",
    "    label_key,\n",
    "    image_size,\n",
    "    ConvertToMultiChannel_BackandForeground, \n",
    "):\n",
    "    \"\"\"\n",
    "    Generate training transform for the GAN.\n",
    "\n",
    "    ARGS:\n",
    "        image_key: the key to represent images in the input json files\n",
    "        label_key: the key to represent labels in the input json files\n",
    "        image_size: final image size for resizing \n",
    "\n",
    "    RETURN:\n",
    "        training transform for the GAN\n",
    "    \"\"\"\n",
    "    compute_dtype = torch.float32\n",
    "    transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[image_key, label_key], meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "            EnsureChannelFirstd(keys=[image_key, label_key]),\n",
    "            EnsureTyped(keys=[image_key, label_key], dtype=torch.float32),\n",
    "            Orientationd(keys=[image_key, label_key], axcodes=\"RAS\"),\n",
    "            ResizeWithPadOrCropd(\n",
    "                    keys=[image_key, label_key],\n",
    "                    spatial_size=image_size,\n",
    "                    mode=\"constant\",\n",
    "                    value=0\n",
    "                ),\n",
    "            #ScaleIntensityd(keys=[image_key], minv=-1, maxv=1),\n",
    "            ConvertToMultiChannel_BackandForeground(\n",
    "                keys=[label_key],\n",
    "            ),\n",
    "            EnsureTyped(keys=[image_key, label_key], dtype=compute_dtype)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def get_loader(IMAGE_SIZE, DATA_LIST_KEY, DATA_DIR):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        image_size: final image size for resizing \n",
    "        batch_size: Batch size\n",
    "        \n",
    "    RETURN:\n",
    "        train_loader: data loader\n",
    "        train_data: dict of the data loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Get train transforms\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannelBasedOnBratsClasses2023d\n",
    "    transforms = generate_detection_train_transform(\n",
    "            image_key = \"t1c\",\n",
    "            label_key = \"seg\",\n",
    "            image_size = IMAGE_SIZE,\n",
    "            ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground\n",
    "        )\n",
    "\n",
    "    # Get training data dict \n",
    "    data_set = load_decathlon_datalist(\n",
    "            DATA_LIST_FILE_PATH,\n",
    "            is_segmentation=True,\n",
    "            data_list_key=DATA_LIST_KEY,\n",
    "            base_dir=DATA_DIR,\n",
    "        )\n",
    "    print(data_set[0])\n",
    "    ds = CacheDataset(\n",
    "        data=data_set[:],\n",
    "        transform=transforms,\n",
    "        cache_rate=1,\n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        shuffle=False,\n",
    "        #collate_fn=no_collation,\n",
    "    )\n",
    "\n",
    "    return loader, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gen_infer(gen, data):\n",
    "    fake_image = gen(data)\n",
    "    return fake_image\n",
    "\n",
    "def get_affine(file_path):\n",
    "    nii_img = nib.load(file_path)\n",
    "    affine_matrix = nii_img.affine\n",
    "    return affine_matrix\n",
    "\n",
    "def save_nifti(data, reality, affine=None, save=None):\n",
    "    if affine is None:\n",
    "        affine = np.array([[1, 0, 0, 0],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 1, 0],  # Assuming 3 for the spacing along the third axis\n",
    "                   [0, 0, 0, 1]])\n",
    "    try:\n",
    "        np_fake = np.squeeze((data).data.cpu().numpy()).astype(np.float32)\n",
    "    except:\n",
    "        #print(\"Not torch!\")\n",
    "        np_fake = data\n",
    "    nifti_fake = nib.Nifti1Image(np_fake, affine=affine)\n",
    "    #plotting.plot_img(nifti_fake, title=reality, cut_coords=None, annotate=False, draw_cross=False, black_bg=True)\n",
    "    if save!=None:\n",
    "        nib.save(nifti_fake, save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(image, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Normalise the intensities into a new min and a new max \n",
    "    \"\"\"\n",
    "    # Assuming 'image' is a NumPy array with intensities in the range [-1, 1]\n",
    "    clipped_image = torch.clip(image, -1, 1)\n",
    "    \n",
    "    # Define the original range\n",
    "    original_min = -1\n",
    "    original_max = 1\n",
    "    \n",
    "    # Perform linear transformation to the new range\n",
    "    normalized_image = (clipped_image - original_min) / (original_max - original_min) * (new_max - new_min) + new_min\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def post_processing(fake_image, seg, ct_scan):\n",
    "    \"\"\"\n",
    "    Performing post processing to the generated cases.\n",
    "    Normalise intensity and crop.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    fake_image_np = fake_image[0][0]\n",
    "    ct_scan_np = ct_scan[0][0]\n",
    "\n",
    "    # Converting segmentation back from regions \n",
    "    seg_0 = seg[0][0]\n",
    "    seg_1 = seg[0][1]\n",
    "    seg_2 = seg[0][2]\n",
    "    final_seg = seg_0 + seg_1 + seg_2\n",
    "    new_seg = torch.zeros_like(final_seg)\n",
    "    new_seg[final_seg==2] = 1\n",
    "    new_seg[final_seg==1] = 2\n",
    "    new_seg[final_seg==3] = 3\n",
    "\n",
    "    # Normalise intensities\n",
    "    fake_image_np_norm = normalize_intensity(image=fake_image_np, new_min=ct_scan_np.min(), new_max=ct_scan_np.max())\n",
    "    \n",
    "    # Cropping\n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = 8, -8, 8, -8, 50, -51\n",
    "    cropped_ct_scan = ct_scan_np[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "    cropped_fake_scan = fake_image_np_norm[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "    cropped_seg = new_seg[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "\n",
    "    # Flipping to have the same orientation as the original cases\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=1)\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=0)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=1)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=0)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=1)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=0)\n",
    "    \n",
    "    return cropped_ct_scan, cropped_fake_scan, cropped_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t1n': '/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00266-000/BraTS-GLI-00266-000-t1n.nii.gz', 't1c': '/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00266-000/BraTS-GLI-00266-000-t1c.nii.gz', 't2w': '/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00266-000/BraTS-GLI-00266-000-t2w.nii.gz', 't2f': '/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00266-000/BraTS-GLI-00266-000-t2f.nii.gz', 'seg': '/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00266-000/BraTS-GLI-00266-000-seg.nii.gz'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:21<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "loader, ds = get_loader(IMAGE_SIZE=IMAGE_SIZE, DATA_LIST_KEY=DATA_LIST_KEY, DATA_DIR=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: ../../checkpoint/style_256/BraTS_W_PWA100__W_PWT100__Unet_FC_new, 999\n"
     ]
    }
   ],
   "source": [
    "gen = get_gen(checkpoint_path=CHECKPOINT_PATH, RESUME=RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_make_dir(directory=SAVE_DIR)\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"imagesTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr_origin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_transforms(prob, label_key):\n",
    "    affine_transforms = Compose(\n",
    "            [\n",
    "                # The label will:\n",
    "                    # Rotate between -180 and +180 degrees\n",
    "                    # Shear between -10% and +10%\n",
    "                    # Flip in all 3 axis \n",
    "                RandAffined(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    rotate_range = np.pi, \n",
    "                    shear_range = 0.1, \n",
    "                    #translate_range = 50, \n",
    "                    mode = \"nearest\",\n",
    "                    padding_mode = \"zeros\"\n",
    "                    ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=0\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=1\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=2\n",
    "                ),\n",
    "                RandZoomd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    min_zoom = 0.9,\n",
    "                    max_zoom = 1.1,\n",
    "                    mode = \"nearest\",\n",
    "                ),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    translate_transforms = Compose(\n",
    "            [\n",
    "                    # Shift between -50 and +50 voxels in all 3 axis\n",
    "                RandAffined(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    translate_range = 10, \n",
    "                    mode = \"nearest\",\n",
    "                    padding_mode = \"zeros\"\n",
    "                    ),\n",
    "                EnsureTyped(keys=[label_key], dtype=torch.float32)\n",
    "            ]\n",
    "        )\n",
    "    return affine_transforms, translate_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordenates_cropping_label(label):\n",
    "    # Find non-background coordinates\n",
    "    non_background_coords = np.where(label != 0)\n",
    "\n",
    "    # Determine the cropping bounds\n",
    "    min_x, max_x = np.min(non_background_coords[0]), np.max(non_background_coords[0])\n",
    "    min_y, max_y = np.min(non_background_coords[1]), np.max(non_background_coords[1])\n",
    "    min_z, max_z = np.min(non_background_coords[2]), np.max(non_background_coords[2])\n",
    "\n",
    "    return min_x, max_x, min_y, max_y, min_z, max_z\n",
    "    \n",
    "def get_origin_center_coord(min_x, max_x, min_y, max_y, min_z, max_z):\n",
    "    # Returns the center\n",
    "    center_x = (min_x + max_x) // 2\n",
    "    center_y = (min_y + max_y) // 2\n",
    "    center_z = (min_z + max_z) // 2\n",
    "\n",
    "    return center_x, center_y, center_z\n",
    "\n",
    "# This portion of code rotates the tumour and places it again the in same place\n",
    "def paste_rotated_seg(trans_seg, center_x, center_y, center_z):\n",
    "    # Cropping the rotated and sheared tumour\n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = coordenates_cropping_label(torch.sum(trans_seg[0], dim=0).numpy())\n",
    "    cropped_seg = trans_seg[:, :, min_x:max_x+1, min_y:max_y+1, min_z:max_z+1] # shape [1,3,x,y,z]\n",
    "\n",
    "    x, y, z = cropped_seg.shape[2], cropped_seg.shape[3], cropped_seg.shape[4]\n",
    "\n",
    "    new_seg = torch.zeros_like(seg)\n",
    "    # Calculate the bounds for placing the new tumour\n",
    "    start_x = max(0, center_x - x // 2)\n",
    "    end_x = min(seg.shape[2], center_x + (x + 1) // 2)\n",
    "\n",
    "    start_y = max(0, center_y - y // 2)\n",
    "    end_y = min(seg.shape[3], center_y + (y + 1) // 2)\n",
    "\n",
    "    start_z = max(0, center_z - z // 2)\n",
    "    end_z = min(seg.shape[4], center_z + (z + 1) // 2)\n",
    "\n",
    "    # Placing the new tumour with the same center \n",
    "    new_seg[:, :, start_x:end_x, start_y:end_y, start_z:end_z] = cropped_seg\n",
    "    return new_seg\n",
    "\n",
    "def apply_rotate_and_shear(seg, affine_transforms):\n",
    "    # Geting center of tumour \n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = coordenates_cropping_label(torch.sum(seg[0], dim=0).numpy())\n",
    "    center_x, center_y, center_z = get_origin_center_coord(min_x, max_x, min_y, max_y, min_z, max_z)\n",
    "\n",
    "    # Applying transforms\n",
    "    in_trans = {\"seg\": seg[0]}\n",
    "    trans_batch = affine_transforms(in_trans)\n",
    "    trans_seg = trans_batch[\"seg\"].unsqueeze(0)\n",
    "\n",
    "    new_seg = paste_rotated_seg(trans_seg, center_x, center_y, center_z)\n",
    "    return new_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the same number of cases as the original dataset (1000)\n",
    "* No transformation is used in the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▊                                                                                                          | 10/147 [00:25<05:46,  2.53s/it, Case=BraTS-GLI-00523-000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Saving synthetic scan\u001b[39;00m\n\u001b[1;32m     21\u001b[0m save_path \u001b[38;5;241m=\u001b[39m join(SAVE_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagesTr/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_0000.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43msave_nifti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropped_fake_scan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFake\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maffine_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Saving segmentation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m save_path \u001b[38;5;241m=\u001b[39m join(SAVE_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabelsTr_origin/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36msave_nifti\u001b[0;34m(data, reality, affine, save)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#plotting.plot_img(nifti_fake, title=reality, cut_coords=None, annotate=False, draw_cross=False, black_bg=True)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnifti_fake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/loadsave.py:163\u001b[0m, in \u001b[0;36msave\u001b[0;34m(img, filename, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Save the type as expected\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ImageFileError:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/filebasedimages.py:307\u001b[0m, in \u001b[0;36mFileBasedImage.to_filename\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Write image to files implied by filename string\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilespec_to_file_map(filename)\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/nifti1.py:2217\u001b[0m, in \u001b[0;36mNifti1Pair.to_file_map\u001b[0;34m(self, file_map, dtype)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_dtype(finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2217\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_data_dtype(img_dtype)\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/analyze.py:1051\u001b[0m, in \u001b[0;36mAnalyzeImage.to_file_map\u001b[0;34m(self, file_map, dtype)\u001b[0m\n\u001b[1;32m   1049\u001b[0m seek_tell(imgf, hdr\u001b[38;5;241m.\u001b[39mget_data_offset(), write0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# Write array data\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[43marr_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m hdrf\u001b[38;5;241m.\u001b[39mclose_if_mine()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hdr_img_same:\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/arraywriters.py:525\u001b[0m, in \u001b[0;36mSlopeInterArrayWriter.to_fileobj\u001b[0;34m(self, fileobj, order)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write array into `fileobj`\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    order (Fortran or C) to which to write array\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m mn, mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing_range()\n\u001b[0;32m--> 525\u001b[0m \u001b[43marray_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_out_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdivslope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan2zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_needs_nan2zero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/volumeutils.py:605\u001b[0m, in \u001b[0;36marray_to_file\u001b[0;34m(data, fileobj, out_dtype, offset, intercept, divslope, mn, mx, order, nan2zero)\u001b[0m\n\u001b[1;32m    603\u001b[0m     pre_clips \u001b[38;5;241m=\u001b[39m _dt_min_max(in_dtype, \u001b[38;5;241m*\u001b[39mpre_clips)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m null_scaling \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(in_dtype, out_dtype):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_write_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_clips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_clips\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Force upcasting for floats by making atleast_1d.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m slope, inter \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39matleast_1d(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (divslope, intercept))\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/volumeutils.py:784\u001b[0m, in \u001b[0;36m_write_data\u001b[0;34m(data, fileobj, out_dtype, order, in_cast, pre_clips, inter, slope, post_clips, nan_fill)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dslice\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m out_dtype:\n\u001b[1;32m    783\u001b[0m     dslice \u001b[38;5;241m=\u001b[39m dslice\u001b[38;5;241m.\u001b[39mastype(out_dtype)\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdslice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/site-packages/nibabel/openers.py:232\u001b[0m, in \u001b[0;36mOpener.write\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, b: \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/andre.ferreira/envs/wdm_publish/lib/python3.10/gzip.py:289\u001b[0m, in \u001b[0;36mGzipFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    286\u001b[0m     length \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m length\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcrc32(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###\n",
    "loop_train = tqdm(loader, leave=True)\n",
    "for batch_idx, batch in enumerate(loop_train):\n",
    "    with torch.no_grad():\n",
    "        ct_scan, seg = batch[\"t1c\"].to(DEVICE), batch[\"seg\"].to(DEVICE)\n",
    "\n",
    "        # Generating synthetic scan\n",
    "        fake_image = do_gen_infer(gen=gen, data=seg)\n",
    "\n",
    "        # Normalising synthetic scan intensity to the same values as the original case, \n",
    "        # and cropping to the same shape\n",
    "        cropped_ct_scan, cropped_fake_scan, cropped_seg = post_processing(fake_image, seg, ct_scan)\n",
    "\n",
    "        # Get affine \n",
    "        ct_path = batch[\"t1c_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        seg_path = batch[\"seg_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        ct_name = f\"{ct_path.split('/')[-1].split('-t1c')[0]}\"\n",
    "        affine_matrix = get_affine(ct_path)\n",
    "\n",
    "        # Saving synthetic scan\n",
    "        save_path = join(SAVE_DIR, f\"imagesTr/{ct_name}_0000.nii.gz\")\n",
    "        save_nifti(data=cropped_fake_scan, reality=\"Fake\", affine=affine_matrix, save=save_path)\n",
    "        # Saving segmentation\n",
    "        save_path = join(SAVE_DIR, f\"labelsTr_origin/{ct_name}.nii.gz\")\n",
    "        save_nifti(data=cropped_seg, reality=\"Fake\", affine=affine_matrix, save=save_path)\n",
    "\n",
    "    loop_train.set_postfix(\n",
    "        Case = ct_name,\n",
    "    )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wdm_publish)",
   "language": "python",
   "name": "wdm_publish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
