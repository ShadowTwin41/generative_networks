{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"BraTS_W_PWA100__W_PWT100__Unet_FC_new\"\n",
    "UNET = \"Unet_FC\" # Unet or Unet_FC\n",
    "RESUME = 990\n",
    "DEVICE = \"cuda:0\"\n",
    "HOME_DIR = \"/projects\"\n",
    "WORK_DIR = \"/projects\"\n",
    "\n",
    "from os.path import join\n",
    "# Fix variablles #\n",
    "DATA_LIST_KEY = \"training\"\n",
    "DATA_LIST_FILE_PATH = join(WORK_DIR, \"aritifcial-head-and-neck-cts/GANs/data/BraTS2023_GLI_data_split.json\") # Path where to save the json file \n",
    "DATA_DIR = \"../../brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\" # WORK_DIR: Working dir\n",
    "CHECKPOINT_PATH = f\"../../checkpoint/style_256/{EXPERIMENT_NAME}\"\n",
    "IMAGE_SIZE = (256, 256, 256)\n",
    "DIM = 1024\n",
    "NOISE_DIM = 512\n",
    "IN_CHANNEL_G = 3\n",
    "OUT_CHANNEL_G = 1\n",
    "SKIP_LATENT = False\n",
    "TAHN_ACT = False\n",
    "SAVE_DIR = \"../../nnUNet/nnUNet_raw/Dataset996_BraTS_GAN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists, dirname, basename\n",
    "from os import listdir, makedirs, environ\n",
    "def maybe_make_dir(directory):\n",
    "    if not exists(directory):\n",
    "        # If it doesn't exist, create the directory\n",
    "        makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from tqdm import tqdm\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset, Dataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    CropForegroundd,\n",
    "    Invertd,\n",
    "    RandZoomd,\n",
    "    \n",
    ")\n",
    "\n",
    "sys.path.insert(1, join(HOME_DIR, \"aritifcial-head-and-neck-cts/GANs/src\"))\n",
    "from utils.data_loader_utils import ConvertToMultiChannelBasedOnBratsClasses2023d\n",
    "if UNET==\"Unet_FC\":\n",
    "    print(\"USING THE UNET Fully Connected LIKE GENERATOR\")\n",
    "    from network.cWGAN_Style_Unet_256_FC import Generator, Critic\n",
    "elif UNET==\"Unet\":\n",
    "    print(\"USING THE UNET LIKE GENERATOR\")\n",
    "    from network.cWGAN_Style_Unet_256 import Generator, Critic\n",
    "else:\n",
    "    print(\"WELCOME TO THE ERROR ZONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen(checkpoint_path, RESUME):\n",
    "    print(f\"Loading from: {checkpoint_path}, {RESUME}\")\n",
    "    gen = Generator(in_channels=DIM, latent_dim=NOISE_DIM, IN_CHANNEL_G=IN_CHANNEL_G, OUT_CHANNEL_G=OUT_CHANNEL_G, z_dim=NOISE_DIM, w_dim=NOISE_DIM, skip_latent=SKIP_LATENT, tahn_act=TAHN_ACT)\n",
    "    gen.to(DEVICE)\n",
    "    gen_weight_path = join(checkpoint_path, \"weights\", f\"{RESUME}_gen.pth\")\n",
    "    checkpoint = torch.load(gen_weight_path, map_location=torch.device(DEVICE))\n",
    "    # Load the model's state dictionary\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "def generate_detection_train_transform(\n",
    "    image_key,\n",
    "    label_key,\n",
    "    image_size,\n",
    "    ConvertToMultiChannel_BackandForeground, \n",
    "):\n",
    "    \"\"\"\n",
    "    Generate training transform for the GAN.\n",
    "\n",
    "    ARGS:\n",
    "        image_key: the key to represent images in the input json files\n",
    "        label_key: the key to represent labels in the input json files\n",
    "        image_size: final image size for resizing \n",
    "\n",
    "    RETURN:\n",
    "        training transform for the GAN\n",
    "    \"\"\"\n",
    "    compute_dtype = torch.float32\n",
    "    transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[image_key, label_key], meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "            EnsureChannelFirstd(keys=[image_key, label_key]),\n",
    "            EnsureTyped(keys=[image_key, label_key], dtype=torch.float32),\n",
    "            Orientationd(keys=[image_key, label_key], axcodes=\"RAS\"),\n",
    "            ResizeWithPadOrCropd(\n",
    "                    keys=[image_key, label_key],\n",
    "                    spatial_size=image_size,\n",
    "                    mode=\"constant\",\n",
    "                    value=0\n",
    "                ),\n",
    "            #ScaleIntensityd(keys=[image_key], minv=-1, maxv=1),\n",
    "            ConvertToMultiChannel_BackandForeground(\n",
    "                keys=[label_key],\n",
    "            ),\n",
    "            EnsureTyped(keys=[image_key, label_key], dtype=compute_dtype)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def get_loader(IMAGE_SIZE, DATA_LIST_KEY, DATA_DIR):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        image_size: final image size for resizing \n",
    "        batch_size: Batch size\n",
    "        \n",
    "    RETURN:\n",
    "        train_loader: data loader\n",
    "        train_data: dict of the data loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Get train transforms\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannelBasedOnBratsClasses2023d\n",
    "    transforms = generate_detection_train_transform(\n",
    "            image_key = \"t1c\",\n",
    "            label_key = \"seg\",\n",
    "            image_size = IMAGE_SIZE,\n",
    "            ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground\n",
    "        )\n",
    "\n",
    "    # Get training data dict \n",
    "    data_set = load_decathlon_datalist(\n",
    "            DATA_LIST_FILE_PATH,\n",
    "            is_segmentation=True,\n",
    "            data_list_key=DATA_LIST_KEY,\n",
    "            base_dir=DATA_DIR,\n",
    "        )\n",
    "    print(data_set[0])\n",
    "    ds = CacheDataset(\n",
    "        data=data_set[:],\n",
    "        transform=transforms,\n",
    "        cache_rate=1,\n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        shuffle=False,\n",
    "        #collate_fn=no_collation,\n",
    "    )\n",
    "\n",
    "    return loader, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gen_infer(gen, data):\n",
    "    fake_image = gen(data)\n",
    "    return fake_image\n",
    "\n",
    "def get_affine(file_path):\n",
    "    nii_img = nib.load(file_path)\n",
    "    affine_matrix = nii_img.affine\n",
    "    return affine_matrix\n",
    "\n",
    "def save_nifti(data, reality, affine=None, save=None):\n",
    "    if affine is None:\n",
    "        affine = np.array([[1, 0, 0, 0],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 1, 0],  # Assuming 3 for the spacing along the third axis\n",
    "                   [0, 0, 0, 1]])\n",
    "    try:\n",
    "        np_fake = np.squeeze((data).data.cpu().numpy()).astype(np.float32)\n",
    "    except:\n",
    "        #print(\"Not torch!\")\n",
    "        np_fake = data\n",
    "    nifti_fake = nib.Nifti1Image(np_fake, affine=affine)\n",
    "    #plotting.plot_img(nifti_fake, title=reality, cut_coords=None, annotate=False, draw_cross=False, black_bg=True)\n",
    "    if save!=None:\n",
    "        nib.save(nifti_fake, save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(image, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Normalise the intensities into a new min and a new max \n",
    "    \"\"\"\n",
    "    # Assuming 'image' is a NumPy array with intensities in the range [-1, 1]\n",
    "    clipped_image = torch.clip(image, -1, 1)\n",
    "    \n",
    "    # Define the original range\n",
    "    original_min = -1\n",
    "    original_max = 1\n",
    "    \n",
    "    # Perform linear transformation to the new range\n",
    "    normalized_image = (clipped_image - original_min) / (original_max - original_min) * (new_max - new_min) + new_min\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def post_processing(fake_image, seg, ct_scan):\n",
    "    \"\"\"\n",
    "    Performing post processing to the generated cases.\n",
    "    Normalise intensity and crop.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    fake_image_np = fake_image[0][0]\n",
    "    ct_scan_np = ct_scan[0][0]\n",
    "\n",
    "    # Converting segmentation back from regions \n",
    "    seg_0 = seg[0][0]\n",
    "    seg_1 = seg[0][1]\n",
    "    seg_2 = seg[0][2]\n",
    "    final_seg = seg_0 + seg_1 + seg_2\n",
    "    new_seg = torch.zeros_like(final_seg)\n",
    "    new_seg[final_seg==2] = 1\n",
    "    new_seg[final_seg==1] = 2\n",
    "    new_seg[final_seg==3] = 3\n",
    "\n",
    "    # Normalise intensities\n",
    "    fake_image_np_norm = normalize_intensity(image=fake_image_np, new_min=ct_scan_np.min(), new_max=ct_scan_np.max())\n",
    "    \n",
    "    # Cropping\n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = 8, -8, 8, -8, 50, -51\n",
    "    cropped_ct_scan = ct_scan_np[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "    cropped_fake_scan = fake_image_np_norm[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "    cropped_seg = new_seg[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "\n",
    "    # Flipping to have the same orientation as the original cases\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=1)\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=0)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=1)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=0)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=1)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=0)\n",
    "    \n",
    "    return cropped_ct_scan, cropped_fake_scan, cropped_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, ds = get_loader(IMAGE_SIZE=IMAGE_SIZE, DATA_LIST_KEY=DATA_LIST_KEY, DATA_DIR=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = get_gen(checkpoint_path=CHECKPOINT_PATH, RESUME=RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_make_dir(directory=SAVE_DIR)\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"imagesTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr_origin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_transforms(prob, label_key):\n",
    "    affine_transforms = Compose(\n",
    "            [\n",
    "                # The label will:\n",
    "                    # Rotate between -180 and +180 degrees\n",
    "                    # Shear between -10% and +10%\n",
    "                    # Flip in all 3 axis \n",
    "                RandAffined(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    rotate_range = np.pi, \n",
    "                    shear_range = 0.1, \n",
    "                    #translate_range = 50, \n",
    "                    mode = \"nearest\",\n",
    "                    padding_mode = \"zeros\"\n",
    "                    ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=0\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=1\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob/2, \n",
    "                    spatial_axis=2\n",
    "                ),\n",
    "                RandZoomd(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    min_zoom = 0.9,\n",
    "                    max_zoom = 1.1,\n",
    "                    mode = \"nearest\",\n",
    "                ),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    translate_transforms = Compose(\n",
    "            [\n",
    "                    # Shift between -50 and +50 voxels in all 3 axis\n",
    "                RandAffined(\n",
    "                    keys = [label_key],\n",
    "                    prob = prob, \n",
    "                    translate_range = 10, \n",
    "                    mode = \"nearest\",\n",
    "                    padding_mode = \"zeros\"\n",
    "                    ),\n",
    "                EnsureTyped(keys=[label_key], dtype=torch.float32)\n",
    "            ]\n",
    "        )\n",
    "    return affine_transforms, translate_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordenates_cropping_label(label):\n",
    "    # Find non-background coordinates\n",
    "    non_background_coords = np.where(label != 0)\n",
    "\n",
    "    # Determine the cropping bounds\n",
    "    min_x, max_x = np.min(non_background_coords[0]), np.max(non_background_coords[0])\n",
    "    min_y, max_y = np.min(non_background_coords[1]), np.max(non_background_coords[1])\n",
    "    min_z, max_z = np.min(non_background_coords[2]), np.max(non_background_coords[2])\n",
    "\n",
    "    return min_x, max_x, min_y, max_y, min_z, max_z\n",
    "    \n",
    "def get_origin_center_coord(min_x, max_x, min_y, max_y, min_z, max_z):\n",
    "    # Returns the center\n",
    "    center_x = (min_x + max_x) // 2\n",
    "    center_y = (min_y + max_y) // 2\n",
    "    center_z = (min_z + max_z) // 2\n",
    "\n",
    "    return center_x, center_y, center_z\n",
    "\n",
    "# This portion of code rotates the tumour and places it again the in same place\n",
    "def paste_rotated_seg(trans_seg, center_x, center_y, center_z):\n",
    "    # Cropping the rotated and sheared tumour\n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = coordenates_cropping_label(torch.sum(trans_seg[0], dim=0).numpy())\n",
    "    cropped_seg = trans_seg[:, :, min_x:max_x+1, min_y:max_y+1, min_z:max_z+1] # shape [1,3,x,y,z]\n",
    "\n",
    "    x, y, z = cropped_seg.shape[2], cropped_seg.shape[3], cropped_seg.shape[4]\n",
    "\n",
    "    new_seg = torch.zeros_like(seg)\n",
    "    # Calculate the bounds for placing the new tumour\n",
    "    start_x = max(0, center_x - x // 2)\n",
    "    end_x = min(seg.shape[2], center_x + (x + 1) // 2)\n",
    "\n",
    "    start_y = max(0, center_y - y // 2)\n",
    "    end_y = min(seg.shape[3], center_y + (y + 1) // 2)\n",
    "\n",
    "    start_z = max(0, center_z - z // 2)\n",
    "    end_z = min(seg.shape[4], center_z + (z + 1) // 2)\n",
    "\n",
    "    # Placing the new tumour with the same center \n",
    "    new_seg[:, :, start_x:end_x, start_y:end_y, start_z:end_z] = cropped_seg\n",
    "    return new_seg\n",
    "\n",
    "def apply_rotate_and_shear(seg, affine_transforms):\n",
    "    # Geting center of tumour \n",
    "    min_x, max_x, min_y, max_y, min_z, max_z = coordenates_cropping_label(torch.sum(seg[0], dim=0).numpy())\n",
    "    center_x, center_y, center_z = get_origin_center_coord(min_x, max_x, min_y, max_y, min_z, max_z)\n",
    "\n",
    "    # Applying transforms\n",
    "    in_trans = {\"seg\": seg[0]}\n",
    "    trans_batch = affine_transforms(in_trans)\n",
    "    trans_seg = trans_batch[\"seg\"].unsqueeze(0)\n",
    "\n",
    "    new_seg = paste_rotated_seg(trans_seg, center_x, center_y, center_z)\n",
    "    return new_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the same number of cases as the original dataset (1000)\n",
    "* No transformation is used in the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "loop_train = tqdm(loader, leave=True)\n",
    "for batch_idx, batch in enumerate(loop_train):\n",
    "    with torch.no_grad():\n",
    "        ct_scan, seg = batch[\"t1c\"].to(DEVICE), batch[\"seg\"].to(DEVICE)\n",
    "\n",
    "        # Generating synthetic scan\n",
    "        fake_image = do_gen_infer(gen=gen, data=seg)\n",
    "\n",
    "        # Normalising synthetic scan intensity to the same values as the original case, \n",
    "        # and cropping to the same shape\n",
    "        cropped_ct_scan, cropped_fake_scan, cropped_seg = post_processing(fake_image, seg, ct_scan)\n",
    "\n",
    "        # Get affine \n",
    "        ct_path = batch[\"t1c_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        seg_path = batch[\"seg_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        ct_name = f\"{ct_path.split('/')[-1].split('-t1c')[0]}\"\n",
    "        affine_matrix = get_affine(ct_path)\n",
    "\n",
    "        # Saving synthetic scan\n",
    "        save_path = join(SAVE_DIR, f\"imagesTr/{ct_name}_0000.nii.gz\")\n",
    "        save_nifti(data=cropped_fake_scan, reality=\"Fake\", affine=affine_matrix, save=save_path)\n",
    "        # Saving segmentation\n",
    "        save_path = join(SAVE_DIR, f\"labelsTr_origin/{ct_name}.nii.gz\")\n",
    "        save_nifti(data=cropped_seg, reality=\"Fake\", affine=affine_matrix, save=save_path)\n",
    "\n",
    "    loop_train.set_postfix(\n",
    "        Case = ct_name,\n",
    "    )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT_GAN_2",
   "language": "python",
   "name": "ct_gan_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
