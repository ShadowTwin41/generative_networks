{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"W_PWA1000__W_PWT100__Unet_FC_min200_200\"\n",
    "UNET = \"Unet_FC\" # Unet or Unet_FC\n",
    "RESUME = 999\n",
    "DEVICE = \"cuda:0\"\n",
    "HOME_DIR = \"../../..\"\n",
    "WORK_DIR = \"../../..\"\n",
    "IN_TYPE = \"Contrast\"\n",
    "\n",
    "from os.path import join\n",
    "# Fix variablles #\n",
    "DATA_LIST_KEY = \"training\"\n",
    "#DATA_LIST_KEY = \"test\"\n",
    "DATA_LIST_FILE_PATH = join(WORK_DIR, \"GANs/data/training.json\") # Path where to save the json file \n",
    "DATA_DIR = join(WORK_DIR, \"HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\")\n",
    "\n",
    "CHECKPOINT_PATH = f\"../../checkpoint/style_256/{EXPERIMENT_NAME}\"\n",
    "IMAGE_SIZE = (256, 256, 256)\n",
    "DIM = 1024\n",
    "NOISE_DIM = 512\n",
    "IN_CHANNEL_G = 3 # This should be changed in case of IN_TYPE = \"Noise\" or IN_TYPE = \"Blank\"\n",
    "OUT_CHANNEL_G = 1\n",
    "SKIP_LATENT = False\n",
    "TAHN_ACT = False\n",
    "\n",
    "### Saving the new cases\n",
    "SAVE_DIR = \"../../nnUNet_seg/nnUNet_raw/TMP/Dataset983_synthCT_HNC/\"\n",
    "\n",
    "from os.path import join, exists, dirname, basename\n",
    "from os import listdir, makedirs, environ\n",
    "def maybe_make_dir(directory):\n",
    "    if not exists(directory):\n",
    "        # If it doesn't exist, create the directory\n",
    "        makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING THE UNET Fully Connected LIKE GENERATOR\n",
      "Using ConvertToMultiChannel_BackandForeground_Contrastd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset, Dataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityd,\n",
    "    ScaleIntensityRanged\n",
    ")\n",
    "\n",
    "sys.path.insert(1, join(HOME_DIR, \"GANs/src\"))\n",
    "\n",
    "# Load the GAN\n",
    "if UNET==\"Unet_FC\":\n",
    "    print(\"USING THE UNET Fully Connected LIKE GENERATOR\")\n",
    "    from networks.cWGAN_Style_Unet_256_FC import Generator, Critic\n",
    "elif UNET==\"Unet\":\n",
    "    print(\"USING THE UNET LIKE GENERATOR\")\n",
    "    from networks.cWGAN_Style_Unet_256 import Generator, Critic\n",
    "else:\n",
    "    print(\"WELCOME TO THE ERROR ZONE\")\n",
    "\n",
    "# Load the segmentation transformer\n",
    "if IN_TYPE == \"Noise\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_blankd and adding noise\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_blankd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_blankd\n",
    "elif IN_TYPE == \"Contrast\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_Contrastd\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "elif  IN_TYPE == \"Blank\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_blankd\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_blankd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_blankd\n",
    "elif IN_TYPE == \"Contrast_Noise_Tumour\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_Contrastd\")\n",
    "    print(\"Using Contrast_Noise_Tumour\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_Contrastd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen(checkpoint_path, RESUME):\n",
    "    print(f\"Loading from: {checkpoint_path}\")\n",
    "    gen = Generator(in_channels=DIM, latent_dim=NOISE_DIM, IN_CHANNEL_G=IN_CHANNEL_G, OUT_CHANNEL_G=OUT_CHANNEL_G, z_dim=NOISE_DIM, w_dim=NOISE_DIM, skip_latent=SKIP_LATENT, tahn_act=TAHN_ACT)\n",
    "    gen.to(DEVICE)\n",
    "    gen_weight_path = join(checkpoint_path, \"weights\", f\"{RESUME}_gen.pth\")\n",
    "    checkpoint = torch.load(gen_weight_path, map_location=torch.device(DEVICE))\n",
    "    # Load the model's state dictionary\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "def generate_detection_train_transform(\n",
    "    image_key,\n",
    "    label_key,\n",
    "    image_size,\n",
    "    ConvertToMultiChannel_BackandForeground, \n",
    "):\n",
    "    \"\"\"\n",
    "    Generate training transform for the GAN.\n",
    "\n",
    "    ARGS:\n",
    "        image_key: the key to represent images in the input json files\n",
    "        label_key: the key to represent labels in the input json files\n",
    "        image_size: final image size for resizing \n",
    "\n",
    "    RETURN:\n",
    "        training transform for the GAN\n",
    "    \"\"\"\n",
    "    compute_dtype = torch.float32\n",
    "    transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[image_key, label_key], meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "                EnsureChannelFirstd(keys=[image_key, label_key]),\n",
    "                EnsureTyped(keys=[image_key, label_key], dtype=torch.float32),\n",
    "                Orientationd(keys=[image_key, label_key], axcodes=\"RAS\"),\n",
    "                ScaleIntensityRanged(keys=[image_key], a_min=-200, a_max=200.0, b_min=-1.0, b_max=1.0, clip=True), # The values than -200 are clipped to -200 as well as greater than 300 clipped to 300.\n",
    "                ResizeWithPadOrCropd(\n",
    "                    keys=[image_key, label_key],\n",
    "                    spatial_size=image_size,\n",
    "                    mode=\"constant\",\n",
    "                    value=-1\n",
    "                ),\n",
    "                ConvertToMultiChannel_BackandForeground(\n",
    "                    keys=[label_key],\n",
    "                ),\n",
    "                EnsureTyped(keys=[image_key, label_key], dtype=torch.float32)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def get_loader(IMAGE_SIZE, DATA_LIST_KEY, DATA_DIR):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        image_size: final image size for resizing \n",
    "        batch_size: Batch size\n",
    "        \n",
    "    RETURN:\n",
    "        train_loader: data loader\n",
    "        train_data: dict of the data loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Get train transforms\n",
    "    transforms = generate_detection_train_transform(\n",
    "            image_key = \"image\",\n",
    "            label_key = \"seg\",\n",
    "            image_size = IMAGE_SIZE,\n",
    "            ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground\n",
    "        )\n",
    "\n",
    "    # Get training data dict \n",
    "    data_set = load_decathlon_datalist(\n",
    "            DATA_LIST_FILE_PATH,\n",
    "            is_segmentation=True,\n",
    "            data_list_key=DATA_LIST_KEY,\n",
    "            base_dir=DATA_DIR,\n",
    "        )\n",
    "    print(data_set[0])\n",
    "    ds = CacheDataset(\n",
    "        data=data_set[:],\n",
    "        transform=transforms,\n",
    "        cache_rate=1,\n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        shuffle=False,\n",
    "        #collate_fn=no_collation,\n",
    "    )\n",
    "\n",
    "    return loader, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gen_infer(gen, data):\n",
    "    fake_image = gen(data)\n",
    "    return fake_image\n",
    "\n",
    "def get_affine_header(file_path):\n",
    "    nii_img = nib.load(file_path)\n",
    "    affine_matrix = nii_img.affine\n",
    "    header_info = nii_img.header\n",
    "    return affine_matrix, header_info\n",
    "\n",
    "def save_nifti(data, reality, affine=None, header_info=None,  save=None):\n",
    "    if affine is None:\n",
    "        affine = np.array([[1, 0, 0, 0],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 1, 0],  # Assuming 3 for the spacing along the third axis\n",
    "                   [0, 0, 0, 1]])\n",
    "    try:\n",
    "        np_fake = np.squeeze((data).data.cpu().numpy()).astype(np.float32)\n",
    "    except:\n",
    "        #print(\"Not torch!\")\n",
    "        np_fake = data\n",
    "    nifti_fake = nib.Nifti1Image(np_fake, affine=affine, header=header_info)\n",
    "    #plotting.plot_img(nifti_fake, title=reality, cut_coords=None, annotate=False, draw_cross=False, black_bg=True)\n",
    "    if save!=None:\n",
    "        nib.save(nifti_fake, save)\n",
    "\n",
    "def save_nifti_with_metadata(exist_nii_path, new_numpy_array, save_path):\n",
    "    existing_nii_file = nib.load(exist_nii_path)\n",
    "\n",
    "    metadata = existing_nii_file.header\n",
    "    affine = existing_nii_file.affine\n",
    "\n",
    "    new_image = nib.Nifti1Image(new_numpy_array, affine, metadata)\n",
    "    #new_image = nib.Nifti1Image(new_numpy_array, affine)\n",
    "    nib.save(new_image, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(image, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Normalise the intensities into a new min and a new max \n",
    "    \"\"\"\n",
    "    # Assuming 'image' is a NumPy array with intensities in the range [-1, 1]\n",
    "    clipped_image = np.clip(image, -1, 1)\n",
    "    \n",
    "    # Define the original range\n",
    "    original_min = -1\n",
    "    original_max = 1\n",
    "    \n",
    "    # Perform linear transformation to the new range\n",
    "    normalized_image = (clipped_image - original_min) / (original_max - original_min) * (new_max - new_min) + new_min\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def post_processing(fake_image, seg, ct_scan, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Performing post processing to the generated cases.\n",
    "    Normalise intensity and crop.\n",
    "    \"\"\"\n",
    "    fake_image_np = fake_image[0][0]#.cpu().numpy()\n",
    "    ct_scan_np = ct_scan[0][0]#.cpu().numpy()\n",
    "    \n",
    "    if seg.shape[1]==3:\n",
    "        # In case the segmentation is composed of 3 channels\n",
    "        background_0 = seg[0][0].cpu().numpy()\n",
    "        background_1 = seg[0][1].cpu().numpy()\n",
    "        binary_seg = seg[0][2].cpu().numpy()\n",
    "        if np.sum(background_0)!=0:\n",
    "            #print(\"NO CONTRAST\")\n",
    "            binary_mask = background_0\n",
    "        elif np.sum(background_1)!=0:\n",
    "            #print(\"CONTRAST\")\n",
    "            binary_mask = background_1\n",
    "        else:\n",
    "            #print(f\"All background is zero!\")\n",
    "            binary_mask = np.ones_like(binary_seg)\n",
    "    elif seg.shape[1]==2:\n",
    "        # In case the segmentation is composed of 2 channels  \n",
    "        background = seg[0][0].cpu().numpy()\n",
    "        binary_seg = seg[0][1].cpu().numpy()\n",
    "        binary_mask = background\n",
    "\n",
    "    # Normalise the scan intensities \n",
    "    fake_image_np_norm = normalize_intensity(image=fake_image_np, new_min=new_min, new_max=new_max)\n",
    "    \n",
    "    # Find connected components in the binary mask\n",
    "    labeled_mask, num_components = label(binary_mask)\n",
    "    # Assume that the region of interest is the largest connected component\n",
    "    largest_component = np.argmax(np.bincount(labeled_mask.flat)[1:]) + 1\n",
    "    # Extract the bounding box of the largest connected component\n",
    "    indices = np.where(labeled_mask == largest_component)\n",
    "    min_x, max_x = np.min(indices[0]), np.max(indices[0])\n",
    "    min_y, max_y = np.min(indices[1]), np.max(indices[1])\n",
    "    min_z, max_z = np.min(indices[2]), np.max(indices[2])\n",
    "\n",
    "    cropped_fake_scan = fake_image_np_norm[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1]\n",
    "    cropped_seg = binary_seg[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1]\n",
    "    cropped_ct_scan = ct_scan_np[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1] # TO remove\n",
    "\n",
    "    # Flipping to have the same orientation as the original cases\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=1)\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=0)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=1)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=0)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=1)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=0)\n",
    "    \n",
    "    return cropped_ct_scan, cropped_fake_scan, cropped_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': '../../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data/anderson_0a908279226c5229e7fe85b8894b62d5.nii.gz', 'seg': '../../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/seg/anderson_74a6346e2ea586bf3837b37a8165f7fa.nii.gz', 'contrast': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|                                                                                                                                    | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:35<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "loader, ds = get_loader(IMAGE_SIZE=IMAGE_SIZE, DATA_LIST_KEY=DATA_LIST_KEY, DATA_DIR=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: ../../checkpoint/style_256/W_PWA100__W_PWT10__Unet_FC_min200_200\n"
     ]
    }
   ],
   "source": [
    "gen = get_gen(checkpoint_path=CHECKPOINT_PATH, RESUME=RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_make_dir(directory=SAVE_DIR)\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"imagesTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                     | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                    | 6/150 [00:28<10:39,  4.44s/it, Case=synt_anderson_0a908279226c5229e7fe85b8894b62d5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                    | 6/150 [00:33<13:27,  5.61s/it, Case=synt_anderson_0a908279226c5229e7fe85b8894b62d5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Saving segmentation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m join(SAVE_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabelsTr/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     \u001b[43msave_nifti_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexist_nii_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_numpy_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropped_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#print(f\"Ignored: {ct_name}\")\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m, in \u001b[0;36msave_nifti_with_metadata\u001b[0;34m(exist_nii_path, new_numpy_array, save_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m new_image \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mNifti1Image(new_numpy_array, affine, metadata)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#new_image = nib.Nifti1Image(new_numpy_array, affine)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/loadsave.py:163\u001b[0m, in \u001b[0;36msave\u001b[0;34m(img, filename, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Save the type as expected\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ImageFileError:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/filebasedimages.py:307\u001b[0m, in \u001b[0;36mFileBasedImage.to_filename\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Write image to files implied by filename string\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilespec_to_file_map(filename)\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/nifti1.py:2217\u001b[0m, in \u001b[0;36mNifti1Pair.to_file_map\u001b[0;34m(self, file_map, dtype)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_dtype(finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2217\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_data_dtype(img_dtype)\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/analyze.py:1051\u001b[0m, in \u001b[0;36mAnalyzeImage.to_file_map\u001b[0;34m(self, file_map, dtype)\u001b[0m\n\u001b[1;32m   1049\u001b[0m seek_tell(imgf, hdr\u001b[38;5;241m.\u001b[39mget_data_offset(), write0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# Write array data\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[43marr_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m hdrf\u001b[38;5;241m.\u001b[39mclose_if_mine()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hdr_img_same:\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/arraywriters.py:525\u001b[0m, in \u001b[0;36mSlopeInterArrayWriter.to_fileobj\u001b[0;34m(self, fileobj, order)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write array into `fileobj`\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    order (Fortran or C) to which to write array\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m mn, mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing_range()\n\u001b[0;32m--> 525\u001b[0m \u001b[43marray_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_out_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdivslope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan2zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_needs_nan2zero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/volumeutils.py:705\u001b[0m, in \u001b[0;36marray_to_file\u001b[0;34m(data, fileobj, out_dtype, offset, intercept, divslope, mn, mx, order, nan2zero)\u001b[0m\n\u001b[1;32m    703\u001b[0m post_mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([post_mx, both_mx])\n\u001b[1;32m    704\u001b[0m in_cast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cast_in_dtype \u001b[38;5;241m==\u001b[39m in_dtype \u001b[38;5;28;01melse\u001b[39;00m cast_in_dtype\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_write_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_cast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_cast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_clips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_clips\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43minter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_clips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpost_mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_fill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnan2zero\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/volumeutils.py:784\u001b[0m, in \u001b[0;36m_write_data\u001b[0;34m(data, fileobj, out_dtype, order, in_cast, pre_clips, inter, slope, post_clips, nan_fill)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dslice\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m out_dtype:\n\u001b[1;32m    783\u001b[0m     dslice \u001b[38;5;241m=\u001b[39m dslice\u001b[38;5;241m.\u001b[39mastype(out_dtype)\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdslice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/site-packages/nibabel/openers.py:232\u001b[0m, in \u001b[0;36mOpener.write\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, b: \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/envs/conda/user/envs/wdm_publish/lib/python3.10/gzip.py:289\u001b[0m, in \u001b[0;36mGzipFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    286\u001b[0m     length \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m length\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcrc32(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loop_train = tqdm(loader, leave=True)\n",
    "for batch_idx, batch in enumerate(loop_train):\n",
    "    with torch.no_grad():\n",
    "        ct_scan, seg = batch[\"image\"].to(DEVICE), batch[\"seg\"].to(DEVICE)\n",
    "        ct_path = batch[\"image_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        seg_path = batch[\"seg_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        ct_name = f\"synt_{ct_path.split('/')[-1].split('.nii.gz')[0]}\"\n",
    "        #if \"0a908279226c5229e7fe85b8894b62d5\" in ct_name:\n",
    "       \n",
    "        if not torch.sum(seg[0][-1])==0:\n",
    "            seg_2 = torch.clone(seg)\n",
    "\n",
    "            #noise = torch.normal(mean=0.0, std=0.5, size=seg[0][1].shape)\n",
    "            #seg[0][1] = seg[0][1]*noise\n",
    "            if IN_TYPE == \"Contrast_Noise_Tumour\":\n",
    "                #print(f\"Contrast_Noise_Tumour\")\n",
    "                noise = torch.normal(mean=0.0, std=0.5, size=seg[0][2].shape).to(DEVICE)\n",
    "                seg[0][2] = seg[0][2]*noise\n",
    "\n",
    "            # Generating synthetic scan\n",
    "            fake_image = do_gen_infer(gen=gen, data=seg)\n",
    "\n",
    "            # Normalising synthetic scan intensity to the same values as the original case, \n",
    "            # and cropping to the same shape\n",
    "            cropped_ct_scan, cropped_fake_scan, cropped_seg = post_processing(fake_image, seg_2, ct_scan, new_min=-200, new_max=200)\n",
    "\n",
    "            # Saving synthetic scan\n",
    "            save_path = join(SAVE_DIR, f\"imagesTr/{ct_name}_0000.nii.gz\")\n",
    "            save_nifti_with_metadata(exist_nii_path=ct_path, new_numpy_array=cropped_fake_scan, save_path=save_path)\n",
    "            \n",
    "            # Saving segmentation\n",
    "            save_path = join(SAVE_DIR, f\"labelsTr/{ct_name}.nii.gz\")\n",
    "            save_nifti_with_metadata(exist_nii_path=seg_path, new_numpy_array=cropped_seg, save_path=save_path)\n",
    "        else:\n",
    "            #print(f\"Ignored: {ct_name}\")\n",
    "            pass\n",
    "        loop_train.set_postfix(\n",
    "            Case = ct_name,\n",
    "        )\n",
    "        #print(ct_path)\n",
    "        #print(seg_path)\n",
    "      \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wdm_publish)",
   "language": "python",
   "name": "wdm_publish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
