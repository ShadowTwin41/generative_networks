{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"W_PWA1000__W_PWT100__Unet_FC_min200_200\"\n",
    "UNET = \"Unet_FC\" # Unet or Unet_FC\n",
    "RESUME = 999\n",
    "DEVICE = \"cuda:0\"\n",
    "HOME_DIR = \"/projects\"\n",
    "WORK_DIR = \"/projects\"\n",
    "IN_TYPE = \"Contrast\"\n",
    "\n",
    "from os.path import join\n",
    "# Fix variablles #\n",
    "DATA_LIST_KEY = \"training\"\n",
    "#DATA_LIST_KEY = \"test\"\n",
    "DATA_LIST_FILE_PATH = join(WORK_DIR, \"aritifcial-head-and-neck-cts/GANs/data/training.json\") # Path where to save the json file \n",
    "DATA_DIR = join(WORK_DIR, \"HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\")\n",
    "\n",
    "CHECKPOINT_PATH = f\"../../checkpoint/style_256/{EXPERIMENT_NAME}\"\n",
    "IMAGE_SIZE = (256, 256, 256)\n",
    "DIM = 1024\n",
    "NOISE_DIM = 512\n",
    "IN_CHANNEL_G = 3 # This should be changed in case of IN_TYPE = \"Noise\" or IN_TYPE = \"Blank\"\n",
    "OUT_CHANNEL_G = 1\n",
    "SKIP_LATENT = False\n",
    "TAHN_ACT = False\n",
    "\n",
    "### Saving the new cases\n",
    "SAVE_DIR = \"../../nnUNet_seg/nnUNet_raw/TMP/Dataset983_synthCT_HNC/\"\n",
    "\n",
    "from os.path import join, exists, dirname, basename\n",
    "from os import listdir, makedirs, environ\n",
    "def maybe_make_dir(directory):\n",
    "    if not exists(directory):\n",
    "        # If it doesn't exist, create the directory\n",
    "        makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset, Dataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityd,\n",
    "    ScaleIntensityRanged\n",
    ")\n",
    "\n",
    "sys.path.insert(1, join(HOME_DIR, \"aritifcial-head-and-neck-cts/GANs/src\"))\n",
    "\n",
    "# Load the GAN\n",
    "if UNET==\"Unet_FC\":\n",
    "    print(\"USING THE UNET Fully Connected LIKE GENERATOR\")\n",
    "    from network.cWGAN_Style_Unet_256_FC import Generator, Critic\n",
    "elif UNET==\"Unet\":\n",
    "    print(\"USING THE UNET LIKE GENERATOR\")\n",
    "    from network.cWGAN_Style_Unet_256 import Generator, Critic\n",
    "else:\n",
    "    print(\"WELCOME TO THE ERROR ZONE\")\n",
    "\n",
    "# Load the segmentation transformer\n",
    "if IN_TYPE == \"Noise\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_blankd and adding noise\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_blankd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_blankd\n",
    "elif IN_TYPE == \"Contrast\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_Contrastd\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "elif  IN_TYPE == \"Blank\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_blankd\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_blankd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_blankd\n",
    "elif IN_TYPE == \"Contrast_Noise_Tumour\":\n",
    "    print(\"Using ConvertToMultiChannel_BackandForeground_Contrastd\")\n",
    "    print(\"Using Contrast_Noise_Tumour\")\n",
    "    from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "    ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground_Contrastd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen(checkpoint_path, RESUME):\n",
    "    print(f\"Loading from: {checkpoint_path}\")\n",
    "    gen = Generator(in_channels=DIM, latent_dim=NOISE_DIM, IN_CHANNEL_G=IN_CHANNEL_G, OUT_CHANNEL_G=OUT_CHANNEL_G, z_dim=NOISE_DIM, w_dim=NOISE_DIM, skip_latent=SKIP_LATENT, tahn_act=TAHN_ACT)\n",
    "    gen.to(DEVICE)\n",
    "    gen_weight_path = join(checkpoint_path, \"weights\", f\"{RESUME}_gen.pth\")\n",
    "    checkpoint = torch.load(gen_weight_path, map_location=torch.device(DEVICE))\n",
    "    # Load the model's state dictionary\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "def generate_detection_train_transform(\n",
    "    image_key,\n",
    "    label_key,\n",
    "    image_size,\n",
    "    ConvertToMultiChannel_BackandForeground, \n",
    "):\n",
    "    \"\"\"\n",
    "    Generate training transform for the GAN.\n",
    "\n",
    "    ARGS:\n",
    "        image_key: the key to represent images in the input json files\n",
    "        label_key: the key to represent labels in the input json files\n",
    "        image_size: final image size for resizing \n",
    "\n",
    "    RETURN:\n",
    "        training transform for the GAN\n",
    "    \"\"\"\n",
    "    compute_dtype = torch.float32\n",
    "    transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[image_key, label_key], meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "                EnsureChannelFirstd(keys=[image_key, label_key]),\n",
    "                EnsureTyped(keys=[image_key, label_key], dtype=torch.float32),\n",
    "                Orientationd(keys=[image_key, label_key], axcodes=\"RAS\"),\n",
    "                ScaleIntensityRanged(keys=[image_key], a_min=-200, a_max=200.0, b_min=-1.0, b_max=1.0, clip=True), # The values than -200 are clipped to -200 as well as greater than 300 clipped to 300.\n",
    "                ResizeWithPadOrCropd(\n",
    "                    keys=[image_key, label_key],\n",
    "                    spatial_size=image_size,\n",
    "                    mode=\"constant\",\n",
    "                    value=-1\n",
    "                ),\n",
    "                ConvertToMultiChannel_BackandForeground(\n",
    "                    keys=[label_key],\n",
    "                ),\n",
    "                EnsureTyped(keys=[image_key, label_key], dtype=torch.float32)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def get_loader(IMAGE_SIZE, DATA_LIST_KEY, DATA_DIR):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        image_size: final image size for resizing \n",
    "        batch_size: Batch size\n",
    "        \n",
    "    RETURN:\n",
    "        train_loader: data loader\n",
    "        train_data: dict of the data loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Get train transforms\n",
    "    transforms = generate_detection_train_transform(\n",
    "            image_key = \"image\",\n",
    "            label_key = \"seg\",\n",
    "            image_size = IMAGE_SIZE,\n",
    "            ConvertToMultiChannel_BackandForeground = ConvertToMultiChannel_BackandForeground\n",
    "        )\n",
    "\n",
    "    # Get training data dict \n",
    "    data_set = load_decathlon_datalist(\n",
    "            DATA_LIST_FILE_PATH,\n",
    "            is_segmentation=True,\n",
    "            data_list_key=DATA_LIST_KEY,\n",
    "            base_dir=DATA_DIR,\n",
    "        )\n",
    "    print(data_set[0])\n",
    "    ds = CacheDataset(\n",
    "        data=data_set[:],\n",
    "        transform=transforms,\n",
    "        cache_rate=1,\n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        shuffle=False,\n",
    "        #collate_fn=no_collation,\n",
    "    )\n",
    "\n",
    "    return loader, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gen_infer(gen, data):\n",
    "    fake_image = gen(data)\n",
    "    return fake_image\n",
    "\n",
    "def get_affine_header(file_path):\n",
    "    nii_img = nib.load(file_path)\n",
    "    affine_matrix = nii_img.affine\n",
    "    header_info = nii_img.header\n",
    "    return affine_matrix, header_info\n",
    "\n",
    "def save_nifti(data, reality, affine=None, header_info=None,  save=None):\n",
    "    if affine is None:\n",
    "        affine = np.array([[1, 0, 0, 0],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 1, 0],  # Assuming 3 for the spacing along the third axis\n",
    "                   [0, 0, 0, 1]])\n",
    "    try:\n",
    "        np_fake = np.squeeze((data).data.cpu().numpy()).astype(np.float32)\n",
    "    except:\n",
    "        #print(\"Not torch!\")\n",
    "        np_fake = data\n",
    "    nifti_fake = nib.Nifti1Image(np_fake, affine=affine, header=header_info)\n",
    "    #plotting.plot_img(nifti_fake, title=reality, cut_coords=None, annotate=False, draw_cross=False, black_bg=True)\n",
    "    if save!=None:\n",
    "        nib.save(nifti_fake, save)\n",
    "\n",
    "def save_nifti_with_metadata(exist_nii_path, new_numpy_array, save_path):\n",
    "    existing_nii_file = nib.load(exist_nii_path)\n",
    "\n",
    "    metadata = existing_nii_file.header\n",
    "    affine = existing_nii_file.affine\n",
    "\n",
    "    new_image = nib.Nifti1Image(new_numpy_array, affine, metadata)\n",
    "    #new_image = nib.Nifti1Image(new_numpy_array, affine)\n",
    "    nib.save(new_image, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(image, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Normalise the intensities into a new min and a new max \n",
    "    \"\"\"\n",
    "    # Assuming 'image' is a NumPy array with intensities in the range [-1, 1]\n",
    "    clipped_image = np.clip(image, -1, 1)\n",
    "    \n",
    "    # Define the original range\n",
    "    original_min = -1\n",
    "    original_max = 1\n",
    "    \n",
    "    # Perform linear transformation to the new range\n",
    "    normalized_image = (clipped_image - original_min) / (original_max - original_min) * (new_max - new_min) + new_min\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def post_processing(fake_image, seg, ct_scan, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Performing post processing to the generated cases.\n",
    "    Normalise intensity and crop.\n",
    "    \"\"\"\n",
    "    fake_image_np = fake_image[0][0]#.cpu().numpy()\n",
    "    ct_scan_np = ct_scan[0][0]#.cpu().numpy()\n",
    "    \n",
    "    if seg.shape[1]==3:\n",
    "        # In case the segmentation is composed of 3 channels\n",
    "        background_0 = seg[0][0].cpu().numpy()\n",
    "        background_1 = seg[0][1].cpu().numpy()\n",
    "        binary_seg = seg[0][2].cpu().numpy()\n",
    "        if np.sum(background_0)!=0:\n",
    "            #print(\"NO CONTRAST\")\n",
    "            binary_mask = background_0\n",
    "        elif np.sum(background_1)!=0:\n",
    "            #print(\"CONTRAST\")\n",
    "            binary_mask = background_1\n",
    "        else:\n",
    "            #print(f\"All background is zero!\")\n",
    "            binary_mask = np.ones_like(binary_seg)\n",
    "    elif seg.shape[1]==2:\n",
    "        # In case the segmentation is composed of 2 channels  \n",
    "        background = seg[0][0].cpu().numpy()\n",
    "        binary_seg = seg[0][1].cpu().numpy()\n",
    "        binary_mask = background\n",
    "\n",
    "    # Normalise the scan intensities \n",
    "    fake_image_np_norm = normalize_intensity(image=fake_image_np, new_min=new_min, new_max=new_max)\n",
    "    \n",
    "    # Find connected components in the binary mask\n",
    "    labeled_mask, num_components = label(binary_mask)\n",
    "    # Assume that the region of interest is the largest connected component\n",
    "    largest_component = np.argmax(np.bincount(labeled_mask.flat)[1:]) + 1\n",
    "    # Extract the bounding box of the largest connected component\n",
    "    indices = np.where(labeled_mask == largest_component)\n",
    "    min_x, max_x = np.min(indices[0]), np.max(indices[0])\n",
    "    min_y, max_y = np.min(indices[1]), np.max(indices[1])\n",
    "    min_z, max_z = np.min(indices[2]), np.max(indices[2])\n",
    "\n",
    "    cropped_fake_scan = fake_image_np_norm[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1]\n",
    "    cropped_seg = binary_seg[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1]\n",
    "    cropped_ct_scan = ct_scan_np[min_x:max_x + 1, min_y:max_y + 1, min_z:max_z + 1] # TO remove\n",
    "\n",
    "    # Flipping to have the same orientation as the original cases\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=1)\n",
    "    cropped_ct_scan = np.flip(cropped_ct_scan, axis=0)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=1)\n",
    "    cropped_fake_scan = np.flip(cropped_fake_scan, axis=0)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=1)\n",
    "    cropped_seg = np.flip(cropped_seg, axis=0)\n",
    "    \n",
    "    return cropped_ct_scan, cropped_fake_scan, cropped_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, ds = get_loader(IMAGE_SIZE=IMAGE_SIZE, DATA_LIST_KEY=DATA_LIST_KEY, DATA_DIR=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = get_gen(checkpoint_path=CHECKPOINT_PATH, RESUME=RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_make_dir(directory=SAVE_DIR)\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"imagesTr\"))\n",
    "maybe_make_dir(directory=join(SAVE_DIR, \"labelsTr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_train = tqdm(loader, leave=True)\n",
    "for batch_idx, batch in enumerate(loop_train):\n",
    "    with torch.no_grad():\n",
    "        ct_scan, seg = batch[\"image\"].to(DEVICE), batch[\"seg\"].to(DEVICE)\n",
    "        ct_path = batch[\"image_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        seg_path = batch[\"seg_meta_dict\"][\"filename_or_obj\"][0]\n",
    "        ct_name = f\"synt_{ct_path.split('/')[-1].split('.nii.gz')[0]}\"\n",
    "        #if \"0a908279226c5229e7fe85b8894b62d5\" in ct_name:\n",
    "       \n",
    "        if not torch.sum(seg[0][-1])==0:\n",
    "            seg_2 = torch.clone(seg)\n",
    "\n",
    "            #noise = torch.normal(mean=0.0, std=0.5, size=seg[0][1].shape)\n",
    "            #seg[0][1] = seg[0][1]*noise\n",
    "            if IN_TYPE == \"Contrast_Noise_Tumour\":\n",
    "                #print(f\"Contrast_Noise_Tumour\")\n",
    "                noise = torch.normal(mean=0.0, std=0.5, size=seg[0][2].shape).to(DEVICE)\n",
    "                seg[0][2] = seg[0][2]*noise\n",
    "\n",
    "            # Generating synthetic scan\n",
    "            fake_image = do_gen_infer(gen=gen, data=seg)\n",
    "\n",
    "            # Normalising synthetic scan intensity to the same values as the original case, \n",
    "            # and cropping to the same shape\n",
    "            cropped_ct_scan, cropped_fake_scan, cropped_seg = post_processing(fake_image, seg_2, ct_scan, new_min=-200, new_max=200)\n",
    "\n",
    "            # Saving synthetic scan\n",
    "            save_path = join(SAVE_DIR, f\"imagesTr/{ct_name}_0000.nii.gz\")\n",
    "            save_nifti_with_metadata(exist_nii_path=ct_path, new_numpy_array=cropped_fake_scan, save_path=save_path)\n",
    "            \n",
    "            # Saving segmentation\n",
    "            save_path = join(SAVE_DIR, f\"labelsTr/{ct_name}.nii.gz\")\n",
    "            save_nifti_with_metadata(exist_nii_path=seg_path, new_numpy_array=cropped_seg, save_path=save_path)\n",
    "        else:\n",
    "            #print(f\"Ignored: {ct_name}\")\n",
    "            pass\n",
    "        loop_train.set_postfix(\n",
    "            Case = ct_name,\n",
    "        )\n",
    "        #print(ct_path)\n",
    "        #print(seg_path)\n",
    "      \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT_GAN_2",
   "language": "python",
   "name": "ct_gan_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
