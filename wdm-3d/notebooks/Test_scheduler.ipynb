{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "from monai.transforms import Compose, LoadImage, CropForeground, EnsureChannelFirst, ResizeWithPadOrCrop, ScaleIntensityRange\n",
    "from guided_diffusion.c_unet import SuperResModel, UNetModel, EncoderUNetModel\n",
    "import torch\n",
    "import torch as th\n",
    "from diffusers import DDPMScheduler, DPMSolverMultistepScheduler\n",
    "from DWT_IDWT.DWT_IDWT_layer import IDWT_3D, DWT_3D\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "idwt = IDWT_3D(\"haar\")\n",
    "dwt = DWT_3D(\"haar\")\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd,\n",
    "    CopyItemsd\n",
    "    )\n",
    "from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from monai.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(file_path, norm, clip):\n",
    "    \"\"\"\n",
    "    Loads the nii.gz file, and normalises if necessary.\n",
    "    Arguments:\n",
    "        file_path (str): Path to the nii.gz file.\n",
    "        norm (bool): True for clipping and normalisation.\n",
    "    Return:\n",
    "        Numpy array of nii.gz file.\n",
    "    \"\"\"\n",
    "    transforms = [\n",
    "        LoadImage(image_only=True),\n",
    "        EnsureChannelFirst()\n",
    "        ]\n",
    "    if clip:\n",
    "        transforms.append(\n",
    "        ScaleIntensityRange(a_min=-200, a_max=200, b_min=-200, b_max=200, clip=True)\n",
    "        )\n",
    "    if norm:\n",
    "        transforms.append(\n",
    "        ScaleIntensityRange(a_min=-200, a_max=200, b_min=-1, b_max=1, clip=True)\n",
    "        )\n",
    "    apply_transforms = Compose(transforms)\n",
    "    np_tensor = apply_transforms(file_path)[0].numpy()\n",
    "    return np_tensor\n",
    "\n",
    "def get_segmentation(file_path):\n",
    "    \"\"\"\n",
    "    Load the segmentation, crops the foreground and reshape to 128x128x128 using padding.\n",
    "    This ensures that the segmentation is in the middle of the volume\n",
    "    Arguments:\n",
    "        file_path (str): Path to the segmentation file.\n",
    "    Return:\n",
    "        Numpy array of the segmentation.\n",
    "    \"\"\"\n",
    "    transforms = Compose([\n",
    "        LoadImage(image_only=True),\n",
    "        EnsureChannelFirst(),\n",
    "        CropForeground(select_fn=lambda x: x > 0, margin=0),\n",
    "        ResizeWithPadOrCrop(spatial_size=(128,128,128))\n",
    "    ])\n",
    "    segmentation = transforms(file_path)[0].numpy()\n",
    "    return segmentation\n",
    "\n",
    "def rescale_array(arr, minv, maxv): #monai function adapted\n",
    "    \"\"\"\n",
    "    Rescale the values of numpy array `arr` to be from `minv` to `maxv`.\n",
    "    \"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        mina = np.min(arr)\n",
    "        maxa = np.max(arr)\n",
    "    elif isinstance(arr, th.Tensor):\n",
    "        mina = th.min(arr)\n",
    "        maxa = th.max(arr)\n",
    "    if mina == maxa:\n",
    "        return arr * minv\n",
    "    # normalize the array first\n",
    "    norm = (arr - mina) / (maxa - mina) \n",
    "    # rescale by minv and maxv, which is the normalized array by default \n",
    "    return (norm * (maxv - minv)) + minv  \n",
    "\n",
    "from scipy.ndimage import center_of_mass\n",
    "def get_crop_tensors(healthy_ct_scan_full_res, region_to_place_tumour_mask, segmentation, device):\n",
    "    \"\"\"\n",
    "    Selects a random center and crops the volume with that center and shape 128x128x128.\n",
    "    Arguments:\n",
    "        healthy_ct_scan_full_res (numpy array): Healthy volume.\n",
    "        region_to_place_tumour_mask (numpy array): Mask of the region to where the tumour can be placed.\n",
    "        segmentation (numpy array): Tumour segmentation.\n",
    "    \"\"\"\n",
    "    centroid = center_of_mass(segmentation)\n",
    "    random_x, random_y, random_z = int(centroid[0]), int(centroid[1]), int(centroid[2])\n",
    "\n",
    "    # Padding the volume so no region ouside of the volume is selected\n",
    "    healthy_ct_scan_full_res = np.pad(healthy_ct_scan_full_res, pad_width=64, mode='constant', constant_values=-200) # -200 background\n",
    "    region_to_place_tumour_mask = np.pad(region_to_place_tumour_mask, pad_width=64, mode='constant', constant_values=1) # 1 means that the tumour cannot be placed there\n",
    "    \n",
    "    # Select a random center\n",
    "    voxel_indices = np.argwhere(region_to_place_tumour_mask == 2)\n",
    "\n",
    "    # Crop the full resolution scan and mask\n",
    "    healthy_ct_scan = healthy_ct_scan_full_res[\n",
    "        random_x-64:random_x+64,\n",
    "        random_y-64:random_y+64,\n",
    "        random_z-64:random_z+64\n",
    "        ]\n",
    "    region_to_place_tumour_mask_crop = region_to_place_tumour_mask[ \n",
    "        random_x-64:random_x+64,\n",
    "        random_y-64:random_y+64,\n",
    "        random_z-64:random_z+64\n",
    "        ] \n",
    "    # Ensure the segmentation remains within the anatomical boundaries defined by the region_to_place_tumour_mask_crop\n",
    "    segmentation[region_to_place_tumour_mask_crop == 1] = 0\n",
    "\n",
    "    # Keep the original intensities of the cropped region\n",
    "    healthy_ct_scan_origin_intensities = np.copy(healthy_ct_scan)\n",
    "    \n",
    "    # Convert to torch and add two dimentions\n",
    "    healthy_ct_scan = th.from_numpy(healthy_ct_scan)\n",
    "    healthy_ct_scan = rescale_array(healthy_ct_scan, minv=-1, maxv=1)\n",
    "    healthy_ct_scan = healthy_ct_scan.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    healthy_ct_scan_origin_intensities = th.from_numpy(healthy_ct_scan_origin_intensities)\n",
    "    healthy_ct_scan_origin_intensities = healthy_ct_scan_origin_intensities.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    segmentation = th.from_numpy(segmentation)\n",
    "    segmentation = segmentation.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    healthy_ct_scan_full_res = th.from_numpy(healthy_ct_scan_full_res)\n",
    "    healthy_ct_scan_full_res = healthy_ct_scan_full_res.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    return healthy_ct_scan_full_res, healthy_ct_scan, healthy_ct_scan_origin_intensities, segmentation, random_voxel_indices\n",
    "\n",
    "def get_affine_and_header(file_path):\n",
    "  \"\"\"\n",
    "  Extracts the affine transformation matrix and header information from a NIfTI file.\n",
    "  Args:\n",
    "    filename (str): The path to the NIfTI file.\n",
    "  Returns:\n",
    "    tuple: A tuple containing the affine matrix and header information.\n",
    "  \"\"\"\n",
    "  img = nib.load(file_path)\n",
    "  affine = img.affine\n",
    "  header = img.header\n",
    "  return affine, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(in_channels=11, out_channels=8, channel_mult=[1, 2, 2, 4, 4, 4], label_cond_in_channels=0, use_label_cond_conv=False, pretrained_weights_path=None):\n",
    "    model = UNetModel(\n",
    "        image_size=128,\n",
    "        in_channels=in_channels,\n",
    "        model_channels=64,\n",
    "        out_channels=out_channels,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=tuple([]),\n",
    "        dropout=0.0,\n",
    "        channel_mult=channel_mult,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=True,\n",
    "        use_new_attention_order=False,\n",
    "        dims=3,\n",
    "        num_groups=32,\n",
    "        bottleneck_attention=False,\n",
    "        additive_skips=True,\n",
    "        resample_2d=False,\n",
    "        label_cond_in_channels=label_cond_in_channels,\n",
    "        use_label_cond_conv=use_label_cond_conv,\n",
    "    )\n",
    "    # Load the pre-trained weights\n",
    "    state_dict = torch.load(pretrained_weights_path, map_location=torch.device('cuda:0'))  # Load to CPU, or adjust for GPU if needed\n",
    "\n",
    "    # Load weights into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(sch, num_inference_steps):\n",
    "    if sch==\"DPM++_2M\":\n",
    "        use_karras_sigmas = False\n",
    "        algorithm_type = \"dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_Karras\":\n",
    "        use_karras_sigmas = True\n",
    "        algorithm_type = \"dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_SDE\":\n",
    "        use_karras_sigmas = False\n",
    "        algorithm_type = \"sde-dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_SDE_Karras\":\n",
    "        use_karras_sigmas = True\n",
    "        algorithm_type = \"sde-dpmsolver++\"\n",
    "        \n",
    "    scheduler = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=1000, \n",
    "            variance_type=\"fixed_large\", \n",
    "            prediction_type=\"sample\", \n",
    "            use_karras_sigmas=use_karras_sigmas, \n",
    "            algorithm_type=algorithm_type\n",
    "            #use_beta_sigmas=True # https://huggingface.co/papers/2407.12173\n",
    "            )\n",
    "    scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for CT scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole CT scan generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_key, label_key, clip_min, clip_max, image_size, no_seg, full_background, data_split_json, base_dir):\n",
    "    train_transforms = [\n",
    "            LoadImaged(keys=[image_key, label_key], meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "            EnsureChannelFirstd(keys=[image_key, label_key]),\n",
    "            EnsureTyped(keys=[image_key, label_key], dtype=torch.float32),\n",
    "            Orientationd(keys=[image_key, label_key], axcodes=\"RAS\"),\n",
    "            ScaleIntensityRanged(keys=[image_key], a_min=float(clip_min), a_max=float(clip_max), b_min=-1.0, b_max=1.0, clip=True),\n",
    "            ResizeWithPadOrCropd(\n",
    "                    keys=[image_key, label_key],\n",
    "                    spatial_size=image_size,\n",
    "                    mode=\"constant\",\n",
    "                    value=-1 # The value was -1 originally\n",
    "                ),\n",
    "            ConvertToMultiChannel_BackandForeground_Contrastd(\n",
    "                    keys=[label_key], no_seg=no_seg, full_background=full_background\n",
    "                    )\n",
    "        ]\n",
    "    train_transforms.append(EnsureTyped(keys=[image_key, label_key], dtype=torch.float32))\n",
    "    train_transforms_final =  Compose(train_transforms)\n",
    "\n",
    "    data_set = load_decathlon_datalist(\n",
    "                data_split_json,\n",
    "                is_segmentation=True,\n",
    "                data_list_key=\"training\",\n",
    "                base_dir=base_dir,\n",
    "            )\n",
    "\n",
    "    print(f\"Training cases: {len(data_set)}\")\n",
    "\n",
    "    print(data_set[-1:])\n",
    "    print(f\"TOTAL cases {len(data_set)}\")\n",
    "    # Creating traing dataset\n",
    "    ds = CacheDataset( \n",
    "        data=data_set, \n",
    "        transform=train_transforms_final,\n",
    "        cache_rate=0, \n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    dl = DataLoader(\n",
    "                ds,\n",
    "                batch_size=1,\n",
    "                num_workers=4,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                shuffle=False, \n",
    "                #collate_fn=no_collation,\n",
    "            )\n",
    "    return dl, ds, data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Tumour generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_conv_before_concat__DA_tumorW_0_28_11_2024_11:19:14\n",
    "* HU between -200 and 200. tumour weight 0. DA ROI. ROI and segmentation as condition, feeded first to a conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 32\n",
    "label_cond_in_channels = 3\n",
    "use_label_cond_conv = True\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_conv_before_concat__DA_tumorW_0_28_11_2024_11:19:14/checkpoints/hnn_1300000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "            input_model = final_scan.cuda()\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = final_scan\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_conv_before_concat__DA_tumorW_0_28_11_2024_11:19:14\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=100, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path)            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_11:36:09 \n",
    "* HU between -200 and 200. tumour weight 0. DA ROI. downsampled ROI and segmentation as condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "\n",
    "            # create input model\n",
    "            resize = Resize((128, 128, 128), size_mode='all', mode=\"nearest\", align_corners=None, anti_aliasing=False, anti_aliasing_sigma=None, dtype=torch.float32, lazy=False)\n",
    "            label_cond_down = resize(label_condition[0]).unsqueeze(0)\n",
    "            input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 11\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_11:36:09/checkpoints/hnn_1355000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_11:36:09\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_wavelet_cond__DA_tumorW_0_3_12_2024_15:36:29 \n",
    "* HU between -200 and 200. tumour weight 0. DA ROI. wavelet tranformed ROI and segmentation as condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "            LLL = None\n",
    "            # create input model\n",
    "            for condition in label_condition[0]:\n",
    "                condition = condition.unsqueeze(0).unsqueeze(0)\n",
    "                if LLL==None:\n",
    "                    LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH = dwt(condition)\n",
    "                    cond_dwt = th.cat([LLL / 3., LLH, LHL, LHH, HLL, HLH, HHL, HHH], dim=1)\n",
    "                else:\n",
    "                    LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH = dwt(condition)\n",
    "                    cond_dwt = th.cat([cond_dwt, LLL / 3., LLH, LHL, LHH, HLL, HLH, HHL, HHH], dim=1)\n",
    "            input_model = torch.cat((final_scan, cond_dwt), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, cond_dwt), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 32\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_wavelet_cond__DA_tumorW_0_3_12_2024_15:36:29/checkpoints/hnn_985000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_wavelet_cond__DA_tumorW_0_3_12_2024_15:36:29\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bone segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:15:39\n",
    "* HU between -200 and 200. DA ROI. downsampled ROI as condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "            label_condition =label_condition[:,0:2,:,:,:]\n",
    "            print(f\"label_condition: {label_condition.shape}\")\n",
    "\n",
    "\n",
    "            # create input model\n",
    "            resize = Resize((128, 128, 128), size_mode='all', mode=\"nearest\", align_corners=None, anti_aliasing=False, anti_aliasing_sigma=None, dtype=torch.float32, lazy=False)\n",
    "            label_cond_down = resize(label_condition[0]).unsqueeze(0)\n",
    "            input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 10\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:15:39/checkpoints/hnn_1360000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:15:39/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:18:20\n",
    "* HU between -1000 and 1000. DA ROI. downsampled ROI as condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "            label_condition =label_condition[:,0:2,:,:,:]\n",
    "            print(f\"label_condition: {label_condition.shape}\")\n",
    "\n",
    "\n",
    "            # create input model\n",
    "            resize = Resize((128, 128, 128), size_mode='all', mode=\"nearest\", align_corners=None, anti_aliasing=False, anti_aliasing_sigma=None, dtype=torch.float32, lazy=False)\n",
    "            label_cond_down = resize(label_condition[0]).unsqueeze(0)\n",
    "            input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "clip_min = -1000\n",
    "clip_max = 1000\n",
    "in_channels = 10\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:18:20/checkpoints/hnn_1355000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_concat_cond__DA_tumorW_0_28_11_2024_14:18:20/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_CT_concat_cond_data_augment_27_11_2024 - Retrained model from -200 and 200\n",
    "* HU between -1000 and 1000. DA ROI. downsampled ROI as condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            scheduler = get_scheduler(sch, num_inference_steps)\n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            segmentation = label_condition[0][2]\n",
    "            no_contrast_tensor = label_condition[0][0]\n",
    "            contrast_tensor = label_condition[0][1]\n",
    "\n",
    "            label_condition =label_condition[:,0:2,:,:,:]\n",
    "            print(f\"label_condition: {label_condition.shape}\")\n",
    "\n",
    "\n",
    "            # create input model\n",
    "            resize = Resize((128, 128, 128), size_mode='all', mode=\"nearest\", align_corners=None, anti_aliasing=False, anti_aliasing_sigma=None, dtype=torch.float32, lazy=False)\n",
    "            label_cond_down = resize(label_condition[0]).unsqueeze(0)\n",
    "            input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "                cube_coords = th.nonzero(contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                cube_coords = th.nonzero(no_contrast_tensor) \n",
    "                min_coords = cube_coords.min(dim=0)[0]  # Minimum x, y, z coordinates\n",
    "                max_coords = cube_coords.max(dim=0)[0]  # Maximum x, y, z coordinates\n",
    "            case_name = \"generated\"\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "            # Cropping the output of the model considering the ROI\n",
    "            x_min, y_min, z_min = min_coords\n",
    "            x_max, y_max, z_max = max_coords\n",
    "            sample_denorm_corrected = sample_denorm#[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "            # Create a NIfTI image from the NumPy array\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "\n",
    "            # Save the NIfTI image as a .nii.gz file\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_key = \"image\"\n",
    "label_key = \"seg\"\n",
    "image_size = (256, 256, 256)\n",
    "data_split_json = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\"\n",
    "base_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "clip_min = -1000\n",
    "clip_max = 1000\n",
    "in_channels = 10\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_CT_concat_cond__data_augment_27_11_2024_17:23:26/checkpoints/hnn_7380000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_loader(image_key=image_key, \n",
    "                              label_key=label_key, \n",
    "                              clip_min=clip_min, \n",
    "                              clip_max=clip_max, \n",
    "                              image_size=image_size, \n",
    "                              no_seg=no_seg, \n",
    "                              full_background=full_background, \n",
    "                              data_split_json=data_split_json, \n",
    "                              base_dir=base_dir)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 5\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_CT_concat_cond__data_augment_27_11_2024_17:23:26/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cropped CT scans - Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.convert_head_n_neck_cancer import ConvertHeadNNeckCancerd as LABEL_TRANSFORM\n",
    "from utils.crop_scan_center_in_tumour import CropScanCenterInTumour\n",
    "from monai.transforms import ToTensord\n",
    "from monai.data import CSVDataset, CacheDataset, DataLoader, load_decathlon_datalist\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "\n",
    "def get_loader(scan_name, col_names, col_types, use_dilation, clip_min, clip_max, CSV_PATH):\n",
    "    train_transforms = [\n",
    "                        LoadImaged(keys=[scan_name, 'label'], image_only=False),\n",
    "                        EnsureChannelFirstd(keys=[scan_name, \"label\"]),\n",
    "                        EnsureTyped(keys=[scan_name, \"label\"]),\n",
    "                        CopyItemsd(keys=[scan_name], names=[f\"{scan_name}_origin\"]),\n",
    "                        ScaleIntensityRanged(keys=[f\"{scan_name}_origin\"], a_min=int(clip_min), a_max=int(clip_max), b_min=int(clip_min), b_max=int(clip_max), clip=True),\n",
    "                        LABEL_TRANSFORM(keys=\"label\"),\n",
    "                    ] \n",
    "    train_transforms.append(\n",
    "        CropScanCenterInTumour(keys=scan_name, dilation=use_dilation, translate_range=None)\n",
    "        )       \n",
    "    train_transforms.append(\n",
    "        ScaleIntensityRanged(keys=[scan_name], a_min=int(clip_min), a_max=int(clip_max), b_min=-1.0, b_max=1.0, clip=True)\n",
    "    )\n",
    "    train_transforms.append(ToTensord(keys=[scan_name, 'no_contrast_tensor', 'contrast_tensor', 'scan_volume_crop', 'scan_volume_crop_pad', 'label', 'label_crop_pad', 'center_x', 'center_y', 'center_z', 'x_extreme_min', 'x_extreme_max', 'y_extreme_min', 'y_extreme_max', 'z_extreme_min', 'z_extreme_max', 'x_size', 'y_size', 'z_size']))\n",
    "\n",
    "    train_CSVdataset = CSVDataset(src=CSV_PATH, col_names=col_names, col_types=col_types) \n",
    "    train_CSVdataset = CacheDataset(train_CSVdataset, transform=train_transforms, cache_rate=0, num_workers=8, progress=True)  \n",
    "    train_loader = DataLoader(train_CSVdataset, batch_size=1, num_workers=4, drop_last=True, shuffle=False, collate_fn=pad_list_data_collate)\n",
    "    return train_loader, train_CSVdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31\n",
    "* HU between -200 and 200. tumour weight 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(train_loader, model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(train_loader):\n",
    "            noise_start = torch.randn(1, 1, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            segmentation = batch[\"label_crop_pad\"].cuda()\n",
    "            no_contrast_tensor = batch[\"no_contrast_tensor\"].cuda()\n",
    "            contrast_tensor = batch[\"contrast_tensor\"].cuda()\n",
    "            label_condition = torch.cat((no_contrast_tensor, contrast_tensor, segmentation), dim=1)\n",
    "\n",
    "            input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                \n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy().astype(float)[0][0], affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 4\n",
    "out_channels = 1\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31/checkpoints/hnn_tumour_inpainting_2000000.pt'  # Specify the correct path\n",
    "channel_mult=[1, 2, 2, 4, 4]\n",
    "\n",
    "model = get_model(in_channels=in_channels, \n",
    "                  out_channels=out_channels,\n",
    "                  channel_mult=channel_mult,\n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "scan_name = \"scan_ct\"\n",
    "col_names = ['scan_ct', 'label', 'center_x', 'center_y', 'center_z', 'x_extreme_min', 'x_extreme_max', 'y_extreme_min', 'y_extreme_max', 'z_extreme_min', 'z_extreme_max', 'x_size', 'y_size', 'z_size', 'contrast']\n",
    "col_types= {'center_x': {'type': int}, 'center_y': {'type': int}, 'center_z': {'type': int}, 'x_extreme_min': {'type': int}, 'x_extreme_max': {'type': int}, 'y_extreme_min': {'type': int}, 'y_extreme_max': {'type': int}, 'z_extreme_min': {'type': int}, 'z_extreme_max': {'type': int}, 'x_size': {'type': int}, 'y_size': {'type': int}, 'z_size': {'type': int}, 'contrast': {'type': int}}  \n",
    "use_dilation = False\n",
    "CSV_PATH = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/utils/hnn.csv\"\n",
    "\n",
    "train_loader, train_CSVdataset = get_loader(scan_name=scan_name,\n",
    "           col_names=col_names,\n",
    "           col_types=col_types, \n",
    "           use_dilation=use_dilation, \n",
    "           clip_min=clip_min, \n",
    "           clip_max=clip_max, \n",
    "           CSV_PATH=CSV_PATH)\n",
    "\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(train_loader=train_loader,\n",
    "              model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59\n",
    "* HU between -1000 and 1000. tumour weight 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(train_loader, model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(train_loader):\n",
    "            noise_start = torch.randn(1, 1, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            segmentation = batch[\"label_crop_pad\"].cuda()\n",
    "            no_contrast_tensor = batch[\"no_contrast_tensor\"].cuda()\n",
    "            contrast_tensor = batch[\"contrast_tensor\"].cuda()\n",
    "            label_condition = torch.cat((no_contrast_tensor, contrast_tensor, segmentation), dim=1)\n",
    "\n",
    "            input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                \n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy().astype(float)[0][0], affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_min = -1000\n",
    "clip_max = 1000\n",
    "in_channels = 4\n",
    "out_channels = 1\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59/checkpoints/hnn_tumour_inpainting_1310000.pt'  # Specify the correct path\n",
    "channel_mult=[1, 2, 2, 4, 4]\n",
    "\n",
    "model = get_model(in_channels=in_channels, \n",
    "                  out_channels=out_channels,\n",
    "                  channel_mult=channel_mult,\n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "scan_name = \"scan_ct\"\n",
    "col_names = ['scan_ct', 'label', 'center_x', 'center_y', 'center_z', 'x_extreme_min', 'x_extreme_max', 'y_extreme_min', 'y_extreme_max', 'z_extreme_min', 'z_extreme_max', 'x_size', 'y_size', 'z_size', 'contrast']\n",
    "col_types= {'center_x': {'type': int}, 'center_y': {'type': int}, 'center_z': {'type': int}, 'x_extreme_min': {'type': int}, 'x_extreme_max': {'type': int}, 'y_extreme_min': {'type': int}, 'y_extreme_max': {'type': int}, 'z_extreme_min': {'type': int}, 'z_extreme_max': {'type': int}, 'x_size': {'type': int}, 'y_size': {'type': int}, 'z_size': {'type': int}, 'contrast': {'type': int}}  \n",
    "use_dilation = False\n",
    "CSV_PATH = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/utils/hnn.csv\"\n",
    "\n",
    "train_loader, train_CSVdataset = get_loader(scan_name=scan_name,\n",
    "           col_names=col_names,\n",
    "           col_types=col_types, \n",
    "           use_dilation=use_dilation, \n",
    "           clip_min=clip_min, \n",
    "           clip_max=clip_max, \n",
    "           CSV_PATH=CSV_PATH)\n",
    "\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(train_loader=train_loader,\n",
    "              model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test blur mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def blur_mask_3d(mask, label_crop_pad, blur_factor, blur_type):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur to a 3D mask.\n",
    "    \n",
    "    Args:\n",
    "        mask (torch.Tensor): The mask tensor of shape (1, 1, H, W, D).\n",
    "        blur_factor (int): Kernel size for the Gaussian blur. Should be odd.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Blurred mask.\n",
    "    \"\"\"\n",
    "    # Ensure blur_factor is odd\n",
    "    if blur_factor % 2 == 0:\n",
    "        blur_factor += 1\n",
    "    \n",
    "    # Create a 3D Gaussian kernel\n",
    "    sigma = blur_factor / 12.0  # Rule of thumb for Gaussian kernel\n",
    "    x = torch.linspace(-3, 3, blur_factor)\n",
    "    kernel_1d = torch.exp(-0.5 * x**2 / sigma**2)\n",
    "    kernel_1d = kernel_1d / kernel_1d.sum()  # Normalize\n",
    "    \n",
    "    # Create 3D kernel from 1D kernels\n",
    "    kernel_3d = kernel_1d[:, None, None] * kernel_1d[None, :, None] * kernel_1d[None, None, :]\n",
    "    kernel_3d = kernel_3d.to(mask.device)\n",
    "    kernel_3d = kernel_3d.unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    \n",
    "    # Pad the mask for convolution\n",
    "    padding = blur_factor // 2\n",
    "    mask_padded = F.pad(mask, (padding, padding, padding, padding, padding, padding), mode=\"replicate\")\n",
    "    \n",
    "    # Apply convolution\n",
    "    blurred_mask = F.conv3d(mask_padded, kernel_3d, padding=0)\n",
    "    if blur_type==\"edge_blur\":\n",
    "        blurred_mask[label_crop_pad==1] = label_crop_pad[label_crop_pad==1]\n",
    "    elif blur_type==\"full_blur\":\n",
    "        blurred_mask=blurred_mask\n",
    "    else:\n",
    "        raise ValueError(f\"blur_type must be edge_blur or full_blur not {blurred_mask}\")\n",
    "    return blurred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.ndimage import binary_dilation \n",
    "import torch\n",
    "import numpy as np\n",
    "from monai.transforms import Compose, LoadImage, CropForeground, EnsureChannelFirst, ResizeWithPadOrCrop, ScaleIntensityRange\n",
    "import nibabel as nib\n",
    "transforms = [\n",
    "    LoadImage(image_only=True),\n",
    "    EnsureChannelFirst()\n",
    "    ]\n",
    "for i in range(0,40,5):\n",
    "    apply_transforms = Compose(transforms)\n",
    "    segmentation = apply_transforms(\"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/seg/anderson_89f9e1d8eeae55b7ab93ac1fe0cf3801.nii.gz\")[0].numpy()\n",
    "\n",
    "    random_voxel_indices = np.array(center_of_mass(segmentation)).astype(int)\n",
    "    random_x, random_y, random_z = int(random_voxel_indices[0]), int(random_voxel_indices[1]), int(random_voxel_indices[2])\n",
    "    label_crop_pad = segmentation[\n",
    "            random_x-64:random_x+64,\n",
    "            random_y-64:random_y+64,\n",
    "            random_z-64:random_z+64\n",
    "            ]\n",
    "\n",
    "    label_crop_pad_dillated = label_crop_pad\n",
    "    structuring_element = np.ones((3, 3, 3), dtype=str)\n",
    "    dilated_mask = binary_dilation(label_crop_pad_dillated, structure=structuring_element, iterations=5)\n",
    "    dilated_mask = torch.from_numpy(dilated_mask).float()\n",
    "    dilated_mask = dilated_mask.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "    blurred_mask = blur_mask_3d(dilated_mask, label_crop_pad, blur_factor=i, blur_type=\"full_blur\")\n",
    "    blurred_mask.shape\n",
    "\n",
    "\n",
    "    if i==0:\n",
    "        img = nib.Nifti1Image(label_crop_pad, affine=np.eye(4)) \n",
    "        nib.save(img=img, filename=f\"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/trash/real_mask\")\n",
    "    else:\n",
    "        img = nib.Nifti1Image(blurred_mask[0][0].numpy(), affine=np.eye(4)) \n",
    "        nib.save(img=img, filename=f\"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/trash/blurred_mask_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction for Brain tumour scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader_utils import ConvertToMultiChannelBasedOnBratsClasses2023d, QuantileAndScaleIntensityd\n",
    "def get_brats_loader(in_keys, all_image_keys, label_key, base_dir, data_split_json, no_seg, image_size):\n",
    "    train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=in_keys, meta_key_postfix=\"meta_dict\", image_only=False),\n",
    "                EnsureChannelFirstd(keys=in_keys),\n",
    "                EnsureTyped(keys=in_keys, dtype=torch.float32),\n",
    "                Orientationd(keys=in_keys, axcodes=\"RAS\"),\n",
    "                ResizeWithPadOrCropd(\n",
    "                        keys=in_keys,\n",
    "                        spatial_size=image_size,\n",
    "                        mode=\"constant\",\n",
    "                        value=0\n",
    "                    ),\n",
    "                QuantileAndScaleIntensityd(keys=all_image_keys), # a_min=-1, a_max=1),\n",
    "                ConvertToMultiChannelBasedOnBratsClasses2023d(\n",
    "                    keys=[label_key], no_seg=no_seg,\n",
    "                ),\n",
    "                EnsureTyped(keys=in_keys, dtype=torch.float32)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    data_set = load_decathlon_datalist(\n",
    "                data_split_json,\n",
    "                is_segmentation=True,\n",
    "                data_list_key=\"training\",\n",
    "                base_dir=base_dir,\n",
    "            )\n",
    "\n",
    "    print(f\"Training cases: {len(data_set)}\")\n",
    "\n",
    "    print(data_set[-1:])\n",
    "    # Creating traing dataset\n",
    "    ds = CacheDataset( \n",
    "        data=data_set,\n",
    "        transform=train_transforms,\n",
    "        cache_rate=0, \n",
    "        copy_cache=False,\n",
    "        progress=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Creating data loader\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        shuffle=False, \n",
    "        #collate_fn=no_collation,\n",
    "        )\n",
    "    return dl, ds, data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c_brats_t1c_conv_before_concat__tumorW_0_28_11_2024_13:02:05 \n",
    "* tumour weight 0. Three channel segmentation as condition, feeded first to a conv layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            case_path = batch['t1c_meta_dict']['filename_or_obj'][0]\n",
    "            print(f\"case_path: {case_path}\")\n",
    "            \n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "            input_model = final_scan\n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            tumour_core = label_condition[0][0]\n",
    "            whole_tumour = label_condition[0][1]\n",
    "            enhancing_tumour = label_condition[0][2]\n",
    "\n",
    "\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = final_scan\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            affine, header = get_affine_and_header(case_path)\n",
    "            data = nib.load(case_path).get_fdata()\n",
    "            clip_min = np.min(data)\n",
    "            clip_max = np.max(data)\n",
    "\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'MRI_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            segmentation = torch.zeros_like(tumour_core)\n",
    "            segmentation[whole_tumour==1] = 2\n",
    "            segmentation[tumour_core==1] = 1\n",
    "            segmentation[enhancing_tumour==1] = 3\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_size = (256, 256, 256)\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "in_keys = ['t1c', 'seg']\n",
    "all_image_keys = ['t1c']\n",
    "label_key = 'seg'\n",
    "base_dir = \"/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "data_split_json =  os.path.join('/'.join(base_dir.split(\"/\")[0:-1]), \"BraTS2023_GLI_data_split.json\")\n",
    "\n",
    "in_channels = 32\n",
    "label_cond_in_channels = 3\n",
    "use_label_cond_conv = True\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/c_brats_t1c_conv_before_concat__tumorW_0_28_11_2024_16:46:58/checkpoints/c_brats_1295000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_brats_loader(in_keys=in_keys, \n",
    "                                    all_image_keys=all_image_keys, \n",
    "                                    label_key=label_key, \n",
    "                                    base_dir=base_dir, \n",
    "                                    data_split_json=data_split_json, \n",
    "                                    no_seg=no_seg, \n",
    "                                    image_size=image_size)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/c_brats_t1c_conv_before_concat__tumorW_0_28_11_2024_16:46:58/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c_brats_t1c_concat_cond__tumorW_0_28_11_2024_16:45:44\n",
    "* tumour weight 0. downsampled three channel segmentation as condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            case_path = batch['t1c_meta_dict']['filename_or_obj'][0]\n",
    "            print(f\"case_path: {case_path}\")\n",
    "            \n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "          \n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            tumour_core = label_condition[0][0]\n",
    "            whole_tumour = label_condition[0][1]\n",
    "            enhancing_tumour = label_condition[0][2]\n",
    "\n",
    "            # create input model\n",
    "            resize = Resize((128, 128, 128), size_mode='all', mode=\"nearest\", align_corners=None, anti_aliasing=False, anti_aliasing_sigma=None, dtype=torch.float32, lazy=False)\n",
    "            label_cond_down = resize(label_condition[0]).unsqueeze(0)\n",
    "            input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_cond_down), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            affine, header = get_affine_and_header(case_path)\n",
    "            data = nib.load(case_path).get_fdata()\n",
    "            clip_min = np.min(data)\n",
    "            clip_max = np.max(data)\n",
    "\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'MRI_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            segmentation = torch.zeros_like(tumour_core)\n",
    "            segmentation[whole_tumour==1] = 2\n",
    "            segmentation[tumour_core==1] = 1\n",
    "            segmentation[enhancing_tumour==1] = 3\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_size = (256, 256, 256)\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "in_keys = ['t1c', 'seg']\n",
    "all_image_keys = ['t1c']\n",
    "label_key = 'seg'\n",
    "base_dir = \"/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "data_split_json =  os.path.join('/'.join(base_dir.split(\"/\")[0:-1]), \"BraTS2023_GLI_data_split.json\")\n",
    "\n",
    "in_channels = 11\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/c_brats_t1c_concat_cond__tumorW_0_9_12_2024_21:19:23/checkpoints/c_brats_625000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_brats_loader(in_keys=in_keys, \n",
    "                                    all_image_keys=all_image_keys, \n",
    "                                    label_key=label_key, \n",
    "                                    base_dir=base_dir, \n",
    "                                    data_split_json=data_split_json, \n",
    "                                    no_seg=no_seg, \n",
    "                                    image_size=image_size)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/c_brats_t1c_concat_cond__tumorW_0_9_12_2024_21:19:23/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c_brats_t1c_wavelet_cond__tumorW_0_3_12_2024_15:36:12\n",
    "* tumour weight 0. wavelet transformed three channel segmentation as condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, scheduler_list, n, num_inference_steps, out_path):\n",
    "    model.cuda()\n",
    "    for sch in scheduler_list:\n",
    "        scheduler = get_scheduler(sch, num_inference_steps)\n",
    "\n",
    "        for idx, batch  in enumerate(dl):\n",
    "            case_path = batch['t1c_meta_dict']['filename_or_obj'][0]\n",
    "            print(f\"case_path: {case_path}\")\n",
    "            \n",
    "            noise_start = torch.randn(1, 8, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "          \n",
    "\n",
    "            label_condition = batch[\"seg\"].cuda()\n",
    "            tumour_core = label_condition[0][0]\n",
    "            whole_tumour = label_condition[0][1]\n",
    "            enhancing_tumour = label_condition[0][2]\n",
    "\n",
    "            # create input model\n",
    "            LLL = None\n",
    "            # create input model\n",
    "            for condition in label_condition[0]:\n",
    "                condition = condition.unsqueeze(0).unsqueeze(0)\n",
    "                if LLL==None:\n",
    "                    LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH = dwt(condition)\n",
    "                    cond_dwt = th.cat([LLL / 3., LLH, LHL, LHH, HLL, HLH, HHL, HHH], dim=1)\n",
    "                else:\n",
    "                    LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH = dwt(condition)\n",
    "                    cond_dwt = th.cat([cond_dwt, LLL / 3., LLH, LHL, LHH, HLL, HLH, HHL, HHH], dim=1)\n",
    "            input_model = torch.cat((final_scan, cond_dwt), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, cond_dwt), dim=1)\n",
    "            B, C, D, H, W = final_scan.size()\n",
    "            final_scan = idwt(final_scan[:, 0, :, :, :].view(B, 1, H, W, D) * 3.,\n",
    "                        final_scan[:, 1, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 2, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 3, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 4, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 5, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 6, :, :, :].view(B, 1, H, W, D),\n",
    "                        final_scan[:, 7, :, :, :].view(B, 1, H, W, D))\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            affine, header = get_affine_and_header(case_path)\n",
    "            data = nib.load(case_path).get_fdata()\n",
    "            clip_min = np.min(data)\n",
    "            clip_max = np.max(data)\n",
    "\n",
    "            synth_ct_scan_output = os.path.join(out_path, f'MRI_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            segmentation = torch.zeros_like(tumour_core)\n",
    "            segmentation[whole_tumour==1] = 2\n",
    "            segmentation[tumour_core==1] = 1\n",
    "            segmentation[enhancing_tumour==1] = 3\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy(), affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "            if idx+1 == n:\n",
    "                break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for CT\n",
    "image_size = (256, 256, 256)\n",
    "full_background = False\n",
    "no_seg = False\n",
    "\n",
    "# To change\n",
    "in_keys = ['t1c', 'seg']\n",
    "all_image_keys = ['t1c']\n",
    "label_key = 'seg'\n",
    "base_dir = \"/projects/brats2023_a_f/BRAINTUMOUR/data/brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "data_split_json =  os.path.join('/'.join(base_dir.split(\"/\")[0:-1]), \"BraTS2023_GLI_data_split.json\")\n",
    "\n",
    "in_channels = 32\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/c_brats_t1c_wavelet_cond__tumorW_0_3_12_2024_15:36:12/checkpoints/c_brats_955000.pt'  # Specify the correct path\n",
    "    \n",
    "model = get_model(in_channels=in_channels, \n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "dl, ds, data_set = get_brats_loader(in_keys=in_keys, \n",
    "                                    all_image_keys=all_image_keys, \n",
    "                                    label_key=label_key, \n",
    "                                    base_dir=base_dir, \n",
    "                                    data_split_json=data_split_json, \n",
    "                                    no_seg=no_seg, \n",
    "                                    image_size=image_size)\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/c_brats_t1c_wavelet_cond__tumorW_0_3_12_2024_15:36:12\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               out_path=out_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
