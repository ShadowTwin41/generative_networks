{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study real and synthetic radiomic features differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA\n",
    " * No Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each CSV file\n",
    "def process_csv(df, file_name, features):\n",
    "\n",
    "    data = df[features]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    wavelet_data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    pca_result = pca.fit_transform(wavelet_data_scaled)\n",
    "\n",
    "    pca_df = pd.DataFrame(data=pca_result, columns=['PC1'])\n",
    "    pca_df['File'] = file_name\n",
    "\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"./metrics/RADIOMICS\"\n",
    "\n",
    "columns_to_ignore = ['diagnostics_Versions_PyWavelet'] \n",
    "\n",
    "# List to hold results from each file\n",
    "results = []\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, folder_name, 'tumour_radiomics.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        wavelet_features = [col for col in df.columns if col not in columns_to_ignore and 'wavelet' in col.lower()]\n",
    "        results.append(process_csv(df, folder_name, features=wavelet_features))\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "combined_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Plot the results using Plotly\n",
    "fig = px.box(combined_results, x='File', y='PC1', title='Side-by-Side Box Plot of PCA (1 Component) for Each CSV File')\n",
    "fig.update_layout(\n",
    "    xaxis_title='CSV File',\n",
    "    yaxis_title='Principal Component 1',\n",
    "    xaxis=dict(tickangle=45)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm, verbose=False):\n",
    "    # Load the CSV files into DataFrames\n",
    "    real_cases_df = pd.read_csv(real_features_csv)\n",
    "    synthetic_cases_df = pd.read_csv(synthetic_features_csv)\n",
    "\n",
    "    # Add a label column to each DataFrame\n",
    "    real_cases_df['label'] = 'real'\n",
    "    synthetic_cases_df['label'] = 'synthetic'\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([real_cases_df, synthetic_cases_df], ignore_index=True)\n",
    "\n",
    "    # List of columns to exclude \n",
    "    exclude_columns = [\n",
    "        \"diagnostics_Versions_PyRadiomics\",\n",
    "        \"diagnostics_Versions_Numpy\",\n",
    "        \"diagnostics_Versions_SimpleITK\",\t\n",
    "        \"diagnostics_Versions_PyWavelet\",\t\n",
    "        \"diagnostics_Versions_Python\",\t\n",
    "        \"diagnostics_Configuration_Settings\",\t\n",
    "        \"diagnostics_Configuration_EnabledImageTypes\",\t\n",
    "        \"diagnostics_Image-original_Hash\",\n",
    "        \"diagnostics_Image-original_Dimensionality\",\n",
    "        \"diagnostics_Image-original_Spacing\",\n",
    "        \"diagnostics_Image-original_Size\",\n",
    "        'diagnostics_Mask-original_Hash',\n",
    "        'diagnostics_Mask-original_Spacing',\n",
    "        'diagnostics_Mask-original_Size',\n",
    "        'diagnostics_Mask-original_BoundingBox',\n",
    "        'diagnostics_Mask-original_VoxelNum',\n",
    "        'diagnostics_Mask-original_VolumeNum',\n",
    "        'diagnostics_Mask-original_CenterOfMassIndex',\n",
    "        'diagnostics_Mask-original_CenterOfMass',\n",
    "        \"Filename\"\n",
    "    ]\n",
    "\n",
    "    # Drop the excluded columns\n",
    "    combined_df = combined_df.drop(columns=exclude_columns)\n",
    "\n",
    "    # Handle missing values only for numeric columns\n",
    "    numeric_columns = combined_df.select_dtypes(include='number').columns\n",
    "    combined_df[numeric_columns] = combined_df[numeric_columns].fillna(combined_df[numeric_columns].mean())\n",
    "\n",
    "    if norm:\n",
    "        # Normalize the features\n",
    "        scaler = StandardScaler()\n",
    "        features = combined_df.drop(columns=['label'])\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "    else:\n",
    "        features = combined_df.drop(columns=['label'])\n",
    "        features_scaled = features\n",
    "\n",
    "    # Create a new DataFrame with scaled features and the label\n",
    "    combined_df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "    combined_df_scaled['label'] = combined_df['label']\n",
    "\n",
    "    # Encode the label column to numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    combined_df_scaled['label'] = label_encoder.fit_transform(combined_df_scaled['label'])\n",
    "\n",
    "    return combined_df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def show_correlation(combined_df_scaled, verbose=False):\n",
    "    # Calculate correlation with the target variable\n",
    "    correlations = combined_df_scaled.corr()['label'].abs().sort_values(ascending=False)\n",
    "\n",
    "    # Display the top correlated features\n",
    "    if verbose:\n",
    "        print(\"Top correlated features with the target variable:\\n\", correlations.head(10))\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "\n",
    "def get_best_RF_model(combined_df_scaled, selected_features):\n",
    "    # Train a Random Forest model\n",
    "    X = combined_df_scaled.drop(columns=['label'])[selected_features]\n",
    "    y = combined_df_scaled['label']\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Cross-Validation Score: {best_score:.2f}\")\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_best = best_model.predict(X)\n",
    "    y_pred_proba_best = best_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy_best = accuracy_score(y, y_pred_best)\n",
    "    roc_auc_best = roc_auc_score(y, y_pred_proba_best)\n",
    "\n",
    "    print(f\"Test Accuracy with Best Model: {accuracy_best:.2f}\")\n",
    "    print(f\"Test ROC AUC with Best Model: {roc_auc_best:.2f}\")\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Display the top important features\n",
    "    print(\"\\nTop important features according to the Random Forest model:\\n\", importance_df.head(10))\n",
    "\n",
    "    # Visualize feature importances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
    "    plt.title('Top 10 Important Features')\n",
    "    plt.show()\n",
    "\n",
    "    # Select the first tree from the trained random forest\n",
    "    tree = best_model.estimators_[0]\n",
    "    feature_names = X.columns.tolist()\n",
    "    # Plot the tree\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plot_tree(tree, filled=True, feature_names=feature_names, class_names=[\"Class 0\", \"Class 1\"])\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def get_best_DT_model(combined_df_scaled, selected_features, verbose=False):\n",
    "    # Train a Random Forest model\n",
    "    if selected_features==\"All\":\n",
    "        X = combined_df_scaled.drop(columns=['label'])\n",
    "    else:\n",
    "        X = combined_df_scaled.drop(columns=['label'])[selected_features]\n",
    "    y = combined_df_scaled['label']\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Criterion to measure the quality of a split\n",
    "    'max_depth': [1], #[None, 10, 20, 30, 40, 50],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2', ]  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Best Cross-Validation Score: {best_score:.2f}\")\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_best = best_model.predict(X)\n",
    "    y_pred_proba_best = best_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy_best = accuracy_score(y, y_pred_best)\n",
    "    roc_auc_best = roc_auc_score(y, y_pred_proba_best)\n",
    "\n",
    "    print(f\"Test Accuracy with Best Model: {accuracy_best:.6f}\")\n",
    "    if verbose:\n",
    "        print(f\"Test ROC AUC with Best Model: {roc_auc_best:.6f}\")\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    if selected_features==\"All\":\n",
    "        selected_features = X.columns.tolist()\n",
    "   \n",
    "    importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Display the top important features\n",
    "    print(f\"Top feature: {importance_df.head(1)}\")\n",
    "    if verbose:\n",
    "        print(\"\\nTop important features according to the Random Forest model:\\n\", importance_df.head(10))\n",
    "\n",
    "    if verbose:\n",
    "        # Visualize feature importances\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
    "        plt.title('Top 10 Important Features')\n",
    "        plt.show()\n",
    "\n",
    "        # Get feature names\n",
    "        feature_names = X.columns.tolist()\n",
    "\n",
    "        # Plot the Decision Tree\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plot_tree(best_model, filled=True, feature_names=feature_names, class_names=[\"Real\", \"Synth\"], rounded=True)\n",
    "        plt.title(\"Decision Tree\")\n",
    "        plt.show()\n",
    "    \n",
    "    return best_model, importance_df, accuracy_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features_csv_200 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "synthetic_features_csv = \"./metrics/RADIOMICS/Dataset500_HnN/tumour_radiomics.csv\"\n",
    "combined_df_scaled = get_combined_df_scaled(real_features_csv_200, synthetic_features_csv, norm=False, verbose=False)\n",
    "\n",
    "combined_df_scaled.to_excel('./radiomics/tumour_radiomics_clip_200_Dataset532_HnN.xlsx', index=False)\n",
    "\n",
    "correlations = show_correlation(combined_df_scaled, verbose=False)\n",
    "selected_features = correlations.head(10).index.tolist()\n",
    "selected_features.remove('label')\n",
    "\n",
    "best_model, importance_df, accuracy_best = get_best_DT_model(combined_df_scaled, selected_features=\"All\", verbose=True)\n",
    "first_row_list = importance_df.iloc[0].tolist()\n",
    "print(first_row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv_200 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "real_features_csv_1000 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "\n",
    "save_best_feature = {}\n",
    "missing = []\n",
    "\n",
    "\n",
    "for nnunet_id in listdir(RADIOMICS):\n",
    "    try:\n",
    "        if \"HnN\" not in nnunet_id:\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics.csv\")\n",
    "        if nnunet_id in [\"Dataset570_HnN\", \"Dataset571_HnN\", \"Dataset572_HnN\", \"Dataset573_HnN\", \"Dataset574_HnN\", \"Dataset580_HnN\", \"Dataset581_HnN\", \"Dataset582_HnN\", \"Dataset583_HnN\", \"Dataset584_HnN\", \"Dataset540_HnN\", \"Dataset541_HnN\",\"Dataset542_HnN\",\"Dataset543_HnN\",\"Dataset544_HnN\"]:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_1000, synthetic_features_csv, norm=False, verbose=False)\n",
    "            #combined_df_scaled.to_excel('/projects/brats2023_a_f/Aachen/aritifcial-head-and-neck-cts/nnUNet/trash/tumour_radiomics_clip_200_Dataset532_HnN.xlsx', index=False)\n",
    "        else:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_200, synthetic_features_csv, norm=False, verbose=False)\n",
    "\n",
    "        correlations = show_correlation(combined_df_scaled, verbose=False)\n",
    "        selected_features = correlations.head(10).index.tolist()\n",
    "        selected_features.remove('label')\n",
    "        best_model, importance_df, accuracy_best = get_best_DT_model(combined_df_scaled, selected_features=\"All\", verbose=False)              \n",
    "        save_best_feature[nnunet_id] = [importance_df.iloc[0].tolist()[0], accuracy_best]\n",
    "        print(\"##################\")\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv_200 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "real_features_csv_1000 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "\n",
    "save_glcm_Imc_features = {}\n",
    "missing = []\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "for nnunet_id in sorted_list_with_condition:\n",
    "    try:\n",
    "        if \"HnN\" not in nnunet_id:\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics.csv\")\n",
    "        if nnunet_id in [\"Dataset570_HnN\", \"Dataset571_HnN\", \"Dataset572_HnN\", \"Dataset573_HnN\", \"Dataset574_HnN\", \"Dataset580_HnN\", \"Dataset581_HnN\", \"Dataset582_HnN\", \"Dataset583_HnN\", \"Dataset584_HnN\", \"Dataset540_HnN\", \"Dataset541_HnN\",\"Dataset542_HnN\",\"Dataset543_HnN\",\"Dataset544_HnN\"]:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_1000, synthetic_features_csv, norm=False, verbose=False)\n",
    "            #combined_df_scaled.to_excel('/projects/brats2023_a_f/Aachen/aritifcial-head-and-neck-cts/nnUNet/trash/tumour_radiomics_clip_200_Dataset532_HnN.xlsx', index=False)\n",
    "        else:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_200, synthetic_features_csv, norm=False, verbose=False)\n",
    "                              \n",
    "        #selected_features = combined_df_scaled.filter(regex='wavelet-HLL_glcm_InverseVariance').columns.tolist()\n",
    "        best_model, importance_df, accuracy_best = get_best_DT_model(combined_df_scaled, selected_features=\"All\", verbose=False)        \n",
    "\n",
    "        #best_model, importance_df, accuracy_best = get_best_DT_model(combined_df_scaled, selected_features=selected_features, verbose=False)              \n",
    "        save_glcm_Imc_features[nnunet_id] = [importance_df.iloc[0].tolist()[0], accuracy_best]\n",
    "        print(\"##################\")\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_glcm_Imc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in save_best_feature:\n",
    "    if save_glcm_Imc_features[key][0]==\"wavelet-HHL_glcm_Imc1\":\n",
    "        print(f\"{key}\")\n",
    "        #print(f\"{key}: {save_best_feature[key]} | {save_glcm_Imc_features[key]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features_list_from_all = []\n",
    "for key in save_best_feature:\n",
    "    features_list_from_all.append(save_best_feature[key][0])\n",
    "Counter(features_list_from_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check all correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv_200 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "real_features_csv_1000 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "\n",
    "save_all_corr_features = []\n",
    "missing = []\n",
    "for nnunet_id in listdir(RADIOMICS):\n",
    "    try:\n",
    "        if \"HnN\" not in nnunet_id:\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics.csv\")\n",
    "        if nnunet_id in [\"Dataset570_HnN\", \"Dataset571_HnN\", \"Dataset572_HnN\", \"Dataset573_HnN\", \"Dataset574_HnN\", \"Dataset580_HnN\", \"Dataset581_HnN\", \"Dataset582_HnN\", \"Dataset583_HnN\", \"Dataset584_HnN\", \"Dataset540_HnN\", \"Dataset541_HnN\",\"Dataset542_HnN\",\"Dataset543_HnN\",\"Dataset544_HnN\"]:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_1000, synthetic_features_csv)\n",
    "        else:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_200, synthetic_features_csv)\n",
    "        correlations = show_correlation(combined_df_scaled)\n",
    "        selected_features = correlations.head(10).index.tolist()\n",
    "        selected_features.remove('label')\n",
    "        save_all_corr_features.append(selected_features)\n",
    "    except:\n",
    "        missing.append(nnunet_id)\n",
    "print(f\"missing: {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "correct_save_all_corr_features = []\n",
    "for list_ in save_all_corr_features:\n",
    "    for element in list_:\n",
    "        correct_save_all_corr_features.append(element)\n",
    "        \n",
    "print(f\"Total of cases: {len(save_all_corr_features)}\")\n",
    "# Count the frequency of each element\n",
    "frequency = Counter(correct_save_all_corr_features)\n",
    "\n",
    "# Display the frequency of each element\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only the most important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv_200 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "real_features_csv_1000 = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "\n",
    "save_best_feature = {}\n",
    "missing = []\n",
    "\n",
    "\n",
    "for nnunet_id in listdir(RADIOMICS):\n",
    "    try:\n",
    "        if \"HnN\" not in nnunet_id:\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics.csv\")\n",
    "        if nnunet_id in [\"Dataset570_HnN\", \"Dataset571_HnN\", \"Dataset572_HnN\", \"Dataset573_HnN\", \"Dataset574_HnN\", \"Dataset580_HnN\", \"Dataset581_HnN\", \"Dataset582_HnN\", \"Dataset583_HnN\", \"Dataset584_HnN\", \"Dataset540_HnN\", \"Dataset541_HnN\",\"Dataset542_HnN\",\"Dataset543_HnN\",\"Dataset544_HnN\"]:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_1000, synthetic_features_csv, norm=False, verbose=False)\n",
    "            #combined_df_scaled.to_excel('/projects/brats2023_a_f/Aachen/aritifcial-head-and-neck-cts/nnUNet/trash/tumour_radiomics_clip_200_Dataset532_HnN.xlsx', index=False)\n",
    "        else:\n",
    "            combined_df_scaled = get_combined_df_scaled(real_features_csv_200, synthetic_features_csv, norm=False, verbose=False)\n",
    "\n",
    "        correlations = show_correlation(combined_df_scaled, verbose=False)\n",
    "        selected_features = correlations.head(10).index.tolist()\n",
    "        selected_features.remove('label')\n",
    "        best_model, importance_df, accuracy_best = get_best_DT_model(combined_df_scaled, selected_features=\"All\", verbose=False)              \n",
    "        save_best_feature[nnunet_id] = [importance_df.iloc[0].tolist()[0], accuracy_best]\n",
    "        print(\"##################\")\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for key in save_best_feature:\n",
    "    all_features.append(save_best_feature[key][0])\n",
    "\n",
    "features_to_search = list(set(all_features))\n",
    "\n",
    "feature_keys = {}\n",
    "# Iterate over the dictionary and collect keys for each feature\n",
    "for feature in features_to_search:\n",
    "    feature_keys[feature] = [key for key, value in save_best_feature.items() if value[0] == feature]\n",
    "\n",
    "# Print the results\n",
    "for feature, keys in feature_keys.items():\n",
    "    print(f\"Keys with feature '{feature}': {keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "figs = []\n",
    "# Define the directory path\n",
    "directory_path = \"./metrics/RADIOMICS\"\n",
    "\n",
    "\n",
    "\n",
    "for feature, keys in feature_keys.items():\n",
    "    results = []\n",
    "    ## Adding the real radiomics\n",
    "    file_path = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    features = [feature]\n",
    "    data = df[features].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    data.loc[:, 'File'] = \"Real_clip_200\"  # Use .loc to set the new column\n",
    "    results.append(data)\n",
    "    for folder_name in keys:\n",
    "        file_path = os.path.join(directory_path, folder_name, 'tumour_radiomics.csv')\n",
    "        if os.path.exists(file_path): \n",
    "            df = pd.read_csv(file_path)\n",
    "            features = [feature]\n",
    "            data = df[features].copy()  \n",
    "            data.loc[:, 'File'] = folder_name  \n",
    "            results.append(data)\n",
    "    ## Adding the real radiomics\n",
    "    file_path = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    features = [feature]\n",
    "    data = df[features].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    data.loc[:, 'File'] = \"Real_clip_1000\"  # Use .loc to set the new column\n",
    "    results.append(data)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    combined_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Plot the results using Plotly\n",
    "    fig = px.box(combined_results, x='File', y=feature, title=feature)\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Dataset\",\n",
    "        yaxis_title=feature,\n",
    "        xaxis=dict(tickangle=45)\n",
    "    )\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "name_conversor = {\n",
    "    \"Dataset500_HnN\": \"M<sub>all_conv</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset501_HnN\": \"M<sub>all_conv</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset502_HnN\": \"M<sub>all_conv</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset503_HnN\": \"M<sub>all_conv</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset504_HnN\": \"M<sub>all_conv</sub><sup>WDM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset510_HnN\": \"M<sub>all_d</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset511_HnN\": \"M<sub>all_d</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset512_HnN\": \"M<sub>all_d</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset513_HnN\": \"M<sub>all_d</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset514_HnN\": \"M<sub>all_d</sub><sup>WDM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset520_HnN\": \"M<sub>all_w</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset521_HnN\": \"M<sub>all_w</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset522_HnN\": \"M<sub>all_w</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset523_HnN\": \"M<sub>all_w</sub><sup>WDM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset524_HnN\": \"M<sub>all_w</sub><sup>WDM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset530_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset531_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset532_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset533_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset534_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset540_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset541_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset542_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset543_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset544_HnN\": \"M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset550_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset551_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset552_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset553_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset554_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset560_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset561_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset562_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset563_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset564_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>200</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>200</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset570_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset571_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset572_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset573_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset574_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> Linear (1000)\",\n",
    "    \"Dataset580_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M\",\n",
    "    \"Dataset581_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras\",\n",
    "    \"Dataset582_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M SDE\",\n",
    "    \"Dataset583_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> DPM++ 2M Karras SDE\",\n",
    "    \"Dataset584_HnN\": \"M<sub>ROI_d</sub><sup>WDM<sub>1000</sub></sup> + M<sub>all_cat</sub><sup>DDPM<sub>1000</sub></sup> <br> Linear (1000)\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"./metrics/RADIOMICS\"\n",
    "\n",
    "# List of folder names to highlight in red\n",
    "nnunet_id_1000_origin = {\"Real_clip_1000\", \"Dataset570_HnN\", \"Dataset571_HnN\", \"Dataset572_HnN\", \"Dataset573_HnN\", \"Dataset574_HnN\",\n",
    "                   \"Dataset580_HnN\", \"Dataset581_HnN\", \"Dataset582_HnN\", \"Dataset583_HnN\", \"Dataset584_HnN\",\n",
    "                   \"Dataset540_HnN\", \"Dataset541_HnN\", \"Dataset542_HnN\", \"Dataset543_HnN\", \"Dataset544_HnN\"}\n",
    "\n",
    "nnunet_id_1000 = [name_conversor[key] for key in keys_list if key in nnunet_id_1000_origin]\n",
    "\n",
    "figs = []\n",
    "\n",
    "for feature, keys in feature_keys.items():\n",
    "    results = []\n",
    "    real_clip_200_added = False  # Track if Real_clip_200 is added\n",
    "    \n",
    "    # Process other datasets\n",
    "    for folder_name in keys:\n",
    "        file_path = os.path.join(directory_path, folder_name, 'tumour_radiomics.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            data = df[[feature]].copy()\n",
    "            data.loc[:, 'File'] = folder_name\n",
    "            results.append(data)\n",
    "    \n",
    "    # Combine all results into a single DataFrame\n",
    "    combined_results = pd.concat(results, ignore_index=True) if results else pd.DataFrame(columns=['File', feature])\n",
    "\n",
    "    # Get actual files present in the DataFrame\n",
    "    actual_files = set(combined_results['File'])\n",
    "\n",
    "    # Check if at least one dataset from nnunet_id_1000 is present\n",
    "    existing_nnunet_id_1000 = sorted(f for f in nnunet_id_1000 if f in actual_files and f != \"Real_clip_1000\")\n",
    "    nnunet_present = bool(existing_nnunet_id_1000)\n",
    "\n",
    "    # Only add Real_clip_200 if at least one dataset outside nnunet_id_1000 exists\n",
    "    non_nnunet_cases = actual_files - set(nnunet_id_1000)\n",
    "    if non_nnunet_cases:\n",
    "        file_path = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_200.csv\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = df[[feature]].copy()\n",
    "        data.loc[:, 'File'] = \"Real_clip_200\"\n",
    "        results.insert(0, data)  # Ensure Real_clip_200 is always at the beginning\n",
    "        real_clip_200_added = True\n",
    "\n",
    "    # Only add Real_clip_1000 if another dataset from nnunet_id_1000 is present\n",
    "    if nnunet_present:\n",
    "        file_path = \"./metrics/RADIOMICS/Dataset910_HnN/tumour_radiomics_clip_1000.csv\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = df[[feature]].copy()\n",
    "        data.loc[:, 'File'] = \"Real_clip_1000\"\n",
    "        results.append(data)\n",
    "        existing_nnunet_id_1000.append(\"Real_clip_1000\")\n",
    "\n",
    "    # Update combined_results after adding real clips\n",
    "    combined_results = pd.concat(results, ignore_index=True) if results else pd.DataFrame(columns=['File', feature])\n",
    "    actual_files = set(combined_results['File'])\n",
    "\n",
    "    # Get sorted order: Real_clip_200 (if present), then non-nnunet cases, then nnunet cases\n",
    "    sorted_files = ([\"Real_clip_200\"] if real_clip_200_added else []) + sorted(non_nnunet_cases) + existing_nnunet_id_1000\n",
    "\n",
    "    # Define color map\n",
    "    color_map = {folder: 'red' if folder in nnunet_id_1000 else 'blue' for folder in actual_files}\n",
    "\n",
    "    # Create the box plot\n",
    "    fig = px.box(combined_results, x='File', y=feature, title=feature, \n",
    "                color='File', color_discrete_map=color_map,\n",
    "                category_orders={\"File\": sorted_files})\n",
    "\n",
    "    # Update the legend to show only Real_clip_1000 and Real_clip_200\n",
    "    for trace in fig.data:\n",
    "        if trace.name not in {\"Real_clip_1000\", \"Real_clip_200\"}:\n",
    "            trace.showlegend = False  # Hide other entries in the legend\n",
    "\n",
    "    # Update the legend to show only Real_clip_1000 and Real_clip_200\n",
    "    for trace in fig.data:\n",
    "        if trace.name == \"Real_clip_1000\":\n",
    "            trace.name = \"Clip 1000\"  # Replace with desired name\n",
    "        elif trace.name == \"Real_clip_200\":\n",
    "            trace.name = \"Clip 200\"  # Replace with desired name\n",
    "        else:\n",
    "            trace.showlegend = False  # Hide other entries in the legend\n",
    "\n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Dataset\",\n",
    "        yaxis_title=feature,\n",
    "        xaxis=dict(tickangle=0),\n",
    "        legend_title_text=\"Legend\"  # Add this line to set the legend title\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    # Append figure to list\n",
    "    figs.append(fig)\n",
    "\n",
    "## Save all figures to an HTML file\n",
    "#pio.write_html(figs, file=\"./metrics/RADIOMICS/uitls/most_important_feature.html\", auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# If you want to save them to a single HTML file:\n",
    "pio.write_html(figs[0], file=\"./metrics/RADIOMICS/uitls/most_important_feature.html\", auto_open=False)\n",
    "\n",
    "# If you want to append all figures to the same file, one after another:\n",
    "with open(\"./metrics/RADIOMICS/uitls/most_important_feature.html\", \"w\") as f:\n",
    "    for fig in figs:\n",
    "        f.write(fig.to_html(full_html=False))\n",
    "\n",
    "# This will append each figure into the same HTML file without reloading the entire HTML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm, verbose=False):\n",
    "    # Load the CSV files into DataFrames\n",
    "    real_cases_df = pd.read_csv(real_features_csv)\n",
    "    synthetic_cases_df = pd.read_csv(synthetic_features_csv)\n",
    "\n",
    "    # Add a label column to each DataFrame\n",
    "    real_cases_df['label'] = 'real'\n",
    "    synthetic_cases_df['label'] = 'synthetic'\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([real_cases_df, synthetic_cases_df], ignore_index=True)\n",
    "\n",
    "    # List of columns to exclude \n",
    "    exclude_columns = [\n",
    "        \"diagnostics_Versions_PyRadiomics\",\n",
    "        \"diagnostics_Versions_Numpy\",\n",
    "        \"diagnostics_Versions_SimpleITK\",\t\n",
    "        \"diagnostics_Versions_PyWavelet\",\t\n",
    "        \"diagnostics_Versions_Python\",\t\n",
    "        \"diagnostics_Configuration_Settings\",\t\n",
    "        \"diagnostics_Configuration_EnabledImageTypes\",\t\n",
    "        \"diagnostics_Image-original_Hash\",\n",
    "        \"diagnostics_Image-original_Dimensionality\",\n",
    "        \"diagnostics_Image-original_Spacing\",\n",
    "        \"diagnostics_Image-original_Size\",\n",
    "        'diagnostics_Mask-original_Hash',\n",
    "        'diagnostics_Mask-original_Spacing',\n",
    "        'diagnostics_Mask-original_Size',\n",
    "        'diagnostics_Mask-original_BoundingBox',\n",
    "        'diagnostics_Mask-original_VoxelNum',\n",
    "        'diagnostics_Mask-original_VolumeNum',\n",
    "        'diagnostics_Mask-original_CenterOfMassIndex',\n",
    "        'diagnostics_Mask-original_CenterOfMass',\n",
    "        \"Filename\"\n",
    "    ]\n",
    "\n",
    "    # Drop the excluded columns\n",
    "    combined_df = combined_df.drop(columns=exclude_columns)\n",
    "\n",
    "    # Handle missing values only for numeric columns\n",
    "    numeric_columns = combined_df.select_dtypes(include='number').columns\n",
    "    combined_df[numeric_columns] = combined_df[numeric_columns].fillna(combined_df[numeric_columns].mean())\n",
    "\n",
    "    if norm:\n",
    "        # Normalize the features\n",
    "        scaler = StandardScaler()\n",
    "        features = combined_df.drop(columns=['label'])\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "    else:\n",
    "        features = combined_df.drop(columns=['label'])\n",
    "        features_scaled = features\n",
    "\n",
    "    # Create a new DataFrame with scaled features and the label\n",
    "    combined_df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "    combined_df_scaled['label'] = combined_df['label']\n",
    "\n",
    "    # Encode the label column to numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    combined_df_scaled['label'] = label_encoder.fit_transform(combined_df_scaled['label'])\n",
    "    \n",
    "\n",
    "    return combined_df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "\n",
    "def plot_clustering(data):\n",
    "    # Separate features and labels\n",
    "    X = data.drop('label', axis=1)  # Features\n",
    "    y = data['label']  # Labels\n",
    "\n",
    "    max_depth = 2\n",
    "    # Define the parameter grid for GridSearchCV, fixing max_depth to 10\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],  # Explore different splitting criteria\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': [None, 'sqrt', 'log2'],\n",
    "    }\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'criterion': ['gini'],  # Explore different splitting criteria\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'max_features': [None],\n",
    "    }\n",
    "    \"\"\"\n",
    "    # First decision tree to decide the best 2/3 features\n",
    "    tree_classifier = DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n",
    "    grid_search = GridSearchCV(tree_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    best_tree_classifier = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "    # Get feature importances and train with the best 2\n",
    "    feature_importances = best_tree_classifier.feature_importances_\n",
    "    top_2_features = X.columns[feature_importances.argsort()[-2:]]  # Select the 2 most important features\n",
    "    print(\"Selected Features:\", top_2_features)\n",
    "    # Train a new Decision Tree using only those 2 features\n",
    "    X_selected = X[top_2_features]\n",
    "    grid_search = GridSearchCV(tree_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_selected, y)\n",
    "    best_tree_classifier = grid_search.best_estimator_\n",
    "    feature_importances = best_tree_classifier.feature_importances_\n",
    "\n",
    "    # Assuming best_tree_classifier is a DecisionTreeClassifier\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_tree(best_tree_classifier, \n",
    "            feature_names=X_selected.columns,   # Replace X.columns with your feature names\n",
    "            class_names=['Class 0', 'Class 1'],  # Update with your class labels\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            fontsize=10)\n",
    "    plt.title(\"Decision Tree Visualization\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a DataFrame to display feature importances\n",
    "    importance_df = pd.DataFrame({'Feature': X_selected.columns, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Display the top N important features (e.g., top 10)\n",
    "    top_n = 2\n",
    "    print(importance_df)\n",
    "\n",
    "    # Select the top N important features for PCA\n",
    "    #selected_features = importance_df['Feature'][:top_n].tolist()\n",
    "    #X_selected = X_selected[selected_features]\n",
    "\n",
    "    # Initialize and apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_selected)\n",
    "\n",
    "    # **Define pca_df**\n",
    "    pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "    pca_df['label'] = y.map({0: 'Real', 1: 'Synthetic'})  # Map labels for better visualization\n",
    "\n",
    "\n",
    "    # Create the scatter plot with both real and synthetic data\n",
    "    fig1 = px.scatter(pca_df, x='PC1', y='PC2', color='label',\n",
    "                    title='PCA of Synthetic vs. Real Data',\n",
    "                    labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
    "                    opacity=0.7)            # Adjust opacity for all points\n",
    "    fig1.update_layout(\n",
    "        title={'font': {'size': 72}},  # Adjust title font size\n",
    "        xaxis={'title': {'font': {'size': 64}}, 'tickfont': {'size': 64}},  # Adjust x-axis label size and tick label size\n",
    "        yaxis={'title': {'font': {'size': 64}}, 'tickfont': {'size': 64}},  # Adjust y-axis label size and tick label size\n",
    "        legend={'font': {'size': 72}},  # Adjust legend font size\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Add jittering\n",
    "    jitter_amount = 5  # Adjust the amount of jittering\n",
    "    pca_df['PC1_jittered'] = pca_df['PC1'] + np.random.normal(0, jitter_amount, size=len(pca_df))\n",
    "    pca_df['PC2_jittered'] = pca_df['PC2'] + np.random.normal(0, jitter_amount, size=len(pca_df))\n",
    "\n",
    "    # Add overlay trace with green color for the intersection of real and synthetic\n",
    "    real_data = pca_df[pca_df['label'] == 'Real']\n",
    "    synthetic_data = pca_df[pca_df['label'] == 'Synthetic']\n",
    "\n",
    "    # Update marker size and opacity for all traces\n",
    "    for trace in fig1.data:\n",
    "        trace.update(marker={'size': 8, 'opacity': 0.7})  # Adjust size and opacity as needed\n",
    "\n",
    "    # Update traces: Real (bottom layer) and Synthetic (top layer) for clear visibility\n",
    "    fig1.for_each_trace(lambda trace: trace.update(marker={'size': 8, 'opacity': 0.9}\n",
    "                                               if trace.name == 'Real' \n",
    "                                               else {'size': 8, 'opacity': 0.4}))\n",
    "    # Force layout computation\n",
    "    fig1 = fig1.full_figure_for_development()\n",
    "\n",
    "    x_range = [fig1.layout.xaxis.range[0], fig1.layout.xaxis.range[1]]\n",
    "    y_range = [fig1.layout.yaxis.range[0], fig1.layout.yaxis.range[1]]\n",
    "\n",
    "    #fig1.show()\n",
    "\n",
    "    # Create the scatter plot with only real data, using the same scale\n",
    "    fig2 = px.scatter(pca_df[pca_df['label'] == 'Real'], x='PC1', y='PC2',\n",
    "                    title='PCA of Real Data',\n",
    "                    labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'})\n",
    "\n",
    "    # Update marker size and opacity for fig2\n",
    "    for trace in fig2.data:\n",
    "        trace.update(marker={'size': 8, 'opacity': 0.7})  # Adjust size and opacity as needed\n",
    "\n",
    "    fig2.update_xaxes(range=x_range)\n",
    "    fig2.update_yaxes(range=y_range)\n",
    "    #fig2.show()\n",
    "\n",
    "    # Create the scatter plot with only synthetic data, using the same scale\n",
    "    fig3 = px.scatter(pca_df[pca_df['label'] == 'Synthetic'], x='PC1', y='PC2',\n",
    "                    title='PCA of Synthetic Data',\n",
    "                    labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'})\n",
    "\n",
    "    # Update marker size and opacity for fig3\n",
    "    for trace in fig3.data:\n",
    "        trace.update(marker={'size': 8, 'opacity': 0.7})  # Adjust size and opacity as needed\n",
    "    \n",
    "    fig3.update_xaxes(range=x_range)\n",
    "    fig3.update_yaxes(range=y_range)\n",
    "    #fig3.show()\n",
    "\n",
    "    # Predict on the training data\n",
    "    y_pred = best_tree_classifier.predict(X_selected)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"Training Accuracy: {accuracy}\")\n",
    "\n",
    "    return fig1, fig2, fig3, top_2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_1.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition[5:]):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\" or nnunet_id==\"Dataset996_BraTS_GAN\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_1.csv\")\n",
    "        \n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        if idx == 0:\n",
    "            save_combined_df_scaled = combined_df_scaled\n",
    "        else:\n",
    "            save_combined_df_scaled = pd.concat([save_combined_df_scaled, combined_df_scaled])\n",
    "       \n",
    "        print(\"##################\")\n",
    "        \n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "save_combined_df_scaled = save_combined_df_scaled.reset_index(drop=True)\n",
    "\n",
    "fig1, fig2, fig3 = plot_clustering(data=save_combined_df_scaled)\n",
    "figs.append(fig1)\n",
    "figs.append(fig2)\n",
    "figs.append(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_1.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition[5:]):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\" or nnunet_id==\"Dataset996_BraTS_GAN\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_2.csv\")\n",
    "        \n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        if idx == 0:\n",
    "            save_combined_df_scaled = combined_df_scaled\n",
    "        else:\n",
    "            save_combined_df_scaled = pd.concat([save_combined_df_scaled, combined_df_scaled])\n",
    "       \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "save_combined_df_scaled = save_combined_df_scaled.reset_index(drop=True)\n",
    "\n",
    "fig1, fig2, fig3 = plot_clustering(data=save_combined_df_scaled)\n",
    "figs.append(fig1)\n",
    "figs.append(fig2)\n",
    "figs.append(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_1.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition[5:]):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\" or nnunet_id==\"Dataset996_BraTS_GAN\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_3.csv\")\n",
    "        \n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        if idx == 0:\n",
    "            save_combined_df_scaled = combined_df_scaled\n",
    "        else:\n",
    "            save_combined_df_scaled = pd.concat([save_combined_df_scaled, combined_df_scaled])\n",
    "       \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "save_combined_df_scaled = save_combined_df_scaled.reset_index(drop=True)\n",
    "\n",
    "fig1, fig2, fig3 = plot_clustering(data=save_combined_df_scaled)\n",
    "figs.append(fig1)\n",
    "figs.append(fig2)\n",
    "figs.append(fig3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each dataset individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_imaginary(value):\n",
    "    try:\n",
    "        float(value)  # Try converting to float\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return 'j' in str(value).lower() or 'i' in str(value).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_1.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "individual_figs = []\n",
    "save_top_features_1 = {}\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "sorted_list_with_condition = [\"Dataset602_BraTS\", \"Dataset613_BraTS\", \"Dataset996_BraTS_GAN\"]\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_1.csv\")\n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        # Drop columns with imaginary values\n",
    "        columns_with_imaginary = combined_df_scaled.applymap(contains_imaginary).any()\n",
    "        combined_df_scaled = combined_df_scaled.drop(columns=columns_with_imaginary[columns_with_imaginary].index)\n",
    "\n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_1[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        \n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_1_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "        \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "individual_figs_to_save = [x[0] for x in individual_figs]\n",
    "\n",
    "# If you want to save them to a single HTML file:\n",
    "pio.write_html(individual_figs_to_save[0], file=\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_1.html\", auto_open=False)\n",
    "\n",
    "# If you want to append all figures to the same file, one after another:\n",
    "with open(\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_1.html\", \"w\") as f:\n",
    "    for fig in individual_figs_to_save:\n",
    "        f.write(fig.to_html(full_html=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_2.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "individual_figs = []\n",
    "save_top_features_2 = {}\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "sorted_list_with_condition = [\"Dataset602_BraTS\", \"Dataset613_BraTS\", \"Dataset996_BraTS_GAN\"]\n",
    "\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_2.csv\")\n",
    "        \n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        # Drop columns with imaginary values\n",
    "        columns_with_imaginary = combined_df_scaled.applymap(contains_imaginary).any()\n",
    "        combined_df_scaled = combined_df_scaled.drop(columns=columns_with_imaginary[columns_with_imaginary].index)\n",
    "\n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_2[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_2_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "       \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "individual_figs_to_save = [x[0] for x in individual_figs]\n",
    "\n",
    "# If you want to save them to a single HTML file:\n",
    "pio.write_html(individual_figs_to_save[0], file=\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_2.html\", auto_open=False)\n",
    "\n",
    "# If you want to append all figures to the same file, one after another:\n",
    "with open(\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_2.html\", \"w\") as f:\n",
    "    for fig in individual_figs_to_save:\n",
    "        f.write(fig.to_html(full_html=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_3.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "individual_figs = []\n",
    "save_top_features_3 = {}\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "sorted_list_with_condition = [\"Dataset602_BraTS\", \"Dataset613_BraTS\", \"Dataset996_BraTS_GAN\"]\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition):\n",
    "    #try:\n",
    "        if \"BraTS\" not in nnunet_id or nnunet_id==\"Dataset960_BraTS\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_3.csv\")\n",
    "        \n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_3[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_3_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "        \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "individual_figs_to_save = [x[0] for x in individual_figs]\n",
    "\n",
    "# If you want to save them to a single HTML file:\n",
    "pio.write_html(individual_figs_to_save[0], file=\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_3.html\", auto_open=False)\n",
    "\n",
    "# If you want to append all figures to the same file, one after another:\n",
    "with open(\"./metrics/RADIOMICS/uitls/MRI_cluster_most_important_feature_tumour_3.html\", \"w\") as f:\n",
    "    for fig in individual_figs_to_save:\n",
    "        f.write(fig.to_html(full_html=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def show_feat_freq(save_top_features):\n",
    "    new_list = []\n",
    "    for key in save_top_features:\n",
    "        features = save_top_features[key]\n",
    "        for feat in features:\n",
    "            new_list.append(feat)\n",
    "\n",
    "    # Count the number of individual elements\n",
    "    element_count = len(new_list)\n",
    "\n",
    "    # Count the number of times each element appears\n",
    "    element_frequency = Counter(new_list)\n",
    "\n",
    "    print(\"Number of individual elements:\", element_count)\n",
    "    # Sort the elements by frequency in descending order\n",
    "    sorted_by_frequency = element_frequency.most_common()\n",
    "\n",
    "    print(\"Frequency of each element, sorted by frequency:\")\n",
    "    for element, count in sorted_by_frequency:\n",
    "        print(f\"{element}: {count}\")\n",
    "show_feat_freq(save_top_features=save_top_features_1)\n",
    "print(\"#################\")\n",
    "show_feat_freq(save_top_features=save_top_features_2)\n",
    "print(\"#################\")\n",
    "show_feat_freq(save_top_features=save_top_features_3)\n",
    "print(\"#################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIOMICS = \"./metrics/RADIOMICS\"\n",
    "real_features_csv = \"./metrics/RADIOMICS/Dataset960_BraTS/tumour_radiomics_1.csv\"\n",
    "\n",
    "\n",
    "nnunet_id_list = listdir(RADIOMICS)\n",
    "sorted_list_with_condition = sorted(\n",
    "    [x for x in nnunet_id_list if 'Dataset' in x],\n",
    "    key=lambda x: int(x.split('Dataset')[1].split('_')[0])\n",
    ")\n",
    "\n",
    "individual_figs = []\n",
    "save_top_features_1 = {}\n",
    "save_combined_df_scaled = None\n",
    "\n",
    "sorted_list_with_condition = [\"Dataset602_BraTS\", \"Dataset613_BraTS\", \"Dataset996_BraTS_GAN\"]\n",
    "for idx, nnunet_id in enumerate(sorted_list_with_condition):\n",
    "    #try:\n",
    "        if nnunet_id!=\"Dataset996_BraTS_GAN\":\n",
    "            continue\n",
    "        print(nnunet_id)\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_1.csv\")\n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        # Drop columns with imaginary values\n",
    "        columns_with_imaginary = combined_df_scaled.applymap(contains_imaginary).any()\n",
    "        combined_df_scaled = combined_df_scaled.drop(columns=columns_with_imaginary[columns_with_imaginary].index)\n",
    "\n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_1[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        \n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_1_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "        \n",
    "        ####################################\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_2.csv\")\n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        # Drop columns with imaginary values\n",
    "        columns_with_imaginary = combined_df_scaled.applymap(contains_imaginary).any()\n",
    "        combined_df_scaled = combined_df_scaled.drop(columns=columns_with_imaginary[columns_with_imaginary].index)\n",
    "\n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_1[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        \n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_2_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "        \n",
    "        ####################################\n",
    "        synthetic_features_csv = join(RADIOMICS, nnunet_id, \"tumour_radiomics_3.csv\")\n",
    "        combined_df_scaled = get_combined_df_scaled(real_features_csv, synthetic_features_csv, norm=False, verbose=False)\n",
    "        \n",
    "        # Drop columns with imaginary values\n",
    "        columns_with_imaginary = combined_df_scaled.applymap(contains_imaginary).any()\n",
    "        combined_df_scaled = combined_df_scaled.drop(columns=columns_with_imaginary[columns_with_imaginary].index)\n",
    "\n",
    "        fig1, fig2, fig3, top_2_features = plot_clustering(data=combined_df_scaled)\n",
    "        save_top_features_1[nnunet_id] = top_2_features\n",
    "        individual_figs.append([fig1, fig2, fig3])\n",
    "        \n",
    "        fig1.write_image(f\"./metrics/RADIOMICS/uitls/MRI_label_3_clusterplot_{nnunet_id}.png\", scale=0.8, width=2000, height=1200)\n",
    "        \n",
    "        print(\"##################\")\n",
    "    #except:\n",
    "    #    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnunet_ct)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
