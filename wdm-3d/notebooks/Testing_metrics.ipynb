{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dice score using nnunet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bone segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total spine segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n",
    "    isfile\n",
    "from nnunetv2.utilities.json_export import recursive_fix_for_json_export\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from test_utils import compute_metrics_on_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset100_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_id = \"Dataset100_HnN\"\n",
    "\n",
    "for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "    folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "    output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "    os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    try:\n",
    "        compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                folder_pred=folder_pred, \n",
    "                                output_file=output_file,\n",
    "                                image_reader_writer=SimpleITKIO(),\n",
    "                                file_ending='.nii.gz',\n",
    "                                regions_or_labels=regions_or_labels,\n",
    "                                ignore_label=None,\n",
    "                                num_processes=8,\n",
    "                                chill=False)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "    except:\n",
    "        print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset100_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset101_HnN\", \"Dataset102_HnN\", \"Dataset103_HnN\", \"Dataset104_HnN\", \"Dataset105_HnN\", \"Dataset111_HnN\", \"Dataset112_HnN\", \"Dataset113_HnN\", \"Dataset114_HnN\", \"Dataset115_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        try:\n",
    "            compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                    folder_pred=folder_pred, \n",
    "                                    output_file=output_file,\n",
    "                                    image_reader_writer=SimpleITKIO(),\n",
    "                                    file_ending='.nii.gz',\n",
    "                                    regions_or_labels=regions_or_labels,\n",
    "                                    ignore_label=None,\n",
    "                                    num_processes=8,\n",
    "                                    chill=False)\n",
    "            print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "        except:\n",
    "            print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_semantic_seg_regions(pred_file_data, vertebrae, discs, spinal_canal, spinal_cord):\n",
    "    return [\n",
    "        np.isin(pred_file_data, vertebrae).astype(np.uint8),\n",
    "        np.isin(pred_file_data, discs).astype(np.uint8),\n",
    "        np.isin(pred_file_data, spinal_canal).astype(np.uint8),\n",
    "        np.isin(pred_file_data, spinal_cord).astype(np.uint8)\n",
    "    ]\n",
    "\n",
    "def get_TP_TN_FP_FN(ground_truth, prediction):\n",
    "    TP = np.sum((ground_truth == 1) & (prediction == 1))\n",
    "    TN = np.sum((ground_truth == 0) & (prediction == 0))\n",
    "    FP = np.sum((ground_truth == 0) & (prediction == 1))\n",
    "    FN = np.sum((ground_truth == 1) & (prediction == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def compute_dsc(TP, FP, FN):\n",
    "    return (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0.0\n",
    "\n",
    "def process_file(predictions_folder, pred_file_name, gt_folder, vertebrae, discs, spinal_canal, spinal_cord):\n",
    "    gt_file_data = nib.load(join(gt_folder, pred_file_name)).get_fdata()\n",
    "    pred_file_data = nib.load(join(predictions_folder, pred_file_name)).get_fdata()\n",
    "    \n",
    "    gt_regions = get_semantic_seg_regions(gt_file_data, vertebrae, discs, spinal_canal, spinal_cord)\n",
    "    pred_regions = get_semantic_seg_regions(pred_file_data, vertebrae, discs, spinal_canal, spinal_cord)\n",
    "    \n",
    "    region_names = [\"Vertebrae\", \"Discs\", \"Spinal Canal\", \"Spinal Cord\"]\n",
    "    \n",
    "    file_results = {}\n",
    "    for name, gt, pred in zip(region_names, gt_regions, pred_regions):\n",
    "        TP, TN, FP, FN = get_TP_TN_FP_FN(gt, pred)\n",
    "        file_results[name] = {\"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN}\n",
    "    \n",
    "    return file_results\n",
    "\n",
    "def compute_mean_dsc_folder_totalspineseg(gt_folder, predictions_folder, output_file):\n",
    "    vertebrae = [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    discs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    spinal_canal = [25]\n",
    "    spinal_cord = [26]\n",
    "    \n",
    "    pred_files = [f for f in listdir(predictions_folder) if f.endswith('nii.gz')]\n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.starmap(\n",
    "            process_file,\n",
    "            [(predictions_folder, f, gt_folder, vertebrae, discs, spinal_canal, spinal_cord) for f in pred_files]\n",
    "        )\n",
    "    \n",
    "    aggregated_metrics = {name: {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0} for name in [\"Vertebrae\", \"Discs\", \"Spinal Canal\", \"Spinal Cord\"]}\n",
    "    \n",
    "    for file_result in results:\n",
    "        for region, metrics in file_result.items():\n",
    "            for key in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "                aggregated_metrics[region][key] += metrics[key]\n",
    "    \n",
    "    dsc_results = {region: compute_dsc(metrics[\"TP\"], metrics[\"FP\"], metrics[\"FN\"]) for region, metrics in aggregated_metrics.items()}\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dsc_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Mean DSC results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset100_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset100_HnN\", \"Dataset101_HnN\", \"Dataset102_HnN\", \"Dataset103_HnN\", \"Dataset104_HnN\", \"Dataset105_HnN\", \"Dataset111_HnN\", \"Dataset112_HnN\", \"Dataset113_HnN\", \"Dataset114_HnN\", \"Dataset115_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        \n",
    "        compute_mean_dsc_folder_totalspineseg(gt_folder=gt_folder, predictions_folder=folder_pred, output_file=output_file)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\"))\n",
    "        \n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instance segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN DSC\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "epsilon=1e-6\n",
    "\n",
    "problems = []\n",
    "vertebrae = [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "discs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "spinal_canal = [25]\n",
    "spinal_cord = [26]\n",
    "\n",
    "structure_name_list = ['Vertebrae', 'Discs', 'Spinal Canal', 'Spinal Cord']\n",
    "\n",
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset100_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset100_HnN\", \"Dataset101_HnN\", \"Dataset102_HnN\", \"Dataset103_HnN\", \"Dataset104_HnN\", \"Dataset105_HnN\", \"Dataset111_HnN\", \"Dataset112_HnN\", \"Dataset113_HnN\", \"Dataset114_HnN\", \"Dataset115_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_instance.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "\n",
    "        sumary_json_file = f\"./metrics/DSC/{dataset_id}/{sub_folder}/summary.json\"\n",
    "        if not os.path.exists(sumary_json_file):\n",
    "            problems.append(sumary_json_file)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        else:\n",
    "            with open(sumary_json_file, 'r') as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                dsc_results = {}\n",
    "                for idx_structure, structure in enumerate([vertebrae, discs, spinal_canal, spinal_cord]):\n",
    "                    n_TP = 0\n",
    "                    n_FP = 0\n",
    "                    n_FN = 0\n",
    "                    for case in data['metric_per_case']: # go to all predictions\n",
    "                        for label_n in structure: # To all labels\n",
    "                            if str(label_n) in case['metrics']:\n",
    "                                n_TP += case['metrics'][str(label_n)]['TP']\n",
    "                                n_FP += case['metrics'][str(label_n)]['FP']\n",
    "                                n_FN += case['metrics'][str(label_n)]['FN']\n",
    "                                \n",
    "                    dsc = (2 * n_TP ) / (2 * n_TP + n_FP + n_FN )\n",
    "                    dsc_results[structure_name_list[idx_structure]] = dsc\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(dsc_results, f, indent=4)\n",
    "            \n",
    "            print(f\"Mean DSC results saved to {output_file}\")\n",
    "        \n",
    "  \n",
    "print(f\"problems:\")\n",
    "for problem in problems:\n",
    "    print(problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a excel sheet resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc_folder = \"./metrics/DSC\"\n",
    "dataset_ids_list = [\"Dataset100_HnN\", \"Dataset101_HnN\", \"Dataset102_HnN\", \"Dataset103_HnN\", \"Dataset104_HnN\", \"Dataset105_HnN\", \"Dataset111_HnN\", \"Dataset112_HnN\", \"Dataset113_HnN\", \"Dataset114_HnN\", \"Dataset115_HnN\"]\n",
    "dataset_ids_list_translate = [\"Real\", \"DPM++ 2M 200\", \"DPM++ 2M Karras 200\", \"DPM++ 2M SDE 200\", \"DPM++ 2M SDE Karras 200\", \"Linear (1000) 200\", \"DPM++ 2M 1000\", \"DPM++ 2M Karras 1000\", \"DPM++ 2M SDE 1000\", \"DPM++ 2M SDE Karras 1000\", \"Linear (1000) 1000\"]\n",
    "results = []\n",
    "\n",
    "for nnunet_id_idx, nnunet_id in enumerate(dataset_ids_list):\n",
    "    nnunet_id_path = join(dsc_folder, nnunet_id)\n",
    "    folder_name = nnunet_id\n",
    "    sub_fold = 'hu_no_clipped'\n",
    "    #for sub_fold in listdir(nnunet_id_path):\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_instance.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"Vertebrae\": 'missing',\n",
    "            \"Discs\": 'missing',\n",
    "            \"Spinal Canal\": 'missing',\n",
    "            \"Spinal Cord\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"Vertebrae\": data['Vertebrae'],\n",
    "            \"Discs\": data['Discs'],\n",
    "            \"Spinal Canal\": data['Spinal Canal'],\n",
    "            \"Spinal Cord\": data['Spinal Cord']\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_semantic.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"Vertebrae\": 'missing',\n",
    "            \"Discs\": 'missing',\n",
    "            \"Spinal Canal\": 'missing',\n",
    "            \"Spinal Cord\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"Vertebrae\": data['Vertebrae'],\n",
    "            \"Discs\": data['Discs'],\n",
    "            \"Spinal Canal\": data['Spinal Canal'],\n",
    "            \"Spinal Cord\": data['Spinal Cord']\n",
    "        }\n",
    "        results.append(result)\n",
    "print(results)\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('./metrics/DSC/resume_total_spine_seg.xlsx', index=False)\n",
    "\n",
    "print(\"./metrics/DSC/resume_total_spine_seg.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total segmentator segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n",
    "    isfile\n",
    "from nnunetv2.utilities.json_export import recursive_fix_for_json_export\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from test_utils import compute_metrics_on_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset200_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_id = \"Dataset200_HnN\"\n",
    "\n",
    "for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "    folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "    output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "    os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    try:\n",
    "        compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                folder_pred=folder_pred, \n",
    "                                output_file=output_file,\n",
    "                                image_reader_writer=SimpleITKIO(),\n",
    "                                file_ending='.nii.gz',\n",
    "                                regions_or_labels=regions_or_labels,\n",
    "                                ignore_label=None,\n",
    "                                num_processes=16,\n",
    "                                chill=False)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "    except:\n",
    "        print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset200_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset201_HnN\", \"Dataset202_HnN\", \"Dataset203_HnN\", \"Dataset204_HnN\", \"Dataset205_HnN\", \"Dataset211_HnN\", \"Dataset212_HnN\", \"Dataset213_HnN\", \"Dataset214_HnN\", \"Dataset215_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        try:\n",
    "            compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                    folder_pred=folder_pred, \n",
    "                                    output_file=output_file,\n",
    "                                    image_reader_writer=SimpleITKIO(),\n",
    "                                    file_ending='.nii.gz',\n",
    "                                    regions_or_labels=regions_or_labels,\n",
    "                                    ignore_label=None,\n",
    "                                    num_processes=16,\n",
    "                                    chill=False)\n",
    "            print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "        except:\n",
    "            print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_semantic_seg_regions(pred_file_data, skull, claviculas, vertebrae, ribs, sternum, costal_cartilages, spinal_canal):\n",
    "    return [\n",
    "        np.isin(pred_file_data, skull).astype(np.uint8),\n",
    "        np.isin(pred_file_data, claviculas).astype(np.uint8),\n",
    "        np.isin(pred_file_data, vertebrae).astype(np.uint8),\n",
    "        np.isin(pred_file_data, ribs).astype(np.uint8),\n",
    "        np.isin(pred_file_data, sternum).astype(np.uint8),\n",
    "        np.isin(pred_file_data, costal_cartilages).astype(np.uint8),\n",
    "        np.isin(pred_file_data, spinal_canal).astype(np.uint8)\n",
    "    ]\n",
    "\n",
    "def get_TP_TN_FP_FN(ground_truth, prediction):\n",
    "    TP = np.sum((ground_truth == 1) & (prediction == 1))\n",
    "    TN = np.sum((ground_truth == 0) & (prediction == 0))\n",
    "    FP = np.sum((ground_truth == 0) & (prediction == 1))\n",
    "    FN = np.sum((ground_truth == 1) & (prediction == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def compute_dsc(TP, FP, FN):\n",
    "    return (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0.0\n",
    "\n",
    "def process_file(predictions_folder, pred_file_name, gt_folder, skull, claviculas, vertebrae, ribs, sternum, costal_cartilages, spinal_canal):\n",
    "    gt_file_data = nib.load(join(gt_folder, pred_file_name)).get_fdata()\n",
    "    pred_file_data = nib.load(join(predictions_folder, pred_file_name)).get_fdata()\n",
    "    \n",
    "    gt_regions = get_semantic_seg_regions(gt_file_data, skull, claviculas, vertebrae, ribs, sternum, costal_cartilages, spinal_canal)\n",
    "    pred_regions = get_semantic_seg_regions(pred_file_data, skull, claviculas, vertebrae, ribs, sternum, costal_cartilages, spinal_canal)\n",
    "    \n",
    "    region_names = [\"Skull\", \"Claviculas\", \"Vertebrae\", \"Ribs\", \"Sternum\", \"Costal Cartilages\", \"Spinal Canal\"]\n",
    "    \n",
    "    file_results = {}\n",
    "    for name, gt, pred in zip(region_names, gt_regions, pred_regions):\n",
    "        TP, TN, FP, FN = get_TP_TN_FP_FN(gt, pred)\n",
    "        file_results[name] = {\"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN}\n",
    "    \n",
    "    return file_results\n",
    "\n",
    "def compute_mean_dsc_folder_totalsegmentator(gt_folder, predictions_folder, output_file):\n",
    "    skull = [17]\n",
    "    claviculas = [14, 15]\n",
    "    vertebrae = list(range(1, 14))\n",
    "    ribs = list(range(18, 30))\n",
    "    sternum = [30]\n",
    "    costal_cartilages = [31]\n",
    "    spinal_canal = [16]\n",
    "    \n",
    "    pred_files = [f for f in listdir(predictions_folder) if f.endswith('nii.gz')]\n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.starmap(\n",
    "            process_file,\n",
    "            [(predictions_folder, f, gt_folder, skull, claviculas, vertebrae, ribs, sternum, costal_cartilages, spinal_canal) for f in pred_files]\n",
    "        )\n",
    "    \n",
    "    aggregated_metrics = {name: {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0} for name in [\"Skull\", \"Claviculas\", \"Vertebrae\", \"Ribs\", \"Sternum\", \"Costal Cartilages\", \"Spinal Canal\"]}\n",
    "    \n",
    "    for file_result in results:\n",
    "        for region, metrics in file_result.items():\n",
    "            for key in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "                aggregated_metrics[region][key] += metrics[key]\n",
    "    \n",
    "    dsc_results = {region: compute_dsc(metrics[\"TP\"], metrics[\"FP\"], metrics[\"FN\"]) for region, metrics in aggregated_metrics.items()}\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dsc_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Mean DSC results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset200_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset200_HnN\", \"Dataset201_HnN\", \"Dataset202_HnN\", \"Dataset203_HnN\", \"Dataset204_HnN\", \"Dataset205_HnN\", \"Dataset211_HnN\", \"Dataset212_HnN\", \"Dataset213_HnN\", \"Dataset214_HnN\", \"Dataset215_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        \n",
    "        compute_mean_dsc_folder_totalsegmentator(gt_folder=gt_folder, predictions_folder=folder_pred, output_file=output_file)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\"))\n",
    "        \n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instance segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN DSC\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "epsilon=1e-6\n",
    "\n",
    "problems = []\n",
    "skull = [17]\n",
    "claviculas = [14, 15]\n",
    "vertebrae = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "ribs = [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "sternum = [30]\n",
    "costal_cartilages = [31]\n",
    "spinal_canal = [16]\n",
    "\n",
    "structure_name_list = ['Skull', 'Claviculas', 'Vertebrae', 'Ribs', 'Sternum', 'Costal Cartilages', 'Spinal Canal']\n",
    "\n",
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset200_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset200_HnN\", \"Dataset201_HnN\", \"Dataset202_HnN\", \"Dataset203_HnN\", \"Dataset204_HnN\", \"Dataset205_HnN\", \"Dataset211_HnN\", \"Dataset212_HnN\", \"Dataset213_HnN\", \"Dataset214_HnN\", \"Dataset215_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_instance.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "\n",
    "        sumary_json_file = f\"./metrics/DSC/{dataset_id}/{sub_folder}/summary.json\"\n",
    "        if not os.path.exists(sumary_json_file):\n",
    "            problems.append(sumary_json_file)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        else:\n",
    "            with open(sumary_json_file, 'r') as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                dsc_results = {}\n",
    "                for idx_structure, structure in enumerate([vertebrae, discs, spinal_canal, spinal_cord]):\n",
    "                    n_TP = 0\n",
    "                    n_FP = 0\n",
    "                    n_FN = 0\n",
    "                    for case in data['metric_per_case']: # go to all predictions\n",
    "                        for label_n in structure: # To all labels\n",
    "                            if str(label_n) in case['metrics']:\n",
    "                                n_TP += case['metrics'][str(label_n)]['TP']\n",
    "                                n_FP += case['metrics'][str(label_n)]['FP']\n",
    "                                n_FN += case['metrics'][str(label_n)]['FN']\n",
    "                                \n",
    "                    dsc = (2 * n_TP ) / (2 * n_TP + n_FP + n_FN )\n",
    "                    dsc_results[structure_name_list[idx_structure]] = dsc\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(dsc_results, f, indent=4)\n",
    "            \n",
    "            print(f\"Mean DSC results saved to {output_file}\")\n",
    "        \n",
    "  \n",
    "print(f\"problems:\")\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a excel sheet resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc_folder = \"./metrics/DSC\"\n",
    "dataset_ids_list = [\"Dataset200_HnN\", \"Dataset201_HnN\", \"Dataset202_HnN\", \"Dataset203_HnN\", \"Dataset204_HnN\", \"Dataset205_HnN\", \"Dataset211_HnN\", \"Dataset212_HnN\", \"Dataset213_HnN\", \"Dataset214_HnN\", \"Dataset215_HnN\"]\n",
    "dataset_ids_list_translate = [\"Real\", \"DPM++ 2M 200\", \"DPM++ 2M Karras 200\", \"DPM++ 2M SDE 200\", \"DPM++ 2M SDE Karras 200\", \"Linear (1000) 200\", \"DPM++ 2M 1000\", \"DPM++ 2M Karras 1000\", \"DPM++ 2M SDE 1000\", \"DPM++ 2M SDE Karras 1000\", \"Linear (1000) 1000\"]\n",
    "results = []\n",
    "\n",
    "for nnunet_id_idx, nnunet_id in enumerate(dataset_ids_list):\n",
    "    nnunet_id_path = join(dsc_folder, nnunet_id)\n",
    "    folder_name = nnunet_id\n",
    "    sub_fold = 'hu_no_clipped'\n",
    "    #for sub_fold in listdir(nnunet_id_path):\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_instance.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"skull\": 'missing',\n",
    "            \"claviculas\": 'missing',\n",
    "            \"vertebrae\": 'missing',\n",
    "            \"ribs\": 'missing',\n",
    "            \"sternum\": 'missing',\n",
    "            \"costal_cartilages\": 'missing',\n",
    "            \"spinal_canal\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"skull\": data['skull'],\n",
    "            \"claviculas\": data['claviculas'],\n",
    "            \"vertebrae\": data['vertebrae'],\n",
    "            \"ribs\": data['ribs'],\n",
    "            \"sternum\": data['sternum'],\n",
    "            \"costal_cartilages\": data['costal_cartilages'],\n",
    "            \"spinal_canal\": data['spinal_canal']\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_semantic.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"skull\": 'missing',\n",
    "            \"claviculas\": 'missing',\n",
    "            \"vertebrae\": 'missing',\n",
    "            \"ribs\": 'missing',\n",
    "            \"sternum\": 'missing',\n",
    "            \"costal_cartilages\": 'missing',\n",
    "            \"spinal_canal\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"skull\": data['Skull'],\n",
    "            \"claviculas\": data['Claviculas'],\n",
    "            \"vertebrae\": data['Vertebrae'],\n",
    "            \"ribs\": data['Ribs'],\n",
    "            \"sternum\": data['Sternum'],\n",
    "            \"costal_cartilages\": data['Costal Cartilages'],\n",
    "            \"spinal_canal\": data['Spinal Canal']\n",
    "        }\n",
    "        results.append(result)\n",
    "print(results)\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('./metrics/DSC/resume_totalsegmentator.xlsx', index=False)\n",
    "\n",
    "print(\"./metrics/DSC/resume_totalsegmentator.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMASS segmentation (Maxilla Mandibula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n",
    "    isfile\n",
    "from nnunetv2.utilities.json_export import recursive_fix_for_json_export\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from test_utils import compute_metrics_on_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset300_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_id = \"Dataset300_HnN\"\n",
    "\n",
    "for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "    folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "    output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "    os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    try:\n",
    "        compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                folder_pred=folder_pred, \n",
    "                                output_file=output_file,\n",
    "                                image_reader_writer=SimpleITKIO(),\n",
    "                                file_ending='.nii.gz',\n",
    "                                regions_or_labels=regions_or_labels,\n",
    "                                ignore_label=None,\n",
    "                                num_processes=16,\n",
    "                                chill=False)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "    except:\n",
    "        print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [1, 2]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset300_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset311_HnN\", \"Dataset312_HnN\", \"Dataset313_HnN\", \"Dataset314_HnN\", \"Dataset315_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        try:\n",
    "            compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                    folder_pred=folder_pred, \n",
    "                                    output_file=output_file,\n",
    "                                    image_reader_writer=SimpleITKIO(),\n",
    "                                    file_ending='.nii.gz',\n",
    "                                    regions_or_labels=regions_or_labels,\n",
    "                                    ignore_label=None,\n",
    "                                    num_processes=16,\n",
    "                                    chill=False)\n",
    "            print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "        except:\n",
    "            print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_semantic_seg_regions(pred_file_data, mandibula, maxilla):\n",
    "    return [\n",
    "        np.isin(pred_file_data, mandibula).astype(np.uint8),\n",
    "        np.isin(pred_file_data, maxilla).astype(np.uint8)\n",
    "    ]\n",
    "\n",
    "def get_TP_TN_FP_FN(ground_truth, prediction):\n",
    "    TP = np.sum((ground_truth == 1) & (prediction == 1))\n",
    "    TN = np.sum((ground_truth == 0) & (prediction == 0))\n",
    "    FP = np.sum((ground_truth == 0) & (prediction == 1))\n",
    "    FN = np.sum((ground_truth == 1) & (prediction == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def compute_dsc(TP, FP, FN):\n",
    "    return (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0.0\n",
    "\n",
    "def process_file(predictions_folder, pred_file_name, gt_folder, mandibula, maxilla):\n",
    "    gt_file_data = nib.load(join(gt_folder, pred_file_name)).get_fdata()\n",
    "    pred_file_data = nib.load(join(predictions_folder, pred_file_name)).get_fdata()\n",
    "    \n",
    "    gt_regions = get_semantic_seg_regions(gt_file_data, mandibula, maxilla)\n",
    "    pred_regions = get_semantic_seg_regions(pred_file_data, mandibula, maxilla)\n",
    "    \n",
    "    region_names = [\"Mandibula\", \"Maxilla\"]\n",
    "    \n",
    "    file_results = {}\n",
    "    for name, gt, pred in zip(region_names, gt_regions, pred_regions):\n",
    "        TP, TN, FP, FN = get_TP_TN_FP_FN(gt, pred)\n",
    "        file_results[name] = {\"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN}\n",
    "    \n",
    "    return file_results\n",
    "\n",
    "def compute_mean_dsc_folder_amasss(gt_folder, predictions_folder, output_file):\n",
    "    mandibula = [1]\n",
    "    maxilla = [2]\n",
    "    \n",
    "    pred_files = [f for f in listdir(predictions_folder) if f.endswith('nii.gz')]\n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.starmap(\n",
    "            process_file,\n",
    "            [(predictions_folder, f, gt_folder, mandibula, maxilla) for f in pred_files]\n",
    "        )\n",
    "    \n",
    "    aggregated_metrics = {name: {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0} for name in [\"Mandibula\", \"Maxilla\"]}\n",
    "    \n",
    "    for file_result in results:\n",
    "        for region, metrics in file_result.items():\n",
    "            for key in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "                aggregated_metrics[region][key] += metrics[key]\n",
    "    \n",
    "    dsc_results = {region: compute_dsc(metrics[\"TP\"], metrics[\"FP\"], metrics[\"FN\"]) for region, metrics in aggregated_metrics.items()}\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dsc_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Mean DSC results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset300_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset300_HnN\", \"Dataset311_HnN\", \"Dataset312_HnN\", \"Dataset313_HnN\", \"Dataset314_HnN\", \"Dataset315_HnN\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        \n",
    "        compute_mean_dsc_folder_amasss(gt_folder=gt_folder, predictions_folder=folder_pred, output_file=output_file)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary_semantic.json\"))\n",
    "        \n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instance segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN DSC\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "epsilon=1e-6\n",
    "\n",
    "mandibula = [1]\n",
    "maxilla = [2]\n",
    "structure_name_list = ['mandibula', 'maxilla']\n",
    "\n",
    "import os\n",
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "gt_folder=\"../../nnUNet/nnUNet_raw/Dataset300_HnN/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset300_HnN\", \"Dataset311_HnN\", \"Dataset312_HnN\", \"Dataset313_HnN\", \"Dataset314_HnN\", \"Dataset315_HnN\"]\n",
    "\n",
    "problems = []\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary_instance.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "\n",
    "        sumary_json_file = f\"./metrics/DSC/{dataset_id}/{sub_folder}/summary.json\"\n",
    "        if not os.path.exists(sumary_json_file):\n",
    "            problems.append(sumary_json_file)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        else:\n",
    "            with open(sumary_json_file, 'r') as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                dsc_results = {}\n",
    "                for idx_structure, structure in enumerate([mandibula, maxilla]):\n",
    "                    n_TP = 0\n",
    "                    n_FP = 0\n",
    "                    n_FN = 0\n",
    "                    for case in data['metric_per_case']: # go to all predictions\n",
    "                        for label_n in structure: # To all labels\n",
    "                            if str(label_n) in case['metrics']:\n",
    "                                n_TP += case['metrics'][str(label_n)]['TP']\n",
    "                                n_FP += case['metrics'][str(label_n)]['FP']\n",
    "                                n_FN += case['metrics'][str(label_n)]['FN']\n",
    "                                \n",
    "                    dsc = (2 * n_TP ) / (2 * n_TP + n_FP + n_FN )\n",
    "                    dsc_results[structure_name_list[idx_structure]] = dsc\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(dsc_results, f, indent=4)\n",
    "            \n",
    "            print(f\"Mean DSC results saved to {output_file}\")\n",
    "        \n",
    "  \n",
    "print(f\"problems:\")\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a excel sheet resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc_folder = \"./metrics/DSC\"\n",
    "dataset_ids_list = [\"Dataset300_HnN\", \"Dataset311_HnN\", \"Dataset312_HnN\", \"Dataset313_HnN\", \"Dataset314_HnN\", \"Dataset315_HnN\"]\n",
    "dataset_ids_list_translate = [\"Real\", \"DPM++ 2M 1000\", \"DPM++ 2M Karras 1000\", \"DPM++ 2M SDE 1000\", \"DPM++ 2M SDE Karras 1000\", \"Linear (1000) 1000\"]\n",
    "results = []\n",
    "\n",
    "for nnunet_id_idx, nnunet_id in enumerate(dataset_ids_list):\n",
    "    nnunet_id_path = join(dsc_folder, nnunet_id)\n",
    "    folder_name = nnunet_id\n",
    "    sub_fold = 'hu_no_clipped'\n",
    "    #for sub_fold in listdir(nnunet_id_path):\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_instance.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"mandibula\": 'missing',\n",
    "            \"maxilla\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'instance',\n",
    "            \"mandibula\": data['mandibula'],\n",
    "            \"maxilla\": data['maxilla']\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    json_file_path = join(nnunet_id_path, sub_fold, 'summary_semantic.json')\n",
    "    if not os.path.exists(json_file_path):\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"mandibula\": 'missing',\n",
    "            \"maxilla\": 'missing'\n",
    "        }\n",
    "        results.append(result)\n",
    "    else:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        result = {\n",
    "            \"Folder\": dataset_ids_list_translate[nnunet_id_idx],\n",
    "            \"Sub_folder\": sub_fold,\n",
    "            \"DSC type\": 'semantic',\n",
    "            \"mandibula\": data['Mandibula'],\n",
    "            \"maxilla\": data['Maxilla']\n",
    "        }\n",
    "        results.append(result)\n",
    "print(results)\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('./metrics/DSC/resume_amasss.xlsx', index=False)\n",
    "\n",
    "print(\"./metrics/DSC/resume_amasss.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tumour segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head and neck tumour segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n",
    "    isfile\n",
    "from nnunetv2.utilities.json_export import recursive_fix_for_json_export\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from test_utils import compute_metrics_on_folder\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_root_path = \"./metrics/DSC\"\n",
    "test_folder = \"../../nnUNet/nnUNet_test/\"\n",
    "regions_or_labels = [1]\n",
    "folder_ref = \"../../nnUNet/nnUNet_raw/Dataset910_HnN/labelsTs\"\n",
    "# The dataset_name_list is the one to change.\n",
    "dataset_name_list_200 =['Dataset500_HnN', 'Dataset501_HnN', 'Dataset502_HnN', 'Dataset503_HnN', 'Dataset504_HnN', \n",
    "                        'Dataset510_HnN', 'Dataset511_HnN', 'Dataset512_HnN', 'Dataset513_HnN', 'Dataset514_HnN', \n",
    "                        'Dataset520_HnN', 'Dataset521_HnN', 'Dataset522_HnN', 'Dataset523_HnN', 'Dataset524_HnN', \n",
    "                        'Dataset530_HnN', 'Dataset531_HnN', 'Dataset532_HnN', 'Dataset533_HnN', 'Dataset534_HnN', \n",
    "                        'Dataset550_HnN', 'Dataset551_HnN', 'Dataset552_HnN', 'Dataset553_HnN', 'Dataset554_HnN', \n",
    "                        'Dataset560_HnN', 'Dataset561_HnN', 'Dataset562_HnN', 'Dataset563_HnN', 'Dataset564_HnN', 'Dataset910_HnN', 'Dataset982_HnN_GAN']\n",
    "\n",
    "dataset_name_list_1000 = ['Dataset540_HnN', 'Dataset541_HnN', 'Dataset542_HnN', 'Dataset543_HnN', 'Dataset544_HnN',\n",
    "                          'Dataset570_HnN', 'Dataset571_HnN', 'Dataset572_HnN', 'Dataset573_HnN', 'Dataset574_HnN', \n",
    "                          'Dataset580_HnN', 'Dataset581_HnN', 'Dataset582_HnN', 'Dataset583_HnN', 'Dataset584_HnN']\n",
    "\n",
    "dataset_name_list_200_last = [\n",
    "    'Dataset555_HnN', 'Dataset556_HnN', 'Dataset557_HnN', 'Dataset558_HnN', 'Dataset559_HnN',\n",
    "    'Dataset565_HnN', 'Dataset566_HnN', 'Dataset567_HnN', 'Dataset568_HnN', 'Dataset569_HnN',\n",
    "    'Dataset590_HnN', 'Dataset591_HnN', 'Dataset592_HnN', 'Dataset593_HnN', 'Dataset594_HnN',\n",
    "    'Dataset595_HnN', 'Dataset596_HnN', 'Dataset597_HnN', 'Dataset598_HnN', 'Dataset599_HnN',\n",
    "    'Dataset983_HnN_GAN'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "for dataset_id in dataset_name_list_1000:\n",
    "    dataset_id_path = join(test_folder, dataset_id)\n",
    "    for sub_folder in listdir(dataset_id_path):\n",
    "        complete_path = join(dataset_id_path, sub_folder)\n",
    "        if sub_folder == \"nnUNetTrainer__nnUNetPlans__3d_fullres\":\n",
    "            #shutil.rmtree(complete_path)\n",
    "            print(f\"Removed: {complete_path}\")\n",
    "            continue\n",
    "        if len(listdir(complete_path))==198:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"complete_path: {complete_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_name_list_200:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        folder_pred = join(test_folder, dataset_name, sub_folder)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                        folder_pred=folder_pred, \n",
    "                                        output_file=output_file,\n",
    "                                        image_reader_writer=SimpleITKIO(),\n",
    "                                        file_ending='.nii.gz',\n",
    "                                        regions_or_labels=regions_or_labels,\n",
    "                                        ignore_label=None,\n",
    "                                        num_processes=8,\n",
    "                                        chill=False)\n",
    "            except:\n",
    "                print(f\"Problems folder_pred: {folder_pred}\")\n",
    "print(f\"DONE\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_name_list_1000:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        folder_pred = join(test_folder, dataset_name, sub_folder)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                        folder_pred=folder_pred, \n",
    "                                        output_file=output_file,\n",
    "                                        image_reader_writer=SimpleITKIO(),\n",
    "                                        file_ending='.nii.gz',\n",
    "                                        regions_or_labels=regions_or_labels,\n",
    "                                        ignore_label=None,\n",
    "                                        num_processes=8,\n",
    "                                        chill=False)\n",
    "            except:\n",
    "                print(f\"Problems folder_pred: {folder_pred}\")\n",
    "print(f\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_name_list_200_last:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        folder_pred = join(test_folder, dataset_name, sub_folder)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                print(f\"Doing {folder_pred}\")\n",
    "                compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                        folder_pred=folder_pred, \n",
    "                                        output_file=output_file,\n",
    "                                        image_reader_writer=SimpleITKIO(),\n",
    "                                        file_ending='.nii.gz',\n",
    "                                        regions_or_labels=regions_or_labels,\n",
    "                                        ignore_label=None,\n",
    "                                        num_processes=8,\n",
    "                                        chill=False)\n",
    "            except:\n",
    "                print(f\"Problems folder_pred: {folder_pred}\")\n",
    "print(f\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compress all info in a excel file\n",
    "data_list = []\n",
    "\n",
    "dataset_name_list_total =['Dataset500_HnN', 'Dataset501_HnN', 'Dataset502_HnN', 'Dataset503_HnN', 'Dataset504_HnN', \n",
    "                        'Dataset510_HnN', 'Dataset511_HnN', 'Dataset512_HnN', 'Dataset513_HnN', 'Dataset514_HnN', \n",
    "                        'Dataset520_HnN', 'Dataset521_HnN', 'Dataset522_HnN', 'Dataset523_HnN', 'Dataset524_HnN', \n",
    "                        'Dataset530_HnN', 'Dataset531_HnN', 'Dataset532_HnN', 'Dataset533_HnN', 'Dataset534_HnN', \n",
    "                        'Dataset550_HnN', 'Dataset551_HnN', 'Dataset552_HnN', 'Dataset553_HnN', 'Dataset554_HnN', \n",
    "                        'Dataset560_HnN', 'Dataset561_HnN', 'Dataset562_HnN', 'Dataset563_HnN', 'Dataset564_HnN',\n",
    "                        'Dataset540_HnN', 'Dataset541_HnN', 'Dataset542_HnN', 'Dataset543_HnN', 'Dataset544_HnN',\n",
    "                        'Dataset570_HnN', 'Dataset571_HnN', 'Dataset572_HnN', 'Dataset573_HnN', 'Dataset574_HnN', \n",
    "                        'Dataset580_HnN', 'Dataset581_HnN', 'Dataset582_HnN', 'Dataset583_HnN', 'Dataset584_HnN', \n",
    "                        'Dataset910_HnN', 'Dataset982_HnN']\n",
    "\n",
    "for dataset_name in dataset_name_list_total:\n",
    "    for sub_folder in listdir(join(metrics_root_path, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        if sub_folder!=\"hu_no_clipped\":\n",
    "            continue\n",
    "        if not os.path.exists(output_file): \n",
    "            data_list.append([f\"{dataset_name}_{sub_folder}\", \"Missing\", 'Missing'])\n",
    "            continue\n",
    "        else:\n",
    "            with open(output_file, \"r\") as file:\n",
    "                results = json.load(file)\n",
    "            Dice = results['foreground_mean']['Dice']\n",
    "            IoU = results['foreground_mean']['IoU']\n",
    "            #print(f\"{dataset_name}_{sub_folder}\", round(Dice['mean'], 4), round(IoU['mean'], 4))\n",
    "            data_list.append([f\"{dataset_name}_{sub_folder}\", round(Dice['mean'], 4), round(IoU['mean'], 4)])\n",
    "\n",
    "header = [\"Dataset\", \"DSC\", \"IoU\"]\n",
    "df = pd.DataFrame(data_list, columns=header)\n",
    "output_file_excel = \"./metrics/DSC/summary_dsc.xlsx\"\n",
    "df.to_excel(output_file_excel, index=False)\n",
    "print(output_file_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding best segmented case\n",
    "\n",
    "best_seg_dsc = 0\n",
    "\n",
    "best_cases_names = []\n",
    "for dataset_name in dataset_name_list_200+dataset_name_list_1000:\n",
    "    for sub_folder in listdir(join(metrics_root_path, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        if not os.path.exists(output_file): \n",
    "            continue\n",
    "        else:\n",
    "            with open(output_file, \"r\") as file:\n",
    "                results = json.load(file)\n",
    "            for case in results['metric_per_case']:\n",
    "                Dice = case['metrics']['1']['Dice']\n",
    "                if Dice>=0.5:\n",
    "                    best_seg_case = f\"{dataset_name}_{sub_folder}_{case['prediction_file'].split('/')[-1]}: {Dice}\"\n",
    "                    print(f\"best_seg_case: {best_seg_case}\")\n",
    "                    print(case['prediction_file'])\n",
    "                    best_cases_names.append(f\"{dataset_name}_{sub_folder}\")\n",
    "                    print(\"############\")\n",
    "                if Dice > best_seg_dsc:\n",
    "                    best_seg_dsc = Dice\n",
    "                    best_seg_case = f\"{dataset_name}_{sub_folder}_{case['prediction_file'].split('/')[-1]}: {Dice}\"\n",
    "                    #print(f\"best_seg_case: {best_seg_case}\")\n",
    "                    #print(case['prediction_file'])\n",
    "                    #print(\"############\")\n",
    "            \n",
    "\n",
    "print(best_cases_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Count the occurrences of each element\n",
    "element_counts = Counter(best_cases_names)\n",
    "element_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brain tumour segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n",
    "    isfile\n",
    "from nnunetv2.utilities.json_export import recursive_fix_for_json_export\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from test_utils import compute_metrics_on_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [(1,2,3), (2,3), (3)]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset960_BraTS/labelsTs\"\n",
    "# Experiment\n",
    "dataset_id = \"Dataset960_BraTS\"\n",
    "\n",
    "for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "    folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "    print(f\"folder_pred: {folder_pred}\")\n",
    "    output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "    os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    #try:\n",
    "    else:\n",
    "        print(f\"output_file: {output_file}\")\n",
    "        compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                folder_pred=folder_pred, \n",
    "                                output_file=output_file,\n",
    "                                image_reader_writer=SimpleITKIO(),\n",
    "                                file_ending='.nii.gz',\n",
    "                                regions_or_labels=regions_or_labels,\n",
    "                                ignore_label=None,\n",
    "                                num_processes=8,\n",
    "                                chill=True)\n",
    "        print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "    #except:\n",
    "    #    print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General\n",
    "test_folder = \"../../nnUNet/nnUNet_test\"\n",
    "output_folder = \"./metrics/DSC\"\n",
    "# Label specific\n",
    "regions_or_labels = [(1,2,3), (2,3), (3)]\n",
    "folder_ref=\"../../nnUNet/nnUNet_raw/Dataset960_BraTS/labelsTs\"\n",
    "# Experiment\n",
    "dataset_ids_list = [\"Dataset996_BraTS_GAN\" ,\"Dataset600_BraTS\", \"Dataset601_BraTS\", \"Dataset602_BraTS\", \"Dataset603_BraTS\", \"Dataset604_BraTS\", \"Dataset610_BraTS\", \"Dataset611_BraTS\", \"Dataset612_BraTS\", \"Dataset613_BraTS\", \"Dataset614_BraTS\", \"Dataset620_BraTS\", \"Dataset621_BraTS\", \"Dataset622_BraTS\", \"Dataset623_BraTS\", \"Dataset624_BraTS\"]\n",
    "\n",
    "for dataset_id in dataset_ids_list:\n",
    "    for sub_folder in listdir(join(test_folder, dataset_id)):\n",
    "        folder_pred=join(test_folder, dataset_id, sub_folder)\n",
    "        output_file= join(output_folder, dataset_id, sub_folder, \"summary.json\")\n",
    "        os.makedirs(join(output_folder, dataset_id, sub_folder), exist_ok=True)\n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"output_file: {output_file}\")\n",
    "            compute_metrics_on_folder(folder_ref=folder_ref, \n",
    "                                    folder_pred=folder_pred, \n",
    "                                    output_file=output_file,\n",
    "                                    image_reader_writer=SimpleITKIO(),\n",
    "                                    file_ending='.nii.gz',\n",
    "                                    regions_or_labels=regions_or_labels,\n",
    "                                    ignore_label=None,\n",
    "                                    num_processes=8,\n",
    "                                    chill=False)\n",
    "            print(join(output_folder, dataset_id, sub_folder, \"summary.json\"))\n",
    "        except:\n",
    "            print(f\"Problems with: {join(dataset_id, sub_folder, 'summary.json')}\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compress all info in a excel file\n",
    "data_list = []\n",
    "\n",
    "for dataset_name in [\"Dataset960_BraTS\"]+dataset_ids_list:\n",
    "    print(f\"dataset_name: {dataset_name}\")\n",
    "    for sub_folder in listdir(join(metrics_root_path, dataset_name)):\n",
    "        output_folder = join(metrics_root_path, dataset_name, sub_folder)\n",
    "        output_file = join(output_folder, \"summary.json\")\n",
    "        if sub_folder!=\"no_clipped\":\n",
    "            continue\n",
    "        if not os.path.exists(output_file): \n",
    "            data_list.append([f\"{dataset_name}_{sub_folder}\", \"Missing\", 'Missing'])\n",
    "            continue\n",
    "        else:\n",
    "            with open(output_file, \"r\") as file:\n",
    "                results = json.load(file)\n",
    "            Dice_mean = results['foreground_mean']['Dice']['mean']\n",
    "            Dice_std = results['foreground_mean']['Dice']['std']\n",
    "            Dice = results['mean']\n",
    "            wt_mean = results['mean']['(1, 2, 3)']['Dice']['mean']\n",
    "            wt_std = results['mean']['(1, 2, 3)']['Dice']['std']\n",
    "            tc_mean = results['mean']['(2, 3)']['Dice']['mean']\n",
    "            tc_std = results['mean']['(2, 3)']['Dice']['std']\n",
    "            et_mean = results['mean']['3']['Dice']['mean']\n",
    "            et_std = results['mean']['3']['Dice']['std']\n",
    "            #IoU = results['mean']['IoU']\n",
    "            #print(f\"{dataset_name}_{sub_folder}\", round(Dice['mean'], 4), round(IoU['mean'], 4))\n",
    "            data_list.append([f\"{dataset_name}_{sub_folder}\", \n",
    "                              f\"{round(wt_mean, 4)} ±{round(wt_std, 4)}\",\n",
    "                              f\"{round(tc_mean, 4)} ±{round(tc_std, 4)}\",\n",
    "                              f\"{round(et_mean, 4)} ±{round(et_std, 4)}\",\n",
    "                              f\"{round(Dice_mean, 4)} ±{round(Dice_std, 4)}\"\n",
    "                              ]\n",
    "                              )\n",
    "\n",
    "header = [\"Dataset\", \"wt\", \"tc\", \"et\", 'mean']\n",
    "df = pd.DataFrame(data_list, columns=header)\n",
    "output_file_excel = \"./metrics/DSC/summary_dsc_brats.xlsx\"\n",
    "df.to_excel(output_file_excel, index=False)\n",
    "print(output_file_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MS-SSIM computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "from glob import glob\n",
    "from generative.metrics import MultiScaleSSIMMetric\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\".\")\n",
    "from test_utils import get_data_loader, get_BraTS_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiprocessing\n",
    "import multiprocessing\n",
    "image_key = 'image'\n",
    "clip_min = -200 # TODO -> this is changed for the cases with -1000 and 1000\n",
    "clip_max = 200\n",
    "data_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data\"\n",
    "\n",
    "image_files = sorted(glob(os.path.join(data_dir, \"*.nii.gz\")))  \n",
    "data_list = [{\"image\": img} for img in image_files]\n",
    "\n",
    "dataloader_1, dataloader_2 = get_data_loader(image_key, clip_min, clip_max, data_list[:5], pad_or_crop=True, two_loaders=True, cache_rate=[0,0], for_ms_ssim=True, not_normalise=True)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "ms_ssim = MultiScaleSSIMMetric(spatial_dims=3, data_range=1.0, kernel_size=7)\n",
    "\n",
    "print(\"Computing MS-SSIM (this takes a while)...\")\n",
    "\n",
    "def compute_ms_ssim(args):\n",
    "    \"\"\"Compute MS-SSIM between two images.\"\"\"\n",
    "    print(\"Computing MSSSIM\")\n",
    "    img1, img2, device = args\n",
    "    ms_ssim_value = ms_ssim(img1.to(device), img2.to(device)).item()\n",
    "    print(f\"ms_ssim_value: {ms_ssim_value}\")\n",
    "    return ms_ssim_value\n",
    "\n",
    "def main(dataloader_1, dataloader_2, device):\n",
    "    ms_ssim_list = []\n",
    "    for step_1, batch in enumerate(dataloader_1):\n",
    "        print(f\"Doing case {step_1}\")\n",
    "        img = batch['image']\n",
    "        print(f\"img.max: {img.max()}\")\n",
    "        print(f\"img.min: {img.min()}\")\n",
    "\n",
    "        # Prepare arguments for multiprocessing\n",
    "        args = []\n",
    "        pbar2 = tqdm(enumerate(dataloader_2), total=len(dataloader_2))\n",
    "        for step_2, batch2 in pbar2:\n",
    "            if step_1 == step_2:\n",
    "                continue\n",
    "            img2 = batch2['image']\n",
    "            args.append((img, img2, device))\n",
    "            if step_2==10:\n",
    "                break\n",
    "        print(f\"args: {len(args)}\")\n",
    "        # Use multiprocessing to compute MS-SSIM with 8 CPUs\n",
    "        with multiprocessing.Pool(processes=1) as pool:  # Limit to 8 CPUs\n",
    "            results = list(pool.imap(compute_ms_ssim, args))\n",
    "        \n",
    "        ms_ssim_list.extend(results)\n",
    "        break\n",
    "    return ms_ssim_list\n",
    "\n",
    "ms_ssim_list = main(dataloader_1, dataloader_2, device)\n",
    "ms_ssim_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_key = 'image'\n",
    "clip_min = -200 # TODO -> this is changed for the cases with -1000 and 1000\n",
    "clip_max = 200\n",
    "data_dir = \"../results/Synthetic_Datasets/Whole_scans/Bone_segmentation/200/DPM++_2M\"\n",
    "\n",
    "image_files = sorted(glob(os.path.join(data_dir, \"*.nii.gz\")))  \n",
    "data_list = [{\"image\": img} for img in image_files]\n",
    "\n",
    "dataloader_1, dataloader_2 = get_data_loader(image_key, clip_min, clip_max, data_list, pad_or_crop=True, two_loaders=True, cache_rate=[0,1])\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "ms_ssim = MultiScaleSSIMMetric(spatial_dims=3, data_range=1.0, kernel_size=7)\n",
    "\n",
    "print(\"Computing MS-SSIM (this takes a while)...\")\n",
    "ms_ssim_list = []\n",
    "\n",
    "for step_1, batch in enumerate(dataloader_1):\n",
    "    pbar2 = tqdm(enumerate(dataloader_2), total=len(dataloader_2))\n",
    "    print(f\"Doing case {step_1}\")\n",
    "    img = batch['image']\n",
    "    for step_2, batch2 in pbar2:\n",
    "        img2 = batch2['image']\n",
    "        if step_1 == step_2:\n",
    "            continue\n",
    "        ms_ssim_value = ms_ssim(img.to(device), img2.to(device)).item()\n",
    "        ms_ssim_list.append(ms_ssim_value)\n",
    "        pbar2.set_postfix({\"ms_ssim\": float(ms_ssim_value)})\n",
    "\n",
    "ms_ssim_list = np.array(ms_ssim_list)\n",
    "print(\"Calculated MS-SSIMs. Computing mean ...\")\n",
    "print(f\"Mean MS-SSIM: {ms_ssim_list.mean():.6f}\")\n",
    "\n",
    "mm_ssim = {}\n",
    "mm_ssim[\"mean\"] = float(ms_ssim_list.mean())\n",
    "#with open(\"./metrics/MS-SSIM/fake_mm_ssim.json\", \"w\") as json_file:\n",
    "#    json.dump(mm_ssim, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_key = 'image'\n",
    "clip_min = -200 # TODO -> this is changed for the cases with -1000 and 1000\n",
    "clip_max = 200\n",
    "data_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "\n",
    "with open(\"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\", 'r') as f:\n",
    "    data_split = json.load(f)\n",
    "    training = data_split['training']\n",
    "image_files = []\n",
    "for file_name in training:\n",
    "    image_files.append(file_name['image'])\n",
    "data_list = [{\"image\": os.path.join(data_dir, img)} for img in image_files]\n",
    "dataloader_1, dataloader_2 = get_data_loader(image_key, clip_min, clip_max, data_list, two_loaders=True, cache_rate=[0,1])\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ms_ssim = MultiScaleSSIMMetric(spatial_dims=3, data_range=1.0, kernel_size=7)\n",
    "\n",
    "print(\"Computing MS-SSIM (this takes a while)...\")\n",
    "ms_ssim_list = []\n",
    "pbar = tqdm(enumerate(dataloader_1), total=len(dataloader_1))\n",
    "for step_1, batch in pbar:\n",
    "    img = batch['image']\n",
    "    for step_2, batch2 in enumerate(dataloader_2):\n",
    "        img2 = batch2['image']\n",
    "        if step_1 == step_2:\n",
    "            continue\n",
    "        ms_ssim_list.append(ms_ssim(img.to(device), img2.to(device)).item())\n",
    "    pbar.update()\n",
    "\n",
    "ms_ssim_list = np.array(ms_ssim_list)\n",
    "print(\"Calculated MS-SSIMs. Computing mean ...\")\n",
    "print(f\"Mean MS-SSIM: {ms_ssim_list.mean():.6f}\")\n",
    "\n",
    "mm_ssim = {}\n",
    "for idx, result in enumerate(ms_ssim_list):\n",
    "    mm_ssim[image_files[idx].split('/')[-1].split('.nii.gz')[0]] = result\n",
    "mm_ssim[\"mean\"] = ms_ssim_list.mean():.6f\n",
    "with open(\"./metrics/MS-SSIM/real_mm_ssim.json\", \"w\") as json_file:\n",
    "    json.dump(mm_ssim, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a excel file\n",
    "with open(\"./metrics/MS-SSIM/real_mm_ssim.json\", \"r\") as json_file:\n",
    "    data_real = json.load(json_file)\n",
    "with open(\"./metrics/MS-SSIM/fake_mm_ssim.json\", \"r\") as json_file:\n",
    "    data_fake = json.load(json_file)\n",
    "data_list = []\n",
    "for key_case in data_real:\n",
    "    data_list.append([key_case, data_real[key_case], data_fake[key_case]])\n",
    "header = [\"Case\", \"MS-SSIM real\", \"MS-SSIM fake\"]\n",
    "df = pd.DataFrame(data_list, columns=header)\n",
    "output_file = \"./metrics/MS-SSIM/summary.xlsx\"\n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MAE - comparision between real case and generated case with the same condition (segmentation and/or ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\".\")\n",
    "from test_utils import absolute_mean_error\n",
    "from test_utils import get_data_loader, get_BraTS_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CT_dataloader(clip_min, clip_max, fake_data_dir, pad_or_crop, load_only_fake):\n",
    "    # Real data loader\n",
    "    image_key = 'image'\n",
    "\n",
    "    original_data_dir = \"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256\"\n",
    "    # Load Training and Test data split\n",
    "    with open(\"../../HnN_cancer_data/HnN_cancer_data_1_1_1_256_256_256/data_split.json\", 'r') as f:\n",
    "        data_split = json.load(f)\n",
    "        training = data_split['training']\n",
    "\n",
    "    # Loading all real cases to memory\n",
    "    real_image_files = []\n",
    "    for file_name in training:\n",
    "        real_image_files.append(file_name['image'])\n",
    "    original_data_list = [{\"image\": os.path.join(original_data_dir, img)} for img in real_image_files]\n",
    "    print(f\"real original_data_list: {original_data_list[0]}\")\n",
    "\n",
    "\n",
    "    fake_image_files = []\n",
    "    black_list = []\n",
    "    for img in real_image_files:\n",
    "        img = img.split('/')[-1]\n",
    "        fake_name = img.replace('.nii.gz', '_CT_n0.nii.gz')\n",
    "        fake_path = os.path.join(fake_data_dir, fake_name)\n",
    " \n",
    "        if os.path.exists(fake_path):\n",
    "            fake_image_files.append(fake_path)\n",
    "        else:\n",
    "            black_list.append(img)\n",
    "    data_list = [{\"image\": img} for img in fake_image_files]\n",
    "    print(f\"fake data_list: {data_list[0]}\")\n",
    "    fake_data_loader = get_data_loader(image_key, clip_min, clip_max, data_list, pad_or_crop=pad_or_crop, two_loaders=False, cache_rate=[0])\n",
    "    \n",
    "    if not load_only_fake:        \n",
    "        real_data_loader = get_data_loader(image_key, clip_min, clip_max, original_data_list, pad_or_crop=pad_or_crop, two_loaders=False, cache_rate=[0]) # change 0 to 1\n",
    "        return real_data_loader, fake_data_loader, black_list\n",
    "    else:\n",
    "        return fake_data_loader, black_list\n",
    "clip_min = -200\n",
    "clip_max = 200\n",
    "load_only_fake = False\n",
    "pad_or_crop=False\n",
    "fake_data_dir = \"../results/Synthetic_Datasets/Whole_scans/Bone_segmentation/200/DPM++_2M\"\n",
    "real_data_loader, fake_data_loader, black_list = get_CT_dataloader(clip_min, clip_max, fake_data_dir, pad_or_crop=pad_or_crop, load_only_fake=load_only_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "real_iter = iter(real_data_loader)\n",
    "pbar = tqdm(enumerate(fake_data_loader), total=len(fake_data_loader))\n",
    "mae_results = {}\n",
    "for fake_batch_idx, fake_batch in pbar: # iterate over fake cases \n",
    "    real_batch = next(real_iter)\n",
    "    case_name = fake_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1].replace('_CT_n0.nii.gz', '.nii.gz')\n",
    "    if case_name in black_list:\n",
    "        continue\n",
    "    else:\n",
    "        y_fake = fake_batch['image']\n",
    "        y_real = real_batch['image']\n",
    "        if case_name != real_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1]:\n",
    "            warnings.warn(f\"The cases don't match. Double check: {case_name} \\n {real_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1]}\", SyntaxWarning)\n",
    "        else:\n",
    "            try:\n",
    "                mae_here = absolute_mean_error(y_real[:,:,:-1,:-1,:-1], y_fake)\n",
    "            except:\n",
    "                # Some cases don't have the same orientation. Therefore, they need to be flipped\n",
    "                y_fake = y_fake.permute(0, 1, 2, 4, 3)\n",
    "                y_fake = torch.flip(y_fake, dims=[4])\n",
    "                mae_here = absolute_mean_error(y_real[:,:,:-1,:-1,:-1], y_fake)\n",
    "            pbar.set_postfix({\"mae_here\": float(mae_here)})\n",
    "        \n",
    "            mae_results[case_name.replace('.nii.gz', '')] = float(mae_here)\n",
    "            \n",
    "print(mae_results)\n",
    "with open(\"./trash/mae.json\", \"w\") as json_file:\n",
    "    json.dump(mae_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compute_mean(data):\n",
    "    all_mae_values = []\n",
    "    for key in data:\n",
    "        all_mae_values.append(data[key])\n",
    "    all_mae_values = np.array(all_mae_values)\n",
    "    # Compute mean and standard deviation\n",
    "    mean = np.mean(all_mae_values)\n",
    "    std_dev = np.std(all_mae_values)\n",
    "    return mean, std_dev\n",
    "\n",
    "results = []\n",
    "root_path = \"./metrics/MAE\"\n",
    "for modality in os.listdir(root_path):\n",
    "    if modality==\"MRI\":\n",
    "        exps_path = os.path.join(root_path, modality, 'Tumour_generation')\n",
    "        for exp in os.listdir(exps_path):\n",
    "            exp_path = os.path.join(exps_path, exp)       \n",
    "            for sampler in os.listdir(exp_path):\n",
    "                mae_json = os.path.join(exp_path, sampler, 'MAE.json')\n",
    "                with open(mae_json, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                mean, std_dev = compute_mean(data)\n",
    "                #print(f\"{mae_json.split('MAE/')[-1]}: {mean}\")\n",
    "                #print(f\"{mae_json.split('MAE/')[-1]}: {std_dev}\")\n",
    "                #print(\"#############\")\n",
    "                result = {\n",
    "                            \"Model\": exp,\n",
    "                            \"Sampling Method\": sampler,\n",
    "                            \"MAE \": f\"{mean}±{std_dev}\"\n",
    "                        }\n",
    "                #results.append(result)\n",
    "    elif modality==\"Whole_scans\":\n",
    "        tasks_path = os.path.join(root_path, modality)\n",
    "        for task in os.listdir(tasks_path):\n",
    "            sub_tasks_path = os.path.join(tasks_path, task)\n",
    "            if task==\"Tumour_inpaint\":\n",
    "                for noise_type in os.listdir(sub_tasks_path):\n",
    "                    noise_type_path = os.path.join(sub_tasks_path, noise_type)\n",
    "                    for hu_value in os.listdir(noise_type_path):\n",
    "                        hu_value_path = os.path.join(noise_type_path, hu_value)\n",
    "                        for sampler in os.listdir(hu_value_path):\n",
    "                            mae_json = os.path.join(hu_value_path, sampler, 'MAE.json')\n",
    "                            with open(mae_json, 'r') as file:\n",
    "                                data = json.load(file)\n",
    "                            mean, std_dev = compute_mean(data)\n",
    "                            #print(f\"{mae_json.split('MAE/')[-1]}: {mean}\")\n",
    "                            #print(f\"{mae_json.split('MAE/')[-1]}: {std_dev}\")\n",
    "                            #print(\"#############\")\n",
    "                            result = {\n",
    "                                \"Model\": hu_value,\n",
    "                                \"Sampling Method\": sampler,\n",
    "                                \"MAE \": f\"{mean}±{std_dev}\"\n",
    "                            }\n",
    "                            results.append(result)\n",
    "\n",
    "            else:\n",
    "                for sub_task in os.listdir(sub_tasks_path):\n",
    "                    sub_task_path = os.path.join(sub_tasks_path, sub_task)\n",
    "                    for sampler in os.listdir(sub_task_path):\n",
    "                        mae_json = os.path.join(sub_task_path, sampler, 'MAE.json')\n",
    "                        with open(mae_json, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "                        mean, std_dev = compute_mean(data)\n",
    "                        print(f\"{mae_json.split('MAE/')[-1]}: {mean}\")\n",
    "                        print(f\"{mae_json.split('MAE/')[-1]}: {std_dev}\")\n",
    "                        print(\"#############\")\n",
    "                        result = {\n",
    "                            \"Model\": sub_task,\n",
    "                            \"Sampling Method\": sampler,\n",
    "                            \"MAE \": f\"{mean}±{std_dev}\"\n",
    "                        }\n",
    "                        #results.append(result)\n",
    "\n",
    "\n",
    "        \n",
    "print(results)\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('./metrics/MAE/resume_mae_200_CT.xlsx', index=False)\n",
    "\n",
    "print(\"./metrics/MAE/resume_mae_200_CT.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_json = './metrics/MAE/Dataset982_HnN_GAN/MAE.json'\n",
    "with open(mae_json, 'r') as file:\n",
    "    data = json.load(file)\n",
    "mean, std_dev = compute_mean(data)\n",
    "print(f\"{mae_json.split('MAE/')[-1]}: {mean}\")\n",
    "print(f\"{mae_json.split('MAE/')[-1]}: {std_dev}\")\n",
    "print(\"##########\")\n",
    "mae_json = './metrics/MAE/Dataset996_BraTS_GAN/MAE.json'\n",
    "with open(mae_json, 'r') as file:\n",
    "    data = json.load(file)\n",
    "mean, std_dev = compute_mean(data)\n",
    "print(f\"{mae_json.split('MAE/')[-1]}: {mean}\")\n",
    "print(f\"{mae_json.split('MAE/')[-1]}: {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BraTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MRI_dataloader(fake_data_dir):\n",
    "    # Real data loader\n",
    "    original_data_dir = \"../../../brats2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "    # Load Training and Test data split\n",
    "    with open(\"../../../brats2023/BraTS2023_GLI_data_split.json\", 'r') as f:\n",
    "        data_split = json.load(f)\n",
    "        training = data_split['training']\n",
    "\n",
    "    # Loading all real cases to memory\n",
    "    real_image_files = []\n",
    "    for file_name in training:\n",
    "        real_image_files.append(file_name['t1c'])\n",
    "    original_data_list = [{\"image\": os.path.join(original_data_dir, img)} for img in real_image_files]\n",
    "    print(f\"real original_data_list: {original_data_list[0]}\")\n",
    "\n",
    "    fake_image_files = []\n",
    "    black_list = []\n",
    "    for img in real_image_files:\n",
    "        img = img.split('/')[-1]\n",
    "        fake_name = img.replace('.nii.gz', '_n0.nii.gz')\n",
    "        fake_path = os.path.join(fake_data_dir, fake_name)\n",
    "        if os.path.exists(fake_path):\n",
    "            fake_image_files.append(fake_path)\n",
    "        else:\n",
    "            black_list.append(img)\n",
    "    data_list = [{\"image\": img} for img in fake_image_files]\n",
    "    print(f\"fake data_list: {data_list[0]}\")\n",
    "\n",
    "    fake_data_loader = get_BraTS_data_loader(in_keys=\"image\", data_list=data_list, cache_rate=0, apply_quantile=False)   \n",
    "    real_data_loader = get_BraTS_data_loader(in_keys=\"image\", data_list=original_data_list, cache_rate=0, apply_quantile=True)\n",
    "    return real_data_loader, fake_data_loader, black_list\n",
    "\n",
    "\n",
    "\n",
    "fake_data_dir = \"../results/Synthetic_Datasets/MRI/Tumour_generation/concat_cond/DPM++_2M\"\n",
    "real_data_loader, fake_data_loader, black_list = get_MRI_dataloader(fake_data_dir=fake_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "real_iter = iter(real_data_loader)\n",
    "pbar = tqdm(enumerate(fake_data_loader), total=len(fake_data_loader))\n",
    "mae_results = {}\n",
    "for fake_batch_idx, fake_batch in pbar: # iterate over fake cases \n",
    "    real_batch = next(real_iter)\n",
    "    case_name = fake_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1].replace('_n0.nii.gz', '.nii.gz')\n",
    "    if case_name in black_list:\n",
    "        continue\n",
    "    else:\n",
    "        y_fake = fake_batch['image']\n",
    "        y_real = real_batch['image']\n",
    "        if case_name != real_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1]:\n",
    "            warnings.warn(f\"The cases don't match. Double check: {case_name} \\n {real_batch['image_meta_dict']['filename_or_obj'][0].split('/')[-1]}\", SyntaxWarning)\n",
    "        else:\n",
    "            mae_here = absolute_mean_error(y_real, y_fake)\n",
    "            pbar.set_postfix({\"mae_here\": float(mae_here)})\n",
    "            mae_results[case_name.replace('.nii.gz', '')] = float(mae_here)\n",
    "            break\n",
    "print(mae_results)\n",
    "with open(\"./trash/mae.json\", \"w\") as json_file:\n",
    "    json.dump(mae_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnunet_ct)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
