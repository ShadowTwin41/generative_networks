{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "from monai.transforms import Compose, LoadImage, CropForeground, EnsureChannelFirst, ResizeWithPadOrCrop, ScaleIntensityRange\n",
    "from guided_diffusion.c_unet import SuperResModel, UNetModel, EncoderUNetModel\n",
    "import torch\n",
    "import torch as th\n",
    "from diffusers import DDPMScheduler, DPMSolverMultistepScheduler\n",
    "from DWT_IDWT.DWT_IDWT_layer import IDWT_3D, DWT_3D\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "idwt = IDWT_3D(\"haar\")\n",
    "dwt = DWT_3D(\"haar\")\n",
    "from monai.data import load_decathlon_datalist, DataLoader, CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd, \n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged, \n",
    "    ResizeWithPadOrCropd,\n",
    "    CopyItemsd\n",
    "    )\n",
    "from utils.data_loader_utils import ConvertToMultiChannel_BackandForeground_Contrastd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from monai.transforms import Resize\n",
    "from scipy.ndimage import center_of_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(file_path, norm, clip):\n",
    "    \"\"\"\n",
    "    Loads the nii.gz file, and normalises if necessary.\n",
    "    Arguments:\n",
    "        file_path (str): Path to the nii.gz file.\n",
    "        norm (bool): True for clipping and normalisation.\n",
    "    Return:\n",
    "        Numpy array of nii.gz file.\n",
    "    \"\"\"\n",
    "    transforms = [\n",
    "        LoadImage(image_only=True),\n",
    "        EnsureChannelFirst()\n",
    "        ]\n",
    "    if clip:\n",
    "        transforms.append(\n",
    "        ScaleIntensityRange(a_min=-200, a_max=200, b_min=-200, b_max=200, clip=True)\n",
    "        )\n",
    "    if norm:\n",
    "        transforms.append(\n",
    "        ScaleIntensityRange(a_min=-200, a_max=200, b_min=-1, b_max=1, clip=True)\n",
    "        )\n",
    "    apply_transforms = Compose(transforms)\n",
    "    np_tensor = apply_transforms(file_path)[0].numpy()\n",
    "    return np_tensor\n",
    "\n",
    "def get_segmentation(file_path):\n",
    "    \"\"\"\n",
    "    Load the segmentation, crops the foreground and reshape to 128x128x128 using padding.\n",
    "    This ensures that the segmentation is in the middle of the volume\n",
    "    Arguments:\n",
    "        file_path (str): Path to the segmentation file.\n",
    "    Return:\n",
    "        Numpy array of the segmentation.\n",
    "    \"\"\"\n",
    "    transforms = Compose([\n",
    "        LoadImage(image_only=True),\n",
    "        EnsureChannelFirst(),\n",
    "        CropForeground(select_fn=lambda x: x > 0, margin=0),\n",
    "        ResizeWithPadOrCrop(spatial_size=(128,128,128))\n",
    "    ])\n",
    "    segmentation = transforms(file_path)[0].numpy()\n",
    "    return segmentation\n",
    "\n",
    "def rescale_array(arr, minv, maxv): #monai function adapted\n",
    "    \"\"\"\n",
    "    Rescale the values of numpy array `arr` to be from `minv` to `maxv`.\n",
    "    \"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        mina = np.min(arr)\n",
    "        maxa = np.max(arr)\n",
    "    elif isinstance(arr, th.Tensor):\n",
    "        mina = th.min(arr)\n",
    "        maxa = th.max(arr)\n",
    "    if mina == maxa:\n",
    "        return arr * minv\n",
    "    # normalize the array first\n",
    "    norm = (arr - mina) / (maxa - mina) \n",
    "    # rescale by minv and maxv, which is the normalized array by default \n",
    "    return (norm * (maxv - minv)) + minv  \n",
    "\n",
    "\n",
    "def get_crop_tensors(healthy_ct_scan_full_res, region_to_place_tumour_mask, segmentation, device):\n",
    "    \"\"\"\n",
    "    Selects a random center and crops the volume with that center and shape 128x128x128.\n",
    "    Arguments:\n",
    "        healthy_ct_scan_full_res (numpy array): Healthy volume.\n",
    "        region_to_place_tumour_mask (numpy array): Mask of the region to where the tumour can be placed.\n",
    "        segmentation (numpy array): Tumour segmentation.\n",
    "    \"\"\"\n",
    "    centroid = center_of_mass(segmentation)\n",
    "    random_x, random_y, random_z = int(centroid[0]), int(centroid[1]), int(centroid[2])\n",
    "\n",
    "    # Padding the volume so no region ouside of the volume is selected\n",
    "    healthy_ct_scan_full_res = np.pad(healthy_ct_scan_full_res, pad_width=64, mode='constant', constant_values=-200) # -200 background\n",
    "    region_to_place_tumour_mask = np.pad(region_to_place_tumour_mask, pad_width=64, mode='constant', constant_values=1) # 1 means that the tumour cannot be placed there\n",
    "    \n",
    "    # Select a random center\n",
    "    voxel_indices = np.argwhere(region_to_place_tumour_mask == 2)\n",
    "    \n",
    "    \n",
    "    #random_voxel_indices = [[165, 167, 111]] \n",
    "    #random_x, random_y, random_z = 165, 167, 111\n",
    "    # Crop the full resolution scan and mask\n",
    "    healthy_ct_scan = healthy_ct_scan_full_res[\n",
    "        random_x-64:random_x+64,\n",
    "        random_y-64:random_y+64,\n",
    "        random_z-64:random_z+64\n",
    "        ]\n",
    "    region_to_place_tumour_mask_crop = region_to_place_tumour_mask[ \n",
    "        random_x-64:random_x+64,\n",
    "        random_y-64:random_y+64,\n",
    "        random_z-64:random_z+64\n",
    "        ] \n",
    "    # Ensure the segmentation remains within the anatomical boundaries defined by the region_to_place_tumour_mask_crop\n",
    "    segmentation[region_to_place_tumour_mask_crop == 1] = 0\n",
    "\n",
    "    # Keep the original intensities of the cropped region\n",
    "    healthy_ct_scan_origin_intensities = np.copy(healthy_ct_scan)\n",
    "    \n",
    "    # Convert to torch and add two dimentions\n",
    "    healthy_ct_scan = th.from_numpy(healthy_ct_scan)\n",
    "    healthy_ct_scan = rescale_array(healthy_ct_scan, minv=-1, maxv=1)\n",
    "    healthy_ct_scan = healthy_ct_scan.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    healthy_ct_scan_origin_intensities = th.from_numpy(healthy_ct_scan_origin_intensities)\n",
    "    healthy_ct_scan_origin_intensities = healthy_ct_scan_origin_intensities.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    segmentation = th.from_numpy(segmentation)\n",
    "    segmentation = segmentation.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    healthy_ct_scan_full_res = th.from_numpy(healthy_ct_scan_full_res)\n",
    "    healthy_ct_scan_full_res = healthy_ct_scan_full_res.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    return healthy_ct_scan_full_res, healthy_ct_scan, healthy_ct_scan_origin_intensities, segmentation, random_voxel_indices\n",
    "\n",
    "def get_affine_and_header(file_path):\n",
    "  \"\"\"\n",
    "  Extracts the affine transformation matrix and header information from a NIfTI file.\n",
    "  Args:\n",
    "    filename (str): The path to the NIfTI file.\n",
    "  Returns:\n",
    "    tuple: A tuple containing the affine matrix and header information.\n",
    "  \"\"\"\n",
    "  img = nib.load(file_path)\n",
    "  affine = img.affine\n",
    "  header = img.header\n",
    "  return affine, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(in_channels=11, out_channels=8, channel_mult=[1, 2, 2, 4, 4, 4], label_cond_in_channels=0, use_label_cond_conv=False, pretrained_weights_path=None):\n",
    "    model = UNetModel(\n",
    "        image_size=128,\n",
    "        in_channels=in_channels,\n",
    "        model_channels=64,\n",
    "        out_channels=out_channels,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=tuple([]),\n",
    "        dropout=0.0,\n",
    "        channel_mult=channel_mult,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=True,\n",
    "        use_new_attention_order=False,\n",
    "        dims=3,\n",
    "        num_groups=32,\n",
    "        bottleneck_attention=False,\n",
    "        additive_skips=True,\n",
    "        resample_2d=False,\n",
    "        label_cond_in_channels=label_cond_in_channels,\n",
    "        use_label_cond_conv=use_label_cond_conv,\n",
    "    )\n",
    "    # Load the pre-trained weights\n",
    "    state_dict = torch.load(pretrained_weights_path, map_location=torch.device('cuda:0'))  # Load to CPU, or adjust for GPU if needed\n",
    "\n",
    "    # Load weights into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(sch, num_inference_steps):\n",
    "    if sch==\"DPM++_2M\":\n",
    "        use_karras_sigmas = False\n",
    "        algorithm_type = \"dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_Karras\":\n",
    "        use_karras_sigmas = True\n",
    "        algorithm_type = \"dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_SDE\":\n",
    "        use_karras_sigmas = False\n",
    "        algorithm_type = \"sde-dpmsolver++\"\n",
    "    elif sch==\"DPM++_2M_SDE_Karras\":\n",
    "        use_karras_sigmas = True\n",
    "        algorithm_type = \"sde-dpmsolver++\"\n",
    "        \n",
    "    scheduler = DPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=1000, \n",
    "            variance_type=\"fixed_large\", \n",
    "            prediction_type=\"sample\", \n",
    "            use_karras_sigmas=use_karras_sigmas, \n",
    "            algorithm_type=algorithm_type\n",
    "            #use_beta_sigmas=True # https://huggingface.co/papers/2407.12173\n",
    "            )\n",
    "    scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cropped CT scans - Inpainting - random label crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    ScaleIntensityd,\n",
    "    CopyItemsd,\n",
    "    CropForegroundd,\n",
    "    SpatialCropd,\n",
    "    ToTensord,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandAffined,\n",
    "    RandSpatialCropd,\n",
    "    Lambda\n",
    ")\n",
    "from monai.data import CSVDataset\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "from utils.convert_head_n_neck_cancer import ConvertHeadNNeckCancerd as LABEL_TRANSFORM\n",
    "from utils.crop_scan_center_in_tumour import CropScanCenterInTumour\n",
    "def set_contrast_tensor(d):\n",
    "    if 'contrast' in d: \n",
    "        if d['contrast']==0:\n",
    "            no_contrast_tensor = np.ones_like(d['label'])\n",
    "            contrast_tensor = np.zeros_like(d['label'])\n",
    "        elif d['contrast']==1:\n",
    "            no_contrast_tensor = np.zeros_like(d['label'])\n",
    "            contrast_tensor = np.ones_like(d['label'])\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong contrast value: {d['contrast']}\")\n",
    "        d[\"no_contrast_tensor\"] = no_contrast_tensor\n",
    "        d[\"contrast_tensor\"] = contrast_tensor\n",
    "        d[\"scan_volume_crop_pad\"] = d[\"scan_ct\"] # Changing the name to work like when no data augmentation is used.\n",
    "        d[\"label_crop_pad\"] = d[\"label\"] # Changing the name to work like when no data augmentation is used.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(CSV_PATH, NUM_WORKERS, use_data_augmentation, clip_min, clip_max): \n",
    "    \n",
    "    scan_name = \"scan_ct\"\n",
    "    col_names = ['scan_ct', 'label', 'center_x', 'center_y', 'center_z', 'x_extreme_min', 'x_extreme_max', 'y_extreme_min', 'y_extreme_max', 'z_extreme_min', 'z_extreme_max', 'x_size', 'y_size', 'z_size', 'contrast']\n",
    "    col_types= {'center_x': {'type': int}, 'center_y': {'type': int}, 'center_z': {'type': int}, 'x_extreme_min': {'type': int}, 'x_extreme_max': {'type': int}, 'y_extreme_min': {'type': int}, 'y_extreme_max': {'type': int}, 'z_extreme_min': {'type': int}, 'z_extreme_max': {'type': int}, 'x_size': {'type': int}, 'y_size': {'type': int}, 'z_size': {'type': int}, 'contrast': {'type': int}}      \n",
    "    print(f\"Scan Modality: {scan_name}\")\n",
    "\n",
    "    train_transforms = [\n",
    "                    LoadImaged(keys=[scan_name, 'label'], image_only=False),\n",
    "                    EnsureChannelFirstd(keys=[scan_name, \"label\"]),\n",
    "                    EnsureTyped(keys=[scan_name, \"label\"]),\n",
    "                    CopyItemsd(keys=[scan_name], names=[f\"{scan_name}_origin\"]),\n",
    "                    ScaleIntensityRanged(keys=[f\"{scan_name}_origin\"], a_min=int(clip_min), a_max=int(clip_max), b_min=int(clip_min), b_max=int(clip_max), clip=True),\n",
    "                    LABEL_TRANSFORM(keys=\"label\"),\n",
    "                ] \n",
    "\n",
    "    new_keys = ['scan_volume_crop_pad', 'label_crop_pad']\n",
    "    interpolation_mode=['trilinear', 'nearest']\n",
    "\n",
    "    if use_data_augmentation:\n",
    "        col_names = ['scan_ct', 'label', 'contrast']\n",
    "        col_types = None\n",
    "        train_transforms.append(\n",
    "            RandSpatialCropd(keys=[scan_name,'label'], roi_size=[128,128,128])\n",
    "            )\n",
    "        train_transforms.append(\n",
    "            Lambda(set_contrast_tensor)\n",
    "            )\n",
    "        train_transforms.append(\n",
    "            RandFlipd(keys=new_keys, spatial_axis=0, prob=0.1, lazy=True)\n",
    "            )\n",
    "        train_transforms.append(\n",
    "            RandFlipd(keys=new_keys, spatial_axis=1, prob=0.1, lazy=True)\n",
    "            )\n",
    "        train_transforms.append(\n",
    "            RandFlipd(keys=new_keys, spatial_axis=2, prob=0.1, lazy=True)\n",
    "            )\n",
    "        # Rotate 90 degrees\n",
    "        train_transforms.append(\n",
    "                        RandRotate90d(keys=new_keys, prob=0.1, max_k=3, lazy=True)\n",
    "        )\n",
    "\n",
    "        # Based on file:///Users/andreferreira/Downloads/s10462-023-10453-z.pdf and https://arxiv.org/pdf/2006.06676.pdf\n",
    "        # rotate 45 degrees\n",
    "        # scale_range (-0.1, 0.1) -> zoom!\n",
    "        # shear_range (-0.1, 0.1)\n",
    "        train_transforms.append(\n",
    "            RandAffined(\n",
    "                        keys=new_keys,\n",
    "                        prob=0.1,\n",
    "                        rotate_range=((-np.pi/4,np.pi/4),(-np.pi/4,np.pi/4),(-np.pi/4,np.pi/4)), # 6 degrees\n",
    "                        #translate_range=(16,16,16), \n",
    "                        scale_range=((-0.2,0.2),(-0.2,0.2),(-0.2,0.2)),\n",
    "                        shear_range=((-0.2,0.2),(-0.2,0.2),(-0.2,0.2)),\n",
    "                        padding_mode=\"border\",\n",
    "                        mode=interpolation_mode,\n",
    "                        lazy=True,\n",
    "                        )\n",
    "        )\n",
    "        train_transforms.append(\n",
    "            ScaleIntensityRanged(keys=[scan_name, \"scan_volume_crop_pad\"], a_min=int(clip_min), a_max=int(clip_max), b_min=-1.0, b_max=1.0, clip=True)\n",
    "        )\n",
    "        train_transforms.append(\n",
    "            ToTensord(keys=[scan_name, 'no_contrast_tensor', 'contrast_tensor', 'scan_volume_crop_pad', 'label', 'label_crop_pad'])\n",
    "            )\n",
    "    else:\n",
    "        train_transforms.append(\n",
    "            CropScanCenterInTumour(keys=scan_name, dilation=False, translate_range=None)\n",
    "            )       \n",
    "        train_transforms.append(\n",
    "            ScaleIntensityRanged(keys=[scan_name], a_min=int(clip_min), a_max=int(clip_max), b_min=-1.0, b_max=1.0, clip=True)\n",
    "        )\n",
    "        \n",
    "        train_transforms.append(ToTensord(keys=[scan_name, 'no_contrast_tensor', 'contrast_tensor', 'scan_volume_crop', 'scan_volume_crop_pad', 'label', 'label_crop_pad', 'center_x', 'center_y', 'center_z', 'x_extreme_min', 'x_extreme_max', 'y_extreme_min', 'y_extreme_max', 'z_extreme_min', 'z_extreme_max', 'x_size', 'y_size', 'z_size']))\n",
    "        \n",
    "    final_train_transforms = Compose(train_transforms)\n",
    "    \n",
    "    train_CSVdataset = CSVDataset(src=CSV_PATH, col_names=col_names, col_types=col_types)\n",
    "    train_CSVdataset = CacheDataset(train_CSVdataset, transform=final_train_transforms, cache_rate=0, num_workers=NUM_WORKERS, progress=True)  \n",
    "    train_loader = DataLoader(train_CSVdataset, batch_size=int(1), num_workers=NUM_WORKERS, drop_last=True, shuffle=False, collate_fn=pad_list_data_collate)\n",
    "    \n",
    "    print(f\"Number of training images: {len(train_CSVdataset)}\")\n",
    "    print(f'Dataset training: number of batches: {len(train_loader)}')\n",
    "    print(\"Leaving the data loader. Good luck!\") \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31\n",
    "* HU between -200 and 200. tumour weight 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(train_loader, model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "\n",
    "    for idx, batch  in enumerate(train_loader):\n",
    "        for sch in scheduler_list:\n",
    "            scheduler = get_scheduler(sch, num_inference_steps)\n",
    "            noise_start = torch.randn(1, 1, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            segmentation = batch[\"label_crop_pad\"].cuda()\n",
    "            no_contrast_tensor = batch[\"no_contrast_tensor\"].cuda()\n",
    "            contrast_tensor = batch[\"contrast_tensor\"].cuda()\n",
    "            label_condition = torch.cat((no_contrast_tensor, contrast_tensor, segmentation), dim=1)\n",
    "            \n",
    "            input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy().astype(float)[0][0], affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "                \n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            \n",
    "        if idx+1 == n:\n",
    "            break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_min = -200\n",
    "clip_max = 200\n",
    "in_channels = 4\n",
    "out_channels = 1\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31/checkpoints/hnn_tumour_inpainting_2000000.pt'  # Specify the correct path\n",
    "channel_mult=[1, 2, 2, 4, 4]\n",
    "\n",
    "model = get_model(in_channels=in_channels, \n",
    "                  out_channels=out_channels,\n",
    "                  channel_mult=channel_mult,\n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "CSV_PATH = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/utils/hnn.csv\" # cases with no empty label not considered\n",
    "use_data_augmentation = True\n",
    "\n",
    "train_loader = get_loader(CSV_PATH, NUM_WORKERS, use_data_augmentation, clip_min, clip_max)\n",
    "\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_tumour_inpainting_CT_default_tumour_inpainting__data_augment_20_11_2024_11:07:31\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(train_loader=train_loader,\n",
    "              model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59\n",
    "* HU between -1000 and 1000. tumour weight 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(train_loader, model, scheduler_list, n, num_inference_steps, clip_min, clip_max, out_path):\n",
    "    model.cuda()\n",
    "\n",
    "    for idx, batch  in enumerate(train_loader):\n",
    "        for sch in scheduler_list:\n",
    "            scheduler = get_scheduler(sch, num_inference_steps)\n",
    "            noise_start = torch.randn(1, 1, 128, 128, 128)  \n",
    "            # Prepare the noisy image\n",
    "            final_scan = noise_start.clone().detach()\n",
    "            final_scan = final_scan.cuda()\n",
    "\n",
    "            segmentation = batch[\"label_crop_pad\"].cuda()\n",
    "            no_contrast_tensor = batch[\"no_contrast_tensor\"].cuda()\n",
    "            contrast_tensor = batch[\"contrast_tensor\"].cuda()\n",
    "            label_condition = torch.cat((no_contrast_tensor, contrast_tensor, segmentation), dim=1)\n",
    "\n",
    "            input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "            input_model = input_model.cuda()\n",
    "\n",
    "            # Start the reverse process (denoising from noise)\n",
    "            for timestep in tqdm(scheduler.timesteps, desc=\"Processing timesteps\"):\n",
    "                # Get the current timestep's noise\n",
    "                t = torch.tensor([timestep] * final_scan.shape[0])\n",
    "                t = t.cuda()\n",
    "                # Perform one step of denoising\n",
    "                with torch.no_grad():\n",
    "                    model_kwargs = {}\n",
    "                    noise_pred = model(input_model, timesteps=t, label_condition=label_condition, **model_kwargs)\n",
    "                    # Update the noisy_latents (reverse the noise process)\n",
    "                    final_scan = scheduler.step(model_output=noise_pred, timestep=timestep, sample=final_scan)\n",
    "                    final_scan = final_scan['prev_sample']\n",
    "                    input_model = torch.cat((final_scan, label_condition), dim=1)\n",
    "\n",
    "            # Assuming final_image is a PyTorch tensor\n",
    "            # Convert the final_image tensor to a NumPy array if it's a tensor\n",
    "            final_image_np = final_scan.squeeze().cpu().numpy()  # Remove the channel dim and move to CPU\n",
    "\n",
    "            if th.sum(contrast_tensor) != 0:\n",
    "                ending_name = \"_Contrast\"\n",
    "            else:\n",
    "                ending_name = \"out_contrast\"\n",
    "                \n",
    "            synth_ct_scan_output = os.path.join(out_path, f'CT_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            sample_denorm = np.clip(final_image_np, a_min=-1, a_max=1) # remove very high and low values\n",
    "\n",
    "            sample_denorm = rescale_array(\n",
    "                            arr=sample_denorm, \n",
    "                            minv=int(clip_min), \n",
    "                            maxv=int(clip_max)\n",
    "                            )\n",
    "\n",
    "            sample_denorm_corrected = sample_denorm\n",
    "\n",
    "            nii_image = nib.Nifti1Image(sample_denorm_corrected, affine=np.eye(4))  # Identity affine for simplicity\n",
    "            nib.save(nii_image, synth_ct_scan_output)\n",
    "\n",
    "            nii_image = nib.Nifti1Image(segmentation.cpu().numpy().astype(float)[0][0], affine=np.eye(4))  # Identity affine for simplicity\n",
    "            seg_ct_scan_output = os.path.join(out_path, f'label_{ending_name}_{clip_min}_{clip_max}_{idx}_{sch}.nii.gz')\n",
    "            nib.save(nii_image, seg_ct_scan_output)\n",
    "        if idx+1 == n:\n",
    "            break\n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_min = -1000\n",
    "clip_max = 1000\n",
    "in_channels = 4\n",
    "out_channels = 1\n",
    "label_cond_in_channels = 0\n",
    "use_label_cond_conv = False\n",
    "pretrained_weights_path = '../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/runs/hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59/checkpoints/hnn_tumour_inpainting_1310000.pt'  # Specify the correct path\n",
    "channel_mult=[1, 2, 2, 4, 4]\n",
    "\n",
    "model = get_model(in_channels=in_channels, \n",
    "                  out_channels=out_channels,\n",
    "                  channel_mult=channel_mult,\n",
    "                  label_cond_in_channels=label_cond_in_channels, \n",
    "                  use_label_cond_conv=use_label_cond_conv,\n",
    "                  pretrained_weights_path=pretrained_weights_path)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "CSV_PATH = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/utils/hnn.csv\" # cases with no empty label not considered\n",
    "use_data_augmentation = True\n",
    "\n",
    "train_loader = get_loader(CSV_PATH, NUM_WORKERS, use_data_augmentation, clip_min, clip_max)\n",
    "\n",
    "print(\"Loaded model and data loader\")\n",
    "\n",
    "# Control inference parameters\n",
    "scheduler_list = [\"DPM++_2M\", \"DPM++_2M_Karras\", \"DPM++_2M_SDE\", \"DPM++_2M_SDE_Karras\"]\n",
    "n = 1\n",
    "num_inference_steps = 100\n",
    "out_path = \"../../aritifcial-head-and-neck-cts/WDM3D/wdm-3d/notebooks/trash/hnn_tumour_inpainting_CT_default_tumour_inpainting__DA_tumorW_10_28_11_2024_14:37:59\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "run_inference(train_loader=train_loader,\n",
    "              model=model,\n",
    "              scheduler_list=scheduler_list,\n",
    "               n=n, \n",
    "               num_inference_steps=num_inference_steps, \n",
    "               clip_min=clip_min,\n",
    "               clip_max=clip_max, \n",
    "               out_path=out_path)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wdm)",
   "language": "python",
   "name": "wdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
